[{"content":" 变量与可变性 数据类型 标量类型 复合类型 函数 注释 控制流 变量与可变性 声明变量使用let关键字 默认情况下，变量是不可变的（Immutable） 声明变量时，在变量前面加mut，就可以使变量可变。 变量和常量 常量（constant），常量在绑定值以后也是不可变的，但是它与不可变的变量有很多区别：\n不可以使用mut，常量永远都是不可变的 声明常量使用 const 关键字，它的类型必须被标注 常量可以在任何作用域内进行声明，包括全局作用域 常量只可以绑定到常量表达式，无法绑定到函数的调用结果或只能在运行时才能计算出的值 在程序运行期间，常量在其声明的作用域内一直有效\n命名规范：Rust锂常量使用全大写字母，每个单词之间使用下划线分类例如：MAX_POINTS\nShadowing 可以使用相同的名字声明新的变量，新的变量就会shadow之前声明的同名变量 在后续的代码中这个变量名代表的就是新的变量 shadow 和把变量标记为 mut 是不一样的： 如果不使用 let 关键字，那么重新给非mut的变量赋值会导致编译时错误 而使用let关键字声明的同名新变量，也是不可变的 使用let声明的同名新变量，它的类型可以与之前不同 ","date":"2023-10-11T17:02:40+08:00","permalink":"https://x-xkang.com/p/rust%E5%AD%A6%E4%B9%A0%E5%9B%9B--%E9%80%9A%E7%94%A8%E7%BC%96%E7%A8%8B%E6%A6%82%E5%BF%B5/","title":"Rust学习（四） -- 通用编程概念"},{"content":" 知识点： let、match 等方法 相关的函数 外部的crate 猜数游戏\u0026ndash;目标 生成一个 1 到 100 间的随机数 提示玩家输入一个猜测数字 猜完之后，程序会提示猜测是太大了还是太小了 如果猜测正确，那么打印出一个庆祝信息，程序退出 如果猜测错误，则会继续提示错误信息，直到猜测正确 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 use std::io; // prelude use rand::Rng; // trait 接口 use std::cmp::Ordering; // 枚举类型 fn main() { println!(\u0026#34;猜数游戏\u0026#34;); // 宏 let secret_number = rand::thread_rng().gen_range(1..101); // 包括1，不包括101 loop { println!(\u0026#34;猜测一个数字:\u0026#34;); let mut guess = String::new(); // mut 可变变量 io::stdin().read_line(\u0026amp;mut guess).expect(\u0026#34;failed to read line\u0026#34;); // shadow， 使用相同变量名进行覆盖 let guess: u32 = guess.trim().parse().expect(\u0026#34;please type a number!\u0026#34;); // 字符串转整数类型 println!(\u0026#34;猜测的数字是： {}\u0026#34;, guess); match guess.cmp(\u0026amp;secret_number) { Ordering::Less =\u0026gt; println!(\u0026#34;Too small\u0026#34;), Ordering::Greater =\u0026gt; println!(\u0026#34;Too big\u0026#34;), Ordering::Equal =\u0026gt; { println!(\u0026#34;You win!\u0026#34;); break; } } } } ","date":"2023-09-28T16:31:03+08:00","permalink":"https://x-xkang.com/p/rust%E5%AD%A6%E4%B9%A0%E4%B8%89--%E7%8C%9C%E6%95%B0%E6%B8%B8%E6%88%8F/","title":"Rust学习（三）-- 猜数游戏"},{"content":"Cargo Cargo是Rust的构建系统和包管理工具\n构建代码、下载依赖的库、构建这些库\u0026hellip; 安装Rust的时候会安装Cargo\n执行cargo --version 查看是否安装 使用Cargo 创建项目 执行 cargo new hello_cargo 1 cargo new hello_cargo 项目名称也是 hello_cargo 创建新目录 hello_cargo Cargo.toml src 目录 main.rs 初始化了一个新的Git仓库, .gitnore 可以使用其他的VCS或不使用VCS： cargo new 的时候使用 --vcs 这个 flag Cargo.toml toml (Tom\u0026rsquo;s Obvious,Minimal Language)格式，是Cargo的配置格式 name: 项目名 version: 项目版本 edition: 使用的Rust版本 [dependencies]：另一个区域的开始\n它会列出项目的依赖项 在Rust里面，代码的包称作crate\nscr/main.rs cargo 生成的 main.rs 在 src 目录下 而Cargo.toml在项目根目录下 源代码都应该在src目录下 项目根目录下可以放置： README、许可信息、配置文件和其他与程序源码无关的文件 如果创建项目时没有使用cargo，也可以把项目转化为使用 cargo: 把源代码文件移动到src下 创建 Cargo.toml 并填写相应的配置 构建 Cargo 项目 cargo build\n创建可执行文件： target/debug/hello_cargo 或target\\debug\\hello_cargo.exe (Windows) 运行可执行文件： ./target/debug/hello_cargo 或 .\\target\\debug\\hello_cargo.exe (Windows) 第一次运行 cargo build 会在顶层目录生成 cargo.lock 文件\n该文件负责追踪项目依赖的精确版本 不需要手动修改该文件 构建和运行 cargo 项目 cargo run, 编译代码 + 执行结果 如果之前编译成功过，并且源码没有修改，那么就会直接运行二进制文件 cargo check cargo check，检查代码，确保能通过编译，但是不产生任何可执行文件 cargo check 要比 cargo build 快的多 编写代码的时候可以连续反复的使用cargo check 检查代码，提高效率 为发布构建 cargo build \u0026ndash;release 编译时会进行优化 代码会运行的更快，但是编译时间更长 会在target/release 而不是 target/debug 生成可执行文件 尽量使用 Cargo","date":"2023-09-28T15:03:17+08:00","permalink":"https://x-xkang.com/p/rust%E5%AD%A6%E4%B9%A0%E4%BA%8C--%E5%88%9D%E8%AF%86cargo/","title":"Rust学习（二）-- 初识Cargo"},{"content":"为什么要用Rust Rust 是一种令人兴奋的新编程语言，他可以让每个人编写可靠且搞笑的软件。 它可以用来替换 C/C++,Rust 和它们具有同样的性能，但是很多常见的bug在编译时就可以被消灭 Rust是一种通用的编程语言，但是它们更善于以下场景： 需要运行时的速度 需要内存安全 更好的利用多核处理器 与其他语言比较 C/C++性能非常好，但类型系统和内存都不太安全。 Java/C#，拥有GC，能保证内存安全，也有很多优秀特性，但是性能不行。 Rust: 安全 无需GC 易于维护、调试、代码安全高效 Rust擅长的领域 高性能 web-service WebAssembly 命令行工具 网络编程 嵌入式设备 系统编程 Rust 和Firefox Rust 最初是Mozilla公司的一个研究性项目。FIrefox是Rust产品应用的一个重要的例子。 Mozilla一直以来都在用Rust创建一个名为Servo的实验性浏览器引擎，其中的所有内容都是并行执行的。 目前Servo的部分功能已经被集成到Firefox里面了 Firefox原来的量子版就包含了Serve的CSS渲染引擎 Rust使得Firefox在这方面得到了巨大的性能改进 Rust 的用户和案例 Google：新操作系统Fuschia，其中Rust代码量大约占30% Amazon: 基于Linux开发的直接可以在裸机、虚拟机上运行容器的操作系统 System76: 纯Rust开发了下一代安全操作系统Redox 蚂蚁金服：库操作系统Occlum 斯坦福和密歇根大学：嵌入式实时操作系统，应用于Google的加密产品 微软： 正在使用Rust重写Windows系统中的一些低级组件 微软：WinRT/Rust项目 Dropbox、Yelp、Coursera、LINE、Cloudflare、Atlassian、npm、Ceph、百度、华为、Sentry、Deno\u0026hellip; Rust 的优点 高性能 安全性 无所畏惧的并发 Rust 的缺点 学习曲线陡峭 【注意】\nRust有很多独有的概念。它们和现在大多主流语言不同，所以学习Rust必须从基础概念一步一步学，否则会懵。 安装Rust 安装：https://www.rust-lang.org/\nLinux or Mac:\ncurl https://sh.rustup.rs -sSf | sh Windows : 按官网指示操作\nWindows Subsystem for Linux(WSL):\ncurl \u0026ndash;proto \u0026lsquo;=https\u0026rsquo; \u0026ndash;tlsv1.2 -sSf https://sh.rustup.rs | sh ","date":"2023-09-28T11:02:54+08:00","permalink":"https://x-xkang.com/p/rust%E5%AD%A6%E4%B9%A0%E4%B8%80--%E5%9F%BA%E6%9C%AC%E4%BA%86%E8%A7%A3/","title":"Rust学习（一）-- 基本了解"},{"content":"二叉树遍历方式有三种\n前序遍历 先遍历根节点，再遍历左子树，再遍历右子树\n中序遍历 先遍历左子树，再遍历根节点，最后遍历右子树\n后序遍历 先遍历左子树，再遍历右子树，再遍历根节点\n其实不难发现，遍历方式是根据根节点遍历的顺序来定义的\n演示代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 package main type TreeNode struct { Left *TreeNode Right *TreeNode Val int } func main(){ root := \u0026amp;TreeNode{ Left: \u0026amp;TreeNode{ Left: \u0026amp;TreeNode{Val: 4}, Right: \u0026amp;TreeNode{Val: 5}, Val: 2, }, Right: \u0026amp;TreeNode{ Val: 3, Left: \u0026amp;TreeNode{Val: 6}, Right: \u0026amp;TreeNode{Val: 7, }, }, Val: 1, } preorder(root) } // 前序遍历 func preorder(root *TreeNode){ if root == nil { return } fmt.Println(\u0026#34;vav:\u0026#34;, root.Val) preorder(root.Left) preorder(root.Right) } // 中序遍历 func inorder (root *TreeNode) { if root == nil { return } inorder(root.Left) fmt.Println(\u0026#34;inorder:\u0026#34;, root.Val) inorder((root.Right)) } // 后序遍历 func postorder(root *TreeNode) { if root == nil { return } postorder(root.Left) postorder(root.Right) fmt.Println(\u0026#34;postorder:\u0026#34;, root.Val) } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 前序遍历输出： val: 1 val: 2 val: 4 val: 5 val: 3 val: 6 val: 7 中序遍历输出： inorder: 4 inorder: 2 inorder: 5 inorder: 1 inorder: 6 inorder: 3 inorder: 7 后序遍历输出： postorder: 4 postorder: 5 postorder: 2 postorder: 6 postorder: 7 postorder: 3 postorder: 1 ","date":"2023-09-22T10:33:53+08:00","permalink":"https://x-xkang.com/p/%E4%BA%8C%E5%8F%89%E6%A0%91%E9%81%8D%E5%8E%86/","title":"二叉树遍历"},{"content":"设计原理 Go 语言中最常见的、也是经常被人提及的设计模式就是：不要通过共享内存的方式进行通信，而是应该通过通信的方式共享内存。在很多主流的编程语言中，多个线程传递数据的方式一般都是共享内存，为了解决线程竞争，我们需要限制同一时间能够读写这些变量的线程数量，然而这与 Go 语言鼓励的设计并不相同。\n多线程使用共享内存传递数据\n虽然我们在 Go 语言中也能使用共享内存加互斥锁进行通信，但是 Go 语言提供了一种不同的并发模型，即通信顺序进程（Communicating sequential processes，CSP）1。Goroutine 和 Channel 分别对应 CSP 中的实体和传递信息的媒介，Goroutine 之间会通过 Channel 传递数据\nGoroutine 使用 Channel 传递数据\n上图中的两个 Goroutine，一个会向 Channel 中发送数据，另一个会从 Channel 中接收数据，它们两者能够独立运行并不存在直接关联，但是能通过 Channel 间接完成通信。\n先入先出 目前的 Channel 收发操作均遵循了先进先出的设计，具体规则如下：\n先从 Channel 读取数据的 Goroutine 会先接收到数据； 先向 Channel 发送数据的 Goroutine 会得到先发送数据的权利； 无锁管道 锁是一种常见的并发控制技术，我们一般会将锁分成乐观锁和悲观锁，即乐观并发控制和悲观并发控制，无锁（lock-free）队列更准确的描述是使用乐观并发控制的队列。乐观并发控制也叫乐观锁，很多人都会误以为乐观锁是与悲观锁差不多，然而它并不是真正的锁，只是一种并发控制的思想。\n悲观并发控制与乐观并发控制\n乐观并发控制本质上是基于验证的协议，我们使用原子指令 CAS（compare-and-swap 或者 compare-and-set）在多线程中同步数据，无锁队列的实现也依赖这一原子指令。\nChannel 在运行时的内部表示是 runtime.hchan，该结构体中包含了用于保护成员变量的互斥锁，从某种程度上说，Channel 是一个用于同步和通信的有锁队列，使用互斥锁解决程序中可能存在的线程竞争问题是很常见的，我们能很容易地实现有锁队列。\n然而锁导致的休眠和唤醒会带来额外的上下文切换，如果临界区过大，加锁解锁导致的额外开销就会成为性能瓶颈。1994 年的论文 Implementing lock-free queues 就研究了如何使用无锁的数据结构实现先进先出队列，而 Go 语言社区也在 2014 年提出了无锁 Channel 的实现方案，该方案将 Channel 分成了以下三种类型：\n同步Channel - 不需要缓冲区，发送方会直接将数据交给（Handoff）接收方； 异步Channel - 基于环形缓存的传统生产者消费者模型； chan struct{} 类型的异步Channel - struct{}类型不占用内存空间，不需要实现缓冲区和直接发送(Handoff)的语义。 这个提案的目的也不是实现完全无锁的队列，只是在一些关键路径上通过无锁提升 Channel 的性能。社区中已经有无锁 Channel 的实现，但是在实际的基准测试中，无锁队列在多核测试中的表现还需要进一步的改进。\n数据结构 源码位置：src/runtime/chan.go#L33\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 type hchan struct { qcount uint // total data in the queue， 队列中的元素数量 dataqsiz uint // size of the circular queue， 底层循环数组的长度 buf unsafe.Pointer // points to an array of dataqsiz elements， 指向底层循环数组的指针，只针对有缓冲区的 channel elemsize uint16 // channel 中的元素大小 closed uint32 // channel是否被关闭的标识 elemtype *_type // element type ，channel中元素类型 sendx uint // send index，已发送元素在循环数组中的索引 recvx uint // receive index，已接收元素在数组中的索引 recvq waitq // list of recv waiters，等待接收的 `goroutine` 队列 sendq waitq // list of send waiters，等待发送的 `goroutine` 队列 // lock protects all fields in hchan, as well as several // fields in sudogs blocked on this channel. // // Do not change another G\u0026#39;s status while holding this lock // (in particular, do not ready a G), as this can deadlock // with stack shrinking. lock mutex // 保护 channel 中的所有字段 } 字段解释 qcount: 队列中的元素数量\ndataqsiz: 底层循环数组的长度\nbuf: 指向底层循环数组的指针，只针对有缓冲区的 channel\nelemsize: channel 中的元素数据类型大小\nclosed: channel是否被关闭的标识\nelemtype: channel中的元素类型\nsendx: 已发送元素在循环数组中的索引\nrecvx: 已接收元素在数组中的索引\nrecvq: 等待接收的goroutine 队列\nsendq: 等待发送的goroutine 队列\nlock: 保护channel 中所有字段，保证每个读或者写channel都是原子的。\nsendq 和 recvq 存储了当前Channel由于缓冲区空间不足二阻塞的 Goroutine 列表，这些等待队列使用双向链表runtime.waitq表示，结构如下：\n1 2 3 4 type waitq struct { first *sudog last *sudog } runtime.sudog 表示一个在等待列表中的Goroutine，该结构中存储了两个分别指向前后runtime.sudog的指针以构成链表。\n创建管道 语法如下：\n1 2 3 4 5 // 无缓冲通道 ch1 := make(chan int) // 有缓冲通道 ch2 := make(chan int, 2); // 创建一个缓冲区长度为2，元素类型为 int 的`channel`，若未指定缓冲区长度，则默认为0 Go 语言中所有Channel的床架都会使用 make关键字。编译器会将make(chan int, 10)表达式转换成 OMAKE类型的节点，并在类型检查阶段将OMAKE类型的节点转换成OMAKECHAN类型:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 func typecheck1(n *Node, top int) (res *Node) { switch n.Op { case OMAKE: ... switch t.Etype { case TCHAN: l = nil if i \u0026lt; len(args) { // 带缓冲区的异步 Channel ... n.Left = l } else { // 不带缓冲区的同步 Channel n.Left = nodintconst(0) } n.Op = OMAKECHAN } } } 这一阶段会对传入的make关键字的缓冲区大小进行检查，如果我们不向make传递表示缓冲区大小参数，那么就会设置一个默认值0，也就是当前的Channel不存在缓冲区。\nOMAKECHAN类型的节点最终都会在SSA中间代码生成阶段之前被转换成调用runtime.makechan或者runtime.makechan64函数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func walkexpr(n *Node, init *Nodes) *Node { switch n.Op { case OMAKECHAN: size := n.Left fnname := \u0026#34;makechan64\u0026#34; argtype := types.Types[TINT64] if size.Type.IsKind(TIDEAL) || maxintval[size.Type.Etype].Cmp(maxintval[TUINT]) \u0026lt;= 0 { fnname = \u0026#34;makechan\u0026#34; argtype = types.Types[TINT] } n = mkcall1(chanfn(fnname, 1, n.Type), n.Type, init, typename(n.Type), conv(size, argtype)) } } runtime.makechan 和 runtime.makechan64 会根据传入的参数类型和缓冲区大小创建一个新的 Channel 结构，其中后者用于处理缓冲区大小大于 2 的 32 次方的情况，因为这在 Channel 中并不常见，所以我们重点关注 runtime.makechan：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 func makechan(t *chantype, size int) *hchan { elem := t.elem mem, _ := math.MulUintptr(elem.size, uintptr(size)) var c *hchan switch { case mem == 0: c = (*hchan)(mallocgc(hchanSize, nil, true)) c.buf = c.raceaddr() case elem.kind\u0026amp;kindNoPointers != 0: c = (*hchan)(mallocgc(hchanSize+mem, nil, true)) c.buf = add(unsafe.Pointer(c), hchanSize) default: c = new(hchan) c.buf = mallocgc(mem, elem, true) } c.elemsize = uint16(elem.size) c.elemtype = elem c.dataqsiz = uint(size) return c } 上述代码根据 Channel 中收发元素的类型和缓冲区的大小初始化 runtime.hchan 和缓冲区：\n如果当前Channel中不存在缓冲区，那么就只会为runtime.hchan分配一段内存空间； 如果当前Channel中存储的类型不是指针类型，会为当前的Channel和底层的数组分配一块连续的内存空间； 在默认情况下会单独为runtime.hchan和缓冲区分配内存。 在函数的最后会统一更新runtime.hchan的elemsize、elemtype和datasize几个字段。\n源码位置：src/runtime/chan.go#L72\n发送数据 我们想要向Channel发送数据时，就需要使用 ch \u0026lt;- i语句，编译器会将它解析成OSEND节点并在cmd/compile/internal/gc.walkexpr中转换成runtime.chansend1:\n1 2 3 4 5 6 7 8 9 10 func walkexpr(n *Node, init *Nodes) *Node { switch n.Op { case OSEND: n1 := n.Right n1 = assignconv(n1, n.Left.Type.Elem(), \u0026#34;chan send\u0026#34;) n1 = walkexpr(n1, init) n1 = nod(OADDR, n1, nil) n = mkcall1(chanfn(\u0026#34;chansend1\u0026#34;, 2, n.Left.Type), nil, init, n.Left, n1) } } chansend1只是调用了runtime.chansend并传入Channel和需要发送的数据。chansend是向Channel 中发送数据时一定会调用的函数，该函数包含了发送数据的全部逻辑，如果我们在调用时将block参数设置成true,那么表示当前发送操作是阻塞的。 源码位置：src/runtime/chan.go#L160\n1 2 3 4 5 6 7 func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool { lock(\u0026amp;c.lock) if c.closed != 0 { unlock(\u0026amp;c.lock) panic(plainError(\u0026#34;send on closed channel\u0026#34;)) } 在发送数据的逻辑执行之前会先为当前 Channel 加锁，防止多个线程并发修改数据。如果 Channel 已经关闭，那么向该 Channel 发送数据时会报 “send on closed channel” 错误并中止程序。\n因为runtime.chansend函数的实现比较复杂，所以我们将该函数的执行过程分为以下三部分：\n当存在等待的接收者，通过runtime.send直接将数据发送给阻塞的接收者； 当缓冲区存在空余空间时，将发送的数据写入Channel的缓冲区； 当不存在缓冲区或者缓冲区已满，等待其他 Goroutine从Channel接收数据。 直接发送\n如果目标Channel没有被关闭并且已经有处于读等待的Goroutine,那么runtime.chansend会从接收队列recvq中取出最先陷入等待的Goroutine并直接向它们发送数据：\n1 2 3 4 if sg := c.recvq.dequeue(); sg != nil { send(c, sg, ep, func() { unlock(\u0026amp;c.lock) }, 3) return true } 发送数据时会调用runtime.send，该函数的执行可以分为两个部分：\n调用runtime.sendDirect将发送的数据直接拷贝到x = \u0026lt;-c表达式中变量x所在的内存地址上； 调用runtime.goready将等待接收数据的Goroutine标记成可运行状态Grunnable并把该Goroutine放到发送方所在的处理器runnext上等待执行，该处理器在下一次调度时会立刻唤醒数据的接收方； 1 2 3 4 5 6 7 8 9 10 func send(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int) { if sg.elem != nil { sendDirect(c.elemtype, sg, ep) sg.elem = nil } gp := sg.g unlockf() gp.param = unsafe.Pointer(sg) goready(gp, skip+1) } 需要注意的是，发送数据的过程只是将接收方的Goroutine放到了处理器的runnext中，程序没有立刻执行该Goroutine。\n缓冲区\n如果创建的Channel包含缓冲区并且Channel中的数据没有装满，会执行下面这段代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool { ... if c.qcount \u0026lt; c.dataqsiz { qp := chanbuf(c, c.sendx) typedmemmove(c.elemtype, qp, ep) c.sendx++ if c.sendx == c.dataqsiz { c.sendx = 0 } c.qcount++ unlock(\u0026amp;c.lock) return true } ... } 在这里我们首先会使用 runtime.chanbuf 计算出下一个可以存储数据的位置，然后通过 runtime.typedmemmove 将发送的数据拷贝到缓冲区中并增加 sendx 索引和 qcount 计数器。\n如果当前Channel的缓冲区未满，向Channel发送的数据会存储在Channel的sendx索引所在的位置，并将sendx索引加1，，因为这里的buf是一个循环数组，所以当sendx等于dataqsiz时会重新回到数组开始的位置。\n阻塞发送\n当Channel没有接收者能够处理数据时，向Channel发送数据会被下游阻塞，当然使用select关键字可以向Channel非阻塞的发送消息。向Channel阻塞地发送数据会执行下面的代码。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool { ... if !block { unlock(\u0026amp;c.lock) return false } gp := getg() mysg := acquireSudog() mysg.elem = ep mysg.g = gp mysg.c = c gp.waiting = mysg c.sendq.enqueue(mysg) goparkunlock(\u0026amp;c.lock, waitReasonChanSend, traceEvGoBlockSend, 3) gp.waiting = nil gp.param = nil mysg.c = nil releaseSudog(mysg) return true } 调用 runtime.getg 获取发送数据使用的 Goroutine； 执行 runtime.acquireSudog 获取 runtime.sudog 结构并设置这一次阻塞发送的相关信息，例如发送的 Channel、是否在 select 中和待发送数据的内存地址等； 将刚刚创建并初始化的 runtime.sudog 加入发送等待队列，并设置到当前 Goroutine 的 waiting 上，表示 Goroutine 正在等待该 sudog 准备就绪； 调用 runtime.goparkunlock 将当前的 Goroutine 陷入沉睡等待唤醒； 被调度器唤醒后会执行一些收尾工作，将一些属性置零并且释放 runtime.sudog 结构体； 函数在最后会返回 true 表示这次我们已经成功向 Channel 发送了数据。\n小结\n我们在这里可以简单梳理和总结一下使用 ch \u0026lt;- i 表达式向 Channel 发送数据时遇到的几种情况：\n如果当前 Channel 的 recvq 上存在已经被阻塞的 Goroutine，那么会直接将数据发送给当前 Goroutine 并将其设置成下一个运行的 Goroutine； 如果 Channel 存在缓冲区并且其中还有空闲的容量，我们会直接将数据存储到缓冲区 sendx 所在的位置上； 如果不满足上面的两种情况，会创建一个 runtime.sudog 结构并将其加入 Channel 的 sendq 队列中，当前 Goroutine 也会陷入阻塞等待其他的协程从 Channel 接收数据； 发送数据的过程中包含几个会触发Goroutine调度的时机：\n发送数据时发现 Channel 上存在等待接收数据的 Goroutine，立刻设置处理器的 runnext 属性，但是并不会立刻触发调度; 发送数据时并没有找到接收方并且缓冲区已经满了，这时会将自己加入 Channel 的 sendq 队列并调用 runtime.goparkunlock 触发 Goroutine 的调度让出处理器的使用权； 接收数据 接收数据有两种写法，一种是只值返回接收数据，第二种是返回接收数据和channel的关闭状态两个字段，当接收到响应类型的零值时需要判断是真实的发送者发送的数据，还是channel被关闭后，返回给接收者的默认类型的零值，可以使用第二种返回channel的关闭状态。\n1 2 i := \u0026lt;- ch i, ok := \u0026lt;- ch 这两种不同的方法经过编译器的处理都会变成 ORECV 类型的节点，后者会在类型检查阶段被转换成 OAS2RECV 类型。虽然不同的接收方式会被转换成 runtime.chanrecv1 和 runtime.chanrecv2 两种不同函数的调用，但是这两个函数最终还是会调用 runtime.chanrecv。\n当我们从一个空 Channel 接收数据时会直接调用 runtime.gopark 让出处理器的使用权。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool) { if c == nil { if !block { return } gopark(nil, nil, waitReasonChanReceiveNilChan, traceEvGoStop, 2) throw(\u0026#34;unreachable\u0026#34;) } lock(\u0026amp;c.lock) if c.closed != 0 \u0026amp;\u0026amp; c.qcount == 0 { unlock(\u0026amp;c.lock) if ep != nil { typedmemclr(c.elemtype, ep) } return true, false } ","date":"2023-03-29T11:08:08+08:00","permalink":"https://x-xkang.com/p/golang--channel/","title":"Golang -- Channel"},{"content":"概述 主从复制是指将主数据库的DDL和DML操作通过二进制日志传到从库服务器中，然后从库上对这些日志重新执行（也叫重做），从而使得从库和主库的数据保持同步。\nMySQL支持一台主库同时向多台从库进行复制，从库同时也可以作为其他从服务器的主库，实现链状复制。\nMySQL复制的优点主要包括一下三个方面：\n主库出现问题，可以快速切换到从库提供服务。 实现读写分离，降低主库的访问压力。 可以在从库中执行备份，以避免备份期间影响主库服务。 原理 MySQL主从复制的原理如下：\n从上图来看，复制分成3步：\nMaster主库在事务提交时，会把数据变更记录在二进制日志文件中binlog中。 从库读取主库的二进制日志文件binlog，写入到从库的中继日志relay log。 Slave重做中继日志中的事件，将改变反应到它自己的数据上。 ","date":"2023-03-23T09:46:34+08:00","permalink":"https://x-xkang.com/p/mysql--%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/","title":"Mysql -- 主从复制"},{"content":" 介绍\n二进制日志（BINLOG）记录了所有的DDL（数据定义语言）语句和DML（数据操纵语言）语句，但不包括数据查询（SELECT、SHOW）语句。\n作用：\n灾难时的数据恢复； MySQL的主从复制。在MySQL8.x版本中，默认二进制日志是开启着的。涉及到的参数如下 日志格式\nMySQL服务器中提供了多种格式来记录二进制日志，具体格式及特点如下：\n日志格式 含义 STATEMENT 基于SQL语句的日志记录，记录的是SQL语句，对数据进行修改的SQL都会记录在日志文件中 ROW 基于行的日志记录，记录的是每一行的数据变更。（默认） MIXED 混合了STATEMENT 和ROW两种格式，默认采用STATEMENT，在某些特殊条件下回自动切换为ROW进行记录 1 show variables like \u0026#39;%binlog_format%\u0026#39;; 日志查看\n由于日志是以二进制方式存储的，不能直接读取，需要通过二进制日志查询工具 mysqlbinlog 来查看，具体语法如下：\nmysqlbinlog [参数选项] logfilename\n参数选项： -d 指定数据库名称，只列出指定的数据库相关操作。 -o 忽略掉日志中的 n 行命令。 -v 将行事件（数据变更）重构为SQL语句\n日志删除\n对于比较繁忙的业务系统，每天生成的binlog数据巨大，如果长时间不清除，将会占用大量磁盘空间，可以通过一下几种方式清理日志：\n指令 含义 reset master 删除全部 binlog 日志，删除之后日志编号将从 binlog.000001重新开始 purge master logs to 'binlog.******' 删除 ****** 编号之前的所有日志 purge master logs before 'yyyy-mm-dd hh24:mi:ss' 删除日志为 yyyy-mm-dd hh24:mi:ss 之前产生的所有日志 注：也可以在mysql配置文件中配置二进制日志的过期时间，设置了之后，二进制日志过期将会自动删除。\n1 show variables like \u0026#39;%binlog_expire_logs_seconds%\u0026#39;; ","date":"2023-03-22T20:50:30+08:00","permalink":"https://x-xkang.com/p/mysql--binlog/","title":"Mysql -- Binlog"},{"content":" 介绍\n触发器是与表有关的数据库对象，指在insert/update/delete之前或之后，触发并执行触发器中定义的SQL语句集合，触发器的这种特性可以协助应用在数据库端确保数据的完整性、日志记录、数据校验等操作。 使用别名OLD和NEW来引用触发器中发生变化的记录内容，这与其他的数据库是相似的。现在触发器还只支持行级触发，不支持语句级触发。\n触发器类型 NEW 和 OLD INSERT 型触发器 NEW 表示将要或者已经新增的数据 UPDATE 型触发器 OLD 表示修改之前的数据，NEW 表示将要或者已经修改的数据 DELETE 型触发器 OLD 表示将要或者已经删除的数据 语法\n创建 1 2 3 4 5 6 CREATE TRIGGER trigger_name BEFORE/AFTER INSERT/UPDATE/DELETE ON tb_name FOR EACH ROW -- 行级触发器 BEGIN trigger_stmt; -- 触发器语句 END; 查看 1 SHOW TRIGGERS; 删除 1 DROP TRIGGER [schema_name.]trigger_name; -- 如果没有指定 schema_name，默认为当前数据库。 ","date":"2023-03-20T15:27:27+08:00","permalink":"https://x-xkang.com/p/mysql--%E8%A7%A6%E5%8F%91%E5%99%A8-trigger/","title":"Mysql -- 触发器 Trigger"},{"content":" 介绍\n存储过程是事先经过编译并存储在数据库中的一段SQL语句的集合，调用存储过程可以简化应用开发人员的很多工作，减少数据在数据库和应用服务器之间的传输，对于提高数据处理的效率是有好处的。 存储过程思想上很简单，就是数据库SQL语言层面的代码封装与重用。\n特点\n封装、复用 可以接收参数，也可以返回数据 减少网络交互，提升效率 创建存储过程\n1 2 3 4 5 6 CREATE PROCEDURE 存储过程名称([参数列表]) BIGEN -- SQL语句 END; 调用 1 CALL 名称([参数]); 查看存储过程 1 2 3 4 5 -- 查询指定数据库的存储过程及状态信息 SELECT * FROM INFORMATION_SCHEMA.ROUTINES WHERE ROUTINE_SCHEMA = \u0026#39;XXX\u0026#39;; -- 查询某个存储过程的定义 SHOW CREATE PROCEDURE 存储过程名称; 删除存储过程 1 DROP PROCEDURE [IF EXISTS] 存储过程名称; 注意：在命令行中，执行创建存储过程的SQL时，需要通过关键字delimiter指定SQL语句的结束符\n1 delimiter $$; 变量 系统变量是MySQL服务器提供，不是用户定义的，属于服务器层面，分为全局变量(GLOBAL)、会话变量(SESSION)。\n查看系统变量 1 2 3 4 5 SHOW [SESSION | GLOBAL] VARIABLES; -- 查看所有系统变量 SHOW [SESSION | GLOBAL] VARIABLES LIKE \u0026#39;...\u0026#39;; -- 可以通过 like 模糊匹配方式查找变量 SELECT @@[SESSION | GLOBAL] 系统变量名; -- 查看指定变量的值 设置系统变量 1 2 3 SET [SESSION | GLOBAL] 系统变量名=值； SET @@[SESSION | GLOBAL] 系统变量名=值; 【待续。。。】\n","date":"2023-03-10T22:35:13+08:00","permalink":"https://x-xkang.com/p/mysql--%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B/","title":"Mysql -- 存储过程"},{"content":" 介绍\n视图（View）是一种虚拟存在的表，视图中经的数据并不在数据库中实际存在，行和列数据来自定义视图的查询中使用的表，并且是在使用视图时动态生成的。 通俗的讲，视图只保存了查询的SQL逻辑，不保存查询结果。所以我们在创建视图的时候，主要的工作就落在创建这条SQL查询语句上。\n作用\n简单 视图不仅可以简化用户对数据的理解，也可以简化他们的操作。那些被经常使用的查询可以被定义为视图，从而使得用户不必为以后的操作每次指定全部的条件\n安全 数据库可以授权，但不能授权到数据库特定行和特定列上。通过视图用户只能查询和修改他们所能见到的数据。\n数据独立 视图可以帮助用户屏蔽真实表结构变化带来的影响。\n语法如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # 创建视图 CREATE OR REPLACE VIEW view_u AS SELECT id, username, password FROM user WHERE id \u0026lt;= 100; # 查询视图 # 1. 查看创建视图语句 SHOW CREATE VIEW view_u; # 2. 查看视图数据 SELECT * FROM VIEW view_u; # 修改视图 # 方式一 # CREATE [OR REPLACE] VIRE 视图名称[(列名列表)] AS SELECT语句 [WITH[CASCADED | LOCAL] CHECK OPTION] CREATE OR REPLACE VIEW view_u AS SELECT id, username FROM `user` WHERE id \u0026lt;= 10; # 方式二 # ALTER VIEW 视图名称[(列名列表)] AS SELECT 语句 [WITH[CASCADED | LOCAL] CHECK OPTION] ALTER VIEW view_u AS SELECT id, username FROM `user` WHERE id \u0026lt;= 20; # 删除视图 # DROP VIEW [IF EXISTS] 视图名称 [, 视图名称]... DROP VIEW IF EXISTS view_u; 视图的检查选项 当使用WITH CHECK OPTION 子句创建视图时，MySQL会通过视图检查正在更改的每个行，例如插入、更新、删除，以使其符合视图的定义，MySQL允许基于另一个视图创建视图，它还会检查依赖视图中的规则以保持一致性，为了确定检查的范围，mysql提供了两个选项：CASCADED 和 LOCAL，默认值为CASCADED。\n1 2 3 4 5 # 基于 tbale 创建视图 create or replace view view_u as select id, username from `user` where id \u0026lt;= 10 with cascaded check option; # 基于 view 创建视图 create or replace view view_u_2 as select id, username from `view_u` where id \u0026lt;= 10 with cascaded check option; cascaded:在创建视图时若使用 cascaded关键字，插入或更新数据时不仅会校验当前视图的约束条件，也会校验关联的父视图和父视图引用的视图，以此类推。若使用local关键字，则只会校验当前视图的约束条件。\n视图的更新 要使视图可更新，视图中的行与基础表中的行之间必须存在一对一的关系。如果视图包含以下任何一项，则该视图不可更新：\n聚合函数或窗口函数(SUM(), MIN(), MAX(), COUNT() 等) DISTINCT GROUP BY HAVING UNION 或者 UNION ALL ","date":"2023-03-08T07:36:27+08:00","permalink":"https://x-xkang.com/p/mysql--view%E8%A7%86%E5%9B%BE/","title":"MySQL -- view视图"},{"content":"插入数据 批量插入 1 insert into `user` values(\u0026#39;user_1\u0026#39;, \u0026#39;password_1\u0026#39;), (\u0026#39;user_2\u0026#39;, \u0026#39;password_2\u0026#39;); 手动提交事务 1 2 3 4 5 6 7 8 9 start transaction; insert into `user` values(\u0026#39;user_1\u0026#39;, \u0026#39;password_1\u0026#39;), (\u0026#39;user_2\u0026#39;, \u0026#39;password_2\u0026#39;); insert into `user` values(\u0026#39;user_3\u0026#39;, \u0026#39;password_3\u0026#39;), (\u0026#39;user_4\u0026#39;, \u0026#39;password_4\u0026#39;); insert into `user` values(\u0026#39;user_5\u0026#39;, \u0026#39;password_5\u0026#39;), (\u0026#39;user_6\u0026#39;, \u0026#39;password_6\u0026#39;); commit; 主键顺序插入 1 2 # 主键乱序插入： 8 1 9 21 88 2 4 15 89 5 7 3 # 主键顺序插入： 1 2 3 4 5 7 8 9 15 21 88 89 大批量插入数据 如果一次性插入大批量数据，使用insert语句插入性能较低，此时可以使用MySQL数据库提供的load指令进行插入。操作如下：\n1 2 3 4 5 6 7 8 9 # 客户端连接服务端时，加上参数 --local-infile mysql --local-infile -u root -p #设置全局参数 local_infile为1，开启本地加在文件导入数据的开关 set global local_infile = 1; # 执行load指令将准备好的数据，加在到表结构中 load data local infile \u0026#39;/root/sql1.log\u0026#39; into table `user` fields terminated by \u0026#39;,\u0026#39; lines terminated by \u0026#39;\\n\u0026#39;; # 主键优化 数据组织优化\n在InnoDB存储引擎中，表数据都是根据主键顺序组织存放的，这种存储方式的表称为索引组织表(index organized table IOT)。\n页分裂\n页可以为空，也可以填充一半，也可以填充100%，每个页包含了2-N行数据（如果一行数据过大，会行溢出），根据主键排列。\n乱序插入时，若插入数据在当前页存储不下时，会将当前页的一半数据移动至新的数据页，然后将待插入数据存储至当前页，并将当前页的下一页指针指向新开辟的数据页。\n页合并\n当删除一行数据时，实际上记录并没有被物理删除，只是记录被标记（flaged）为删除并且它的空间变的允许被其他记录声明使用。当页中删除的记录大奥 MERGE_THRESHOLD(默认为页的50%)，Innodb会开始寻找最靠近的页（前或后）看看是否可以将两个页合并以优化空间使用。\n主键设计原则\n满足业务需求的情况下，尽量降低主键的长度。 插入数据时，尽量选择顺序插入，选择使用 AUTO_INCREMENT自增主键。 尽量不要使用UUID做主键或者是其他自然主键，如身份证号。 ORDER BY 优化 Using filesort: 通过表的索引或全表扫描，读取满足条件的数据行，然后在排序缓冲区 sort buffer 中完成排序操作，所有不是通过索引值直接返回排序结果的排序都叫 FileSort 排序。 Using index: 通过有序索引顺序扫描直接返回有序数据，这种情况即为 using index，不需要额外排序，操作效率高。\nGROUP BY 优化 在分组操作时，可以通过索引来提高效率。 分组操作时，索引的使用也是满足最左前缀法则的。 LIMIT 优化 一个常见又头疼的问题就是limit 2000000, 10,此时需要MySQL排序前2000010记录，仅仅返回2000000 - 2000010的记录，其他记录丢弃，查询排序的代价非常大。\n优化思路：一般分页查询时，通过创建覆盖索引能够较好地提高性能，可以通过覆盖索引加子查询形式进行优化。\n优化前：\n1 select * from user limit 2000000, 10; 优化后：\n1 select u.* from user u, (select id from user order by id limit 2000000, 10) a where u.id = a.id; COUNT 优化 MyISAM 引擎把一个表的总行数存在了磁盘上，因此执行count(*)的时候会直接返回这个数，效率很高； Innodb 引擎就麻烦了，它执行count(*)的时候，需要把数据一行一行地从引擎中读出来，然后累计计数。 优化思路：自己计数。（基于内存的k-v缓存）\ncount 的几种用法 count()是一个聚合函数，对于返回的结果集，一行行的判断，如果count函数的参数不是NULL，累计值就加 1，否则不加，最后返回累计值。 用法：count(*), count(主键), count(字段), count(1) count(主键) Innodb引擎会遍历整张表，把每一行的主键ID值都取出来，返回给服务层。服务层拿到主键后，直接按行进行累加（主键不可能为null）。 count(字段) 没有 not null 约束：Innodb引擎会遍历整张表把每一行的字段值都取出来，返回给服务层，判断值是否为null， 不为null，计数累加。 有 not null 约束：Innodb引擎会遍历正常标把每一行的字段都取出来，返回给服务层，直接按行进行累加。 UPDATE 优化 1 update student set no = \u0026#39;2000100100\u0026#39; where id = 1; 1 update student set no = \u0026#39;2000100105\u0026#39; where name = \u0026#39;韦一笑\u0026#39;; Innodb的行锁是针对索引加的锁，不是针对记录加的锁，并且该索引不能失效，否则会从行锁升级为表锁。\n","date":"2023-03-06T20:44:07+08:00","permalink":"https://x-xkang.com/p/mysql--sql%E4%BC%98%E5%8C%96/","title":"MySQL -- SQL优化"},{"content":"最左前缀法则 如果索引了多列（联合索引），要遵守最左前缀法则。最左前缀法则指的是查询从索引的最左列开始，并且不跳过索引中的列。如果跳过某一列，索引将部分失效（后面的字段索引失效）。 假设user表有username varchar(50)、phone varchar(20)、email varchar(50)三个字段组成的联合索引idx_user_username_phone_email\n按照最左前缀法则有以下情况\n使用索引最左侧字段进行条件查询，命中索引，且索引长度为152，表示只用到索引中的username字段\n1 2 3 4 5 6 7 mysql\u0026gt; explain select * from `user` where `username` = \u0026#39;Bob\u0026#39;; +----+-------------+-------+------------+------+-------------------------------+-------------------------------+---------+-------+------+----------+-------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+-------------------------------+-------------------------------+---------+-------+------+----------+-------+ | 1 | SIMPLE | user | NULL | ref | idx_user_username_phone_email | idx_user_username_phone_email | 152 | const | 1 | 100.00 | NULL | +----+-------------+-------+------------+------+-------------------------------+-------------------------------+---------+-------+------+----------+-------+ 使用索引中的全部三个字段，命中索引 且key_len为366，说明三个字段全部命中\n1 2 3 4 5 6 7 mysql\u0026gt; explain select * from `user` where `username` = \u0026#39;Bob\u0026#39; and `phone`=\u0026#39;18888888888\u0026#39; and `email`=\u0026#39;1888888888@163.com\u0026#39;; +----+-------------+-------+------------+------+-------------------------------+-------------------------------+---------+-------------------+------+----------+-------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+-------------------------------+-------------------------------+---------+-------------------+------+----------+-------+ | 1 | SIMPLE | user | NULL | ref | idx_user_username_phone_email | idx_user_username_phone_email | 366 | const,const,const | 1 | 100.00 | NULL | +----+-------------+-------+------------+------+-------------------------------+-------------------------------+---------+-------------------+------+----------+-------+ 使用 username和email查询，结果 命中索引，但只有username字段是有效的，因为按照最左前缀原则，如果跳过中间列(phone)，则索引将部分失效\n1 2 3 4 5 6 7 mysql\u0026gt; explain select * from `user` where `username` = \u0026#39;Bob\u0026#39; and `email`=\u0026#39;1888888888@163.com\u0026#39;; +----+-------------+-------+------------+------+-------------------------------+-------------------------------+---------+-------+------+----------+-----------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+-------------------------------+-------------------------------+---------+-------+------+----------+-----------------------+ | 1 | SIMPLE | user | NULL | ref | idx_user_username_phone_email | idx_user_username_phone_email | 152 | const | 1 | 10.00 | Using index condition | +----+-------------+-------+------------+------+-------------------------------+-------------------------------+---------+-------+------+----------+-----------------------+ 索引失效情况 使用索引中的第二列phone和第三列email查询，未命中索引，因为查询条件中的字段没有从索引的最左列开始（最左前缀法则）\n1 2 3 4 5 6 7 mysql\u0026gt; explain select * from `user` where `phone`=\u0026#39;18888888888\u0026#39; and `email`=\u0026#39;1888888888@163.com\u0026#39;; +----+-------------+-------+------------+------+---------------+------+---------+------+-------+----------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+---------------+------+---------+------+-------+----------+-------------+ | 1 | SIMPLE | user | NULL | ALL | NULL | NULL | NULL | NULL | 99677 | 1.00 | Using where | +----+-------------+-------+------------+------+---------------+------+---------+------+-------+----------+-------------+ 使用模糊匹配进行查询，未命中索引\n1 2 3 4 5 6 7 mysql\u0026gt; explain select * from `user` where `phone` like \u0026#39;188%\u0026#39;; +----+-------------+-------+------------+------+---------------+------+---------+------+-------+----------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+---------------+------+---------+------+-------+----------+-------------+ | 1 | SIMPLE | user | NULL | ALL | NULL | NULL | NULL | NULL | 99677 | 11.11 | Using where | +----+-------------+-------+------------+------+---------------+------+---------+------+-------+----------+-------------+ 条件语句中使用函数，截取用户名username的前缀进行查询，未命中索引\n1 2 3 4 5 6 7 mysql\u0026gt; explain select * from `user` where substring(username, 1, 3) = \u0026#39;dev\u0026#39;; +----+-------------+-------+------------+------+---------------+------+---------+------+-------+----------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+---------------+------+---------+------+-------+----------+-------------+ | 1 | SIMPLE | user | NULL | ALL | NULL | NULL | NULL | NULL | 99677 | 100.00 | Using where | +----+-------------+-------+------------+------+---------------+------+---------+------+-------+----------+-------------+ 字符串不加单引号''，只命中了最左侧的索引字段 未加单引号的字段 未命中索引\n1 2 3 4 5 6 7 mysql\u0026gt; explain select * from user where username = \u0026#39;bob\u0026#39; and phone = 18895766335; +----+-------------+-------+------------+------+-------------------------------+-------------------------------+---------+-------+------+----------+-----------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+-------------------------------+-------------------------------+---------+-------+------+----------+-----------------------+ | 1 | SIMPLE | user | NULL | ref | idx_user_username_phone_email | idx_user_username_phone_email | 152 | const | 1 | 10.00 | Using index condition | +----+-------------+-------+------------+------+-------------------------------+-------------------------------+---------+-------+------+----------+-----------------------+ OR 作为连接的条件，用OR分开的条件，如果OR前的条件中的列有索引，而后面的列中没有索引，那么涉及的索引都 不会命中。\n1 2 3 4 5 6 7 mysql\u0026gt; explain select * from user where id = 10 or phone = \u0026#39;18895766335\u0026#39;; +----+-------------+-------+------------+------+---------------+------+---------+------+-------+----------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+---------------+------+---------+------+-------+----------+-------------+ | 1 | SIMPLE | user | NULL | ALL | PRIMARY | NULL | NULL | NULL | 99677 | 10.00 | Using where | +----+-------------+-------+------------+------+---------------+------+---------+------+-------+----------+-------------+ 数据分布影响 如果MySQL评估使用索引比全表更慢，则不使用索引。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 结果是使用索引的，因为记录中没有为null的数据，相对整体记录数占比较小，但如果字段属性加了 `not null` 的限制，不管查询条件是is null 还是 is not null，都将不会使用索引 mysql\u0026gt; explain select * from user where username is null; +----+-------------+-------+------------+------+-------------------------------+-------------------------------+---------+-------+------+----------+-----------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+-------------------------------+-------------------------------+---------+-------+------+----------+-----------------------+ | 1 | SIMPLE | user | NULL | ref | idx_user_username_phone_email | idx_user_username_phone_email | 153 | const | 1 | 100.00 | Using index condition | +----+-------------+-------+------------+------+-------------------------------+-------------------------------+---------+-------+------+----------+-----------------------+ # 查询条件为 not null 时没有使用索引。 mysql\u0026gt; explain select * from user where username is not null; +----+-------------+-------+------------+------+-------------------------------+------+---------+------+-------+----------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+-------------------------------+------+---------+------+-------+----------+-------------+ | 1 | SIMPLE | user | NULL | ALL | idx_user_username_phone_email | NULL | NULL | NULL | 99484 | 50.00 | Using where | +----+-------------+-------+------------+------+-------------------------------+------+---------+------+-------+----------+-------------+ SQL提示 SQL提示，是优化数据库的一个重要手段，简单来说，就是在SQL语句中加入一些人为的提示来达到优化操作的目的。\nuse index 1 explain select * from user use index(idx_user_username_phone_email) where username = \u0026#39;Bob\u0026#39;; ignore index 1 explain select * from user ignore index(idx_user_username_phone_email) where username=\u0026#39;Bob\u0026#39;; force index 1 explain select * from user force index(idx_user_username_phone_email) where username=\u0026#39;Bob\u0026#39;; 覆盖索引 尽量使用覆盖索引（查询使用了覆盖索引，并且需要返回的列，在该索引中已经全部能够找到），减少select *。\n1 2 3 4 # select 中的字段在索引中都可以直接查到，因此不用回表，若查询字段使用`*`， # 或者加入不在索引中的字段,比如`password`，查询需要先通过联合索引查到二级索引值， # 取出主键ID，根据主键ID再回表查询出`password`字段值。 explain select id, username, email, `password` from user where username = \u0026#39;user_99999\u0026#39;; 前缀索引 当字段类型为字符串（varchar, text等）时，有时候需要索引很长的字符串，这会让索引变的很大，查询时，浪费大量的磁盘IO，影响查询效率。此时可以只将字符串的一部分前缀，建立索引，这样可以大大节约索引空间，从而提高索引效率。\n语法： 1 create index idx_tb_column on tb(column(n)); 前缀长度 可以根据索引的选择性来决定，而选择性是指不重复的索引值（基数）和数据表的记录总数的比值，索引选择性越高则查询效率越高，唯一索引的选择性是1，这是最好的索引选择性，性能也是最好的。可根据一下结果做参考：\n1 2 3 select count(distinct email) / count(*) from user; select count(distinct substring(email, 15)) / count(*) from user; 单列索引和联合索引 单列索引：即一个索引值包含单个列 联合索引：即一个索引包含了多个列 😀 提示： 在业务场景中，如果存在多个查询条件，考虑对查询字段建立索引时，建议建立联合索引，而非单列索引。\n索引设计原则 1.针对数据量较大，且查询比较频繁的表建立索引。 2.针对常作为查询条件（WHERE）、排序（ORDER BY）、分组（GROUP BY）操作的字段建立索引。 3.尽量选择区分度高的列作为索引，尽量建立唯一索引，区分度越高，使用索引的频率越高。 4.如果是字符串类型的字段，字段的长度较长，可以针对字段的特点，建立前缀索引。 5.尽量使用联合索引，减少单列索引，查询时，联合索引很多时候可以覆盖索引，节省存储空间，避免回表，提高查询效率。 6.要控制索引的数量，索引并不是多多益善，索引越多，维护索引结构的代价就越大，会影响增删改的效率。 7.如果索引列不能存储NULL值，请在创建表时使用NOT NULL约束它。当优化器知道每列是否包含NULL值时，它可以更好的确定哪个索引最有效的用于查询。 ","date":"2023-03-01T12:06:34+08:00","permalink":"https://x-xkang.com/p/mysql--%E7%B4%A2%E5%BC%95%E4%BD%BF%E7%94%A8%E8%A7%84%E5%88%99/","title":"MySQL -- 索引使用规则"},{"content":"查看SQL执行频率 MySQL 客户端连接成功后，通过show[session|global] status 命令可以提供服务器状态信息。通过如下指令可以查看当前数据库的INSERT、UPDATE、DELETE、SELECT的访问频次：\n1 SHOW GLOBAL STATUS LIKE \u0026#39;Com_______\u0026#39;; # 一个 \u0026#39;_\u0026#39; 代表一个字符 结果如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 mysql\u0026gt; show global status like \u0026#39;Com_______\u0026#39;; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | Com_binlog | 0 | | Com_commit | 0 | | Com_delete | 0 | | Com_import | 0 | | Com_insert | 0 | | Com_repair | 0 | | Com_revoke | 0 | | Com_select | 8 | | Com_signal | 0 | | Com_update | 0 | | Com_xa_end | 0 | +---------------+-------+ 11 rows in set (0.00 sec) 查询慢日志 慢查询日志记录了所有执行时间超过指定参数（long_query_time，单位：秒，默认10秒）的所有SQL语句的日志。MySQL的慢查询日志默认没有开启，执行一下SQL查看慢日志开启状态：\n1 SHOW VARIABLES LIKE \u0026#39;slow_query_log\u0026#39;; 需要在MySQL的配置文件（/etc/my.cnf）中配置如下信息：\n1 2 3 4 5 # 开启MySQL慢日志查询开关 slow_query_log=1 # 设置慢日志的时间为2秒，SQL语句执行时间炒锅2秒就会视为慢查询，记录慢查询日志 long_query_time=2 重启MySQL服务，本机环境为win10的WSL2，直接执行以下命令：\n1 sudo service mysql restart profile详情 show profiles 能够在做SQL优化时帮助我们了解时间都耗费到哪去了。通过have_profiling参数，能够看到当前MySQL是否支持profile操作：\n1 2 # 查看是否支持 SELECT @@have_profiling; 默认profiling是关闭的，可以通过set语句在session/global级别开启profiling:\n1 2 3 4 5 # 查看开关状态 SELECT @@profiling; # 设置开关状态 SET profiling = 1; 执行完查询语句后再执行show profiles，结果如下：\n1 2 3 4 5 6 7 8 9 +----------+------------+------------------------+ | Query_ID | Duration | Query | +----------+------------+------------------------+ | 1 | 0.00025800 | select @@profiling | | 2 | 0.00125875 | show tables | | 3 | 0.00014975 | select * form user | | 4 | 0.01059300 | select * from user | | 5 | 0.00152175 | select * from user_log | +----------+------------+------------------------+ explain 执行计划 EXPLAIN 或者 DESC 命令获取MySQL如何执行SELECT语句的信息，包括在SELECT语句执行过程中表如何连接和连接的顺序。语法如下：\n1 2 3 # 直接在select语句之前加上关键字 explain/desc EXPLAIN SELECT 字段列表 FROM 表名 WHERE 条件; 结果如下：\n1 2 3 4 5 6 7 mysql\u0026gt; explain select * from user; +----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+ | 1 | SIMPLE | user | NULL | ALL | NULL | NULL | NULL | NULL | 1 | 100.00 | NULL | +----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+ 1 row in set, 1 warning (0.00 sec) EXPLAIN 输出列如下：\nColumn JSON Name Meaning id select_id SELECT 标识符 select_type None SELECT 类型 table table_name 输出行table partitions partitions 匹配的分区 type access_type 连接类型 possible_keys possible_keys 可能的索引选择 key key 实际选择的索引 key_len key_length 所选键的长度 ref ref 与索引比较的列 rows rows 估计要检查的行 filtered filtered 按table条件过滤的行百分比 Extra None 扩展信息 😀 Note 详细解释可参考 [官方文档]\nid 行标识，如果没有子查询或者联合查询，这个值是1，id列值越大越先执行，如果一样大，那么就从上往下依次执行\nselect_type 查询的类型，可以是下表的任何一种类型\nselect_type 类型说明 SIMPLE 简单SELECT（不使用UNION或子查询） PROMARY 最外层的SELECT UNION UNION中第二个或之后的SELECT语句 DEPENDENT UNION UNION中第二个或之后的SELECT语句取决于外面的查询 UNION RESULT UNION的结果 SUBQUERY 子查询中的第一个SELECT DEPENDENT SUBQUERY 子查询中的第一个SELECT，取决于外面的查询 DERIVED 衍生表（FROM自居中的子查询） MATERIALIZED 物化子查询 UNCACHEABLE SUBQUERY 结果集无法缓存的子查询，必须重新评估外部查询的每一行 UNCACHEABLE UNION UNION中第二个或之后的SELECT，属于无法缓存的子查询 ⭐ DEPENDENT 意味着使用了关联子查询\ntable：输出行所引用表的名称，也可以是以下值之一：\n\u0026lt;unionM,N\u0026gt;: 引用id为M和N UNION之后的结果。 : 引用id为N的结果派生出的表，派生表可以是一个结果集，例如派生自FROM中子查询的结果。 : 引用id为N的子查询结果物化得到的表，即生成一个临时表保存子查询的结果。 partitions：v5.7之前该项是explain partitions显示的选项，v5.7之后成为了默认选项，该列显示的为分区表中的分区情况，非分区表该字段为空（null）。\ntype( 重要 )：连接类型\nsystem 表中只有一行数据或者是空表，这是const类型的一个特例，且只能用于MyISAM和Memory表，如果是InnoDB引擎表，type列在这个情况通常都是all或者index\nconst 最多只有一行记录匹配。当联合主键或唯一索引的所有字段跟常量值比较时，join类型为const。其他数据库也叫做唯一索引扫描\neq_ref 多表join时，对于来自前面表的每一行，在当前表中只能找到一行，这可能是除了system和const之外最好的类型，当主键或唯一非NULL索引的所有字段都被当做join连接时会出现此类型。 eq_ref可用于使用\u0026rsquo;=\u0026lsquo;操作符作比较的索引列，比较的值可以是常量，也可以是使用此表之前读取的表的列表达式。\nref 对于来自前面表的每一行，在此表的索引中可以匹配到多行，若连接只用到索引的最左前缀或索引不是主键或唯一索引时，使用 ref 类型（也就是说，此类型能够匹配到多行连接）。 ref 可用于使用 = 或 \u0026lt;=\u0026gt; 操作符作比较的索引列。\nfulltext 用于全文索引的时候是这个类型，要注意，全文索引的优先级很高，若全文索引和普通索引同时存在，mysql不管代价，优先选择使用全文索引\nref_or_null 类似于ref类型，只是增加了null值得比较，实际用的不多\nindex_merge 此类型表示查询使用两个以上索引，最后取交集或者并集，常见 and, or的条件使用了不同的索引，官方排序这个在ref_or_null之后，但是实际上由于要读取多个索引，性能可能大部分时间都不如range。\nunique_subquery 用于WHERE中的 IN 形式子查询，子查询返回不重复值唯一值，可以完全替换子查询，效率更高。 该类型替换了下面形式的 IN 子查询的 ref: value IN (select primary_key FROM single_table WHERE some_expr)\nindex_subquery 该连接类型类似于unique_subquery。适用于非唯一索引，可以返回重复值\nrange 索引范围查询，常见于使用= \u0026lt;\u0026gt; \u0026gt; \u0026gt;= \u0026lt; \u0026lt;= IS NULL \u0026lt;=\u0026gt; BETWEEN IN 或者 LIKE 等运算符的查询中。\nindex 索引全表扫描，把索引从头到尾扫一遍。这里包含两种情况： 一种是查询使用了覆盖索引，那么它只需要扫描索引就可以获得数据，这个效率要比全表扫描要快，因为索引通常比数据表小，而且还能避免二次查询。在 extra 中显示 Using index，反之，如果在索引上进行全表扫描，没有 Using index的提示。\nall 全表扫描，性能较差\npossible_keys：查询可能用到的索引，如果此列是 NULL，则没有用到索引。\nkey：表示MySQL实际决定使用的索引，如果MySQL决定使用其中一个 possible_keys 索引来查找行，则该索引将作为键值列出。\nkey_length：查询用到的索引长度（字节）， 如果是单列索引，那就整个索引长度算进去，如果是多列索引，那么查询不一定都能使用到所有的列，用多少算多少。例如组合索引为idx_username_create_time，当查询语句为 select * from user where username='qqq'时，只用到了username字段，那么key_len只会计算username字段占用的字节数，假如username字段类型为varchar(50) NOT NULL，字符编码为utf8mb3（utf8编码一个字符占用3个字节，gbk编码一个字符占2字节），记录字符串长度占用2字节，记录字段不为空占用1字节（若没有NOT NULL限制则不占用这1个字节）那么key_len为 50 * 3 + 2 + 1 = 153字节\nref：如果使用的是常数等值查询，这里会显示const，如果是连接查询，被驱动表的执行计划这里会显示驱动表的关联字段，如果是条件使用了表达式或者函数，或者条件列表发生了内部隐式转换，这里可能显示为func。\nrows( 重要 ):rows也是一个重要的字段，这是MySQL估算的需要扫描的行数（不是精确值）。这个值非常直观显示SQL的效率好坏，原则上rows越少越好。\nfiltered：该字段表示存储引擎返回的数据在server层过滤后，剩下多少满足查询的记录数量的比例，注意是百分比，不是具体记录数。\nExtra( 重要 )：EXPLAIN中很多额外的信息会在Extra字段显示，常见的有以下：\ndistinct: 在select部分使用了distinct关键字 Using filesort: 当Extra中有Using filesort时，表示MySQL需额外的排序操作，不能通过索引顺序到达排序效果，一般有 Using filesort 都建议优化去掉，因为这样的查询CPU资源消耗较大。 Using index: “覆盖索引扫描”，表示查询在索引树种就可查找所需数据，不用扫描表数据文件，往往说明性能不错。 Using temporary: 查询有使用临时表，一般出现于排序，分组和多表join的情况，查询效率不高，建议优化。 ","date":"2023-02-28T10:29:46+08:00","permalink":"https://x-xkang.com/p/mysql--%E7%B4%A2%E5%BC%95%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/","title":"MySQL -- 索引性能分析"},{"content":"概念 约束是作用于表中字段行的规则，用于限制存储在表中的数据\n目的 保证数据库中数据的正确性、有效性和完整性\n分类 约束 描述 关键字 非空约束 限制该字段的数据不能为null NOT NULL 唯一约束 保证该字段的所有数据都是唯一、不重复的 UNIQUE 主键约束 主键是一行数据的唯一标识，要求非空且唯一 PRIMARY KEY 默认约束 保存数据时，如果未指定该字段的值，则采用默认值 DEFAULT 检查约束（v8.0.16之后） 保证字段值满足某一个条件 CHECK 外键约束 用来让两张表的数据之间建立连接，保证数据的一致性和完整性 FOREIGN KEY 演示 1 2 3 4 5 create table `user_info` ( `id` int(11) not null primary key auto_increment comment \u0026#39;主键ID\u0026#39;, # 非空约束和主键约束 `phone` varchar(32) not null unique default \u0026#39;\u0026#39; comment \u0026#39;手机号码\u0026#39;, # 非空约束和唯一约束 `nickname` varchar(255) default \u0026#39;user\u0026#39; comment \u0026#39;昵称\u0026#39; # 默认约束 ); ","date":"2023-02-28T10:24:41+08:00","permalink":"https://x-xkang.com/p/mysql--%E7%BA%A6%E6%9D%9F/","title":"MySQL -- 约束"},{"content":"内连接 内连接查询的是两张表交集的部分\n隐式内连接 1 SELECT 字段列表 FROM 表1, 表2 WHERE 查询条件; 显式内连接 1 SELECT 字段列表 FROM 表1 [INNER] JOIN 表2 ON 连接条件; 外连接 左外连接 相当于查询表1（左表）的所有数据包含表1和表2交集部分的数据\n1 SELECT 字段列表 FROM 表1 LEFT [OUTER] JOIN 表2 ON 条件; 右外连接 相当于查询表2（右表）的所有数据 包含 表1和表2交集部分的数据\n自连接 自连接查询，可以是内连接查询，也可以是外连接查询。\n1 SELECT 字段列表 FROM 表A 别名A JOIN 表A 别名B ON 条件; 联合查询-union, union all 对于union查询，就是把多次查询的结果并起来，形成一个新的查询结果集。\n1 2 3 SELECT 字段列表 FROM 表A UNION [ALL] SELECT 字段列表 FROM 表B; 子查询 SQL语句中嵌套SELECT语句，称为嵌套查询，又称子查询。\n1 SELECT * FROM 表1 WHERE 字段1=(SELECT 字段1 FROM 表2); 子查询外部的语句可以是 INSERT / UPDATE / DELETE / SELECT 的任何一个。\n根据子查询结果不同，分为：\n标量子查询（子查询结果为单个值） 列子查询（子查询结果为一列） 行子查询（子查询结果为一行） 表子查询（子查询结果为多行多列） 根据子查询位置，分为：WHERE之后、FROM之后、SELECT之后。\n子查询 \u0026ndash; 标量子查询 标量子查询返回的结果是单个值（数字、字符串、日期等），最简单的形式，这种子查询称为标量子查询。 常用的操作符： = \u0026lt;\u0026gt; \u0026gt; \u0026gt;= \u0026lt; \u0026lt;=\n1 SELECT 字段列表 FROM 表1 WHERE 字段1 = (SELECT 字段2 FROM 表2 WHERE 条件); 子查询 \u0026ndash; 列子查询 子查询返回的结果是一列（可以是多行），这种子查询称为列子查询。 常用的操作符：IN NOT IN ANY SOME ALL\n操作符 描述 IN 在指定的集合范围之内，多选一 NOT IN 不在指定的集合范围内 ANY 子查询返回列表中，有任意一个满足即可 SOME 与ANY等同，使用SOME的地方都可以使用ANY ALL 子查询返回列表的所有值都必须满足 1 SELECT 字段列表 FROM 表1 WHERE 字段1 IN (SELECT 字段2 FROM 表2 WHERE 条件); 子查询 \u0026ndash; 行子查询 子查询返回的结果是一行（可以是多列），这种查询称为行子查询。\n常用的操作符：= \u0026lt;\u0026gt; IN NOT IN\n1 2 # 子查询返回一行数据，可以有多个字段 SELECT 字段列表 FROM 表1 WHERE (字段1， 字段2) = (SELECT 字段3， 字段4 FROM 表2 WHERE 表2); 子查询 \u0026ndash; 表子查询 子查询返回的结果是多行多列，这种子查询称为表子查询。 常用的操作符：IN\n1 2 # 子查询返回的是多行多列数据，字段1和字段2分别匹配子查询结果各行中的字段3和字段4 SELECT 字段列表 FROM 表1 WHERE 字段1, 字段2 IN (SELECT 字段3， 字段4 FROM 表2); ","date":"2023-02-28T10:12:34+08:00","permalink":"https://x-xkang.com/p/mysql--%E5%A4%9A%E8%A1%A8%E6%9F%A5%E8%AF%A2/","title":"MySQL -- 多表查询"},{"content":"排序算法可分为内部排序和外部排序，内部排序是数据记录在内存中进行排序，而外部排序是因为排序的数据很大，一次不能容纳全部的排序记录，在排序过程中需要访问外存。常见的内部排序算法有：插入排序、希尔排序、选择排序、冒泡排序、归并排序、快速排序、堆排序、基数排序等。\n排序算法 平均时间复杂度 最好情况 最坏情况 空间复杂度 排序方式 稳定性 冒泡排序 O(n^2^) O(n) O(n^2^) O(1) In-place 稳定 选择排序 O(n^2^) O(n^2^) O(n^2^) O(1) In-place 不稳定 插入排序 O(n^2^) O(n) O(n^2^) O(1) In-place 稳定 希尔排序 O(n log n) O(n log^2^ n) O(n log^2^ n) O(1) In-place 不稳定 归并排序 O(n log n) O(n log n) O(n log n) O(n) Out-place 稳定 快速排序 O(n log n) O(n log n) O(n^2^) O(log n) In-place 不稳定 堆排序 O(n log n) O(n log n) O(n log n) O(1) In-place 不稳定 计数排序 O(n + k) O(n + k) O(n + k) O(k) Out-place 稳定 冒泡排序 算法步骤 比较相邻的元素，如果第一个比第二个大，那么交换他们两个，对每一对相邻的元素做相同的操作，从开始第一对到结尾的最后一对，这一步做完后，最后一个元素会是最大的数， 针对所有元素重复衣裳步骤，除了最后一个，\n代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 func BubbleSort(arr []int) []int { length := len(arr) for i := 0; i \u0026lt; length; i++ { for j := 0; j \u0026lt; length-i-1; j++ { if arr[j] \u0026gt; arr[j+1] { arr[j], arr[j+1] = arr[j+1], arr[j] } } } return arr } 选择排序 算法步骤 首先在未排序序列中找到最小（大）的元素，存放到序列的起始位置。 再从剩余未排序的未排序元素中继续寻找最小（大）的元素，然后放到已排序序列的末尾。 重复第二步，直到所有元素均排序完毕。\n代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 func SelectSort(arr []int) []int { length := len(arr) var minIndex int // var temp int for i := 0; i \u0026lt; length-1; i++ { minIndex = i for j := i + 1; j \u0026lt; length; j++ { if arr[j] \u0026lt; arr[minIndex] { minIndex = j } } arr[i], arr[minIndex] = arr[minIndex], arr[i] } return arr } 插入排序 算法步骤 将第一待排序序列第一个元素看做一个有序序列，把第二个元素到最后一个元素当成是未排序序列。 从头到尾依次扫描未排序序列，将扫描到的每个元素插入有序序列的适当位置。（如果待插入的元素与有序序列中的某个元素相等，则将待插入元素插入到相等元素的后面。）\n代码实现 1 2 3 4 5 6 7 8 9 10 11 12 func InsertionSort(arr []int) []int { for i := range arr { preIndex := i - 1 current := arr[i] for preIndex \u0026gt;= 0 \u0026amp;\u0026amp; arr[preIndex] \u0026gt; current { arr[preIndex+1] = arr[preIndex] preIndex -= 1 } arr[preIndex+1] = current } return arr } 希尔排序 算法步骤 选择一个增量序列 t1，t2，……，tk，其中 ti \u0026gt; tj, tk = 1； 按增量序列个数 k，对序列进行 k 趟排序； 每趟排序，根据对应的增量 ti，将待排序列分割成若干长度为 m 的子序列，分别对各子表进行直接插入排序。仅增量因子为 1 时，整个序列作为一个表来处理，表长度即为整个序列的长度\n代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func shellSort(arr []int) []int { length := len(arr) gap := 1 for gap \u0026lt; length/3 { gap = gap*3 + 1 } for gap \u0026gt; 0 { for i := gap; i \u0026lt; length; i++ { temp := arr[i] j := i - gap for j \u0026gt;= 0 \u0026amp;\u0026amp; arr[j] \u0026gt; temp { arr[j+gap] = arr[j] j -= gap } arr[j+gap] = temp } gap = gap / 3 } return arr } 归并排序 算法步骤 申请空间，使其大小为两个已经排序序列之和，该空间用来存放合并后的序列； 设定两个指针，最初位置分别为两个已经排序序列的起始位置； 比较两个指针所指向的元素，选择相对小的元素放入到合并空间，并移动指针到下一位置； 重复步骤 3 直到某一指针达到序列尾； 将另一序列剩下的所有元素直接复制到合并序列尾 代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 func MergeSort(arr []int) []int { length := len(arr) if length \u0026lt; 2 { return arr } middle := length / 2 left := arr[0:middle] right := arr[middle:] return merge(mergeSort(left), mergeSort(right)) } func merge(left []int, right []int) []int { var result []int for len(left) != 0 \u0026amp;\u0026amp; len(right) != 0 { if left[0] \u0026lt;= right[0] { result = append(result, left[0]) left = left[1:] } else { result = append(result, right[0]) right = right[1:] } } for len(left) != 0 { result = append(result, left[0]) left = left[1:] } for len(right) != 0 { result = append(result, right[0]) right = right[1:] } return result } 快速排序 算法步骤 从数列中挑出一个元素，称为 \u0026ldquo;基准\u0026rdquo;（pivot）; 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作； 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序；\n代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 func QuickSort(arr []int) []int { return _quickSort(arr, 0, len(arr)-1) } func _quickSort(arr []int, left, right int) []int { if left \u0026lt; right { partitionIndex := partition(arr, left, right) _quickSort(arr, left, partitionIndex-1) _quickSort(arr, partitionIndex+1, right) } return arr } func partition(arr []int, left, right int) int { pivot := left index := pivot + 1 for i := index; i \u0026lt;= right; i++ { if arr[i] \u0026lt; arr[pivot] { swap(arr, i, index) index += 1 } } swap(arr, pivot, index-1) return index - 1 } func swap(arr []int, i, j int) { arr[i], arr[j] = arr[j], arr[i] } 堆排序 算法步骤 创建一个堆 H[0……n-1]； 把堆首（最大值）和堆尾互换； 把堆的尺寸缩小 1，并调用 shift_down(0)，目的是把新的数组顶端数据调整到相应位置； 重复步骤 2，直到堆的尺寸为 1。 代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 func HeapSort(arr []int) []int { arrLen := len(arr) buildMaxHeap(arr, arrLen) for i := arrLen - 1; i \u0026gt;= 0; i-- { swap(arr, 0, i) arrLen -= 1 heapify(arr, 0, arrLen) } return arr } func buildMaxHeap(arr []int, arrLen int) { for i := arrLen / 2; i \u0026gt;= 0; i-- { heapify(arr, i, arrLen) } } func heapify(arr []int, i, arrLen int) { left := 2*i + 1 right := 2*i + 2 largest := i if left \u0026lt; arrLen \u0026amp;\u0026amp; arr[left] \u0026gt; arr[largest] { largest = left } if right \u0026lt; arrLen \u0026amp;\u0026amp; arr[right] \u0026gt; arr[largest] { largest = right } if largest != i { swap(arr, i, largest) heapify(arr, largest, arrLen) } } func swap(arr []int, i, j int) { arr[i], arr[j] = arr[j], arr[i] } 计数排序 算法步骤 找出待排序的数组中最大和最小的元素 （2）统计数组中每个值为i的元素出现的次数，存入数组C的第i项 （3）对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加） （4）反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1\n代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 func CountingSort(arr []int, maxValue int) []int { bucketLen := maxValue + 1 bucket := make([]int, bucketLen) // 初始为0的数组 sortedIndex := 0 length := len(arr) for i := 0; i \u0026lt; length; i++ { bucket[arr[i]] += 1 } for j := 0; j \u0026lt; bucketLen; j++ { for bucket[j] \u0026gt; 0 { arr[sortedIndex] = j sortedIndex += 1 bucket[j] -= 1 } } return arr } ","date":"2023-02-19T15:15:15+08:00","permalink":"https://x-xkang.com/p/%E5%87%A0%E7%A7%8D%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/","title":"几种常用的排序算法"},{"content":"一、全局锁 对整个数据库实例加锁，加锁后整个数据库实例处于只读状态，后续的DML的写语句，DDL语句已经更新操作的事务提交语句都将阻塞。 其典型的使用场景是做全库的逻辑备份，对所有的表进行锁定，从而获取一致性的视图，保证数据的完整性。 加锁操作：\n1 2 3 4 5 6 7 8 9 10 11 -- 加锁 mysql\u0026gt; flush tables with read lock; -- TODO. 逻辑备份 mysqldump -uroot -p123456 [dbname] \u0026gt; dbname.sql -- TODO. 中间穿插的 insert/update/delete 语句将阻塞 mysql\u0026gt; update user set `password`=\u0026#39;654321\u0026#39; where `id` = 1; -- 释放锁 mysql\u0026gt; unlock tables; 特点：\n数据库中加全局锁是比较重的操作，存在一下问题\n如果在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆。 如果在从库上备份，那么在备份期间从库不能执行从主库同步过来的二进制文件(binlog)，会导致主从延迟。 在Innodb存储引擎中，我们可以再备份时加上参数 \u0026ndash;single-transaction 参数来完成不加锁的一致性数据备份\n1 mysqldump --single-transaction -uroot -p123456 [dbname] \u0026gt; dbname.sql 二、表级锁 介绍 每次操作锁住整张表，锁定力度大，发生锁冲突的概率最高，并发度最低。应用在Innodb,MyIsam，BDB等存储引擎中。\n表级锁主要分为以下几类：\n1. 表锁 表共享读锁 表独占写锁 操作如下：\n1 2 3 4 5 -- 加锁 mysql\u0026gt; lock tables [table_name] read/write; -- 释放锁 mysql\u0026gt; unlock tables; 读锁不会阻塞其他客户端的读操作，但是会阻塞写操作。 写锁既会阻塞其他客户端的读，也会阻塞其他客户端的写。 2. 元数据锁(meta data lock, mdl) MDL加锁过程是系统自动控制的，无需显式使用，在访问一张表的时候会自动加上，主要作用是维护表元数据的数据一致性，在表上有活动事务的时候，不可以对元数据进行写操作，为了避免MDL和DDL的冲突，保证读写的正确性 在MySQL5.5中加入了MDL，当对一张表进行增删改查的时候，加MDL读锁（共享锁），当对表结构进行修改的时候，加MDL写锁。 查看元数据锁\n1 mysql\u0026gt; select object_type, object_schema, object_name, lock_type, lock_duration from performance_schema.metadata_locks; 对应SQL 锁类型 说明 lock tables [t_name] read/write SHARED_READ_ONLY / SHARED_NO_READ_WRITE select、select \u0026hellip; lock in share mode SHARED_READ 与SHARED_READ/SHARED_WRITE兼容，与EXCLUSIVE 互斥 insert、update、delete、select \u0026hellip; for update SHARED_WRITE 与SHARE_READ/ SHARED_WRITE兼容，与EXCLUSIVE 互斥 alter table \u0026hellip; EXCLUSIVE 与其他的 MDL 都互斥 3. 意向锁 为了避免MDL在执行时，加的表锁与行锁冲突，在InnoDB中引入了意向锁，使得表锁不用检查每行数据是否加锁，使用意向锁来减少表锁的检查。\n意向共享锁（IS）：由语句 select \u0026hellip; lock in share mode 添加，与表锁共享锁（read）兼容，与表锁排它锁（write）互斥。 意向排它锁（IX）：由语句 insert、update、delete、select \u0026hellip; for update 添加，与表锁共享锁（read）和表锁互斥锁（write）都互斥。意向锁之间不会互斥。 查看意向锁：\n1 mysql\u0026gt; select object_schema, object_name, index_name, lock_type, lock_mode, lock_data from performance_schema.data_locks; 三、行级锁 每次操作锁住对应的数据行。锁定粒度最小，发生锁冲突的概率最低，并发度最高。应用在Innodb存储引擎中。\nInnodb的数据是鲫鱼索引组织的，行锁是通过对索引上的索引项加锁来实现的，而不是对记录加的锁，对于行级锁，主要分为以下三类：\n1. 行锁 Record Lock：锁定单个记录的锁，防止其他事务对此进行update和delete，在 RC 和 RR 隔离级别下都支持。\nInnodb实现了以下两种类型的行锁： 共享锁（S）：允许一个事务去读一行，阻止其他事务获得相同数据集的排它锁。 排它锁（X）：允许获取排它锁的事务更新数据，阻止其他事务获得相同数据集的共享锁和排它锁。 当前锁类型 \\ 请求锁类型 S（共享锁） X（排它锁） S （共享锁） 兼容 冲突 X （排它锁） 冲突 冲突 不同的SQL使用的锁类型： SQL 行锁类型 说明 INSERT \u0026hellip; 排它锁 自动加锁 UPDATE \u0026hellip; 排它锁 自动加锁 DELETE \u0026hellip; 排它锁 自动加锁 SELECT（正常） 不加任何锁 SELECT \u0026hellip; LOCK IN SHARE MODE 共享锁 需要手动在 SELECT 之后加 LOCK IN SHARE MODE SELECT \u0026hellip; FOR UPDATE 排它锁 需要手动在 SELECT 之后加 FOR UPDATE 行锁-演示： 默认情况下，InnoDB在REPEATABLE READ 事务隔离界别运行，InnoDB 使用next key 锁进行搜索和索引扫描，以防止幻读。\n针对唯一索引进行检索时，对已存在的记录进行等值匹配时，将会自动优化为行锁。 InnoDB的行锁是针对于索引加的锁不通过索引条件检索数据，那么InnoDB将对表中的所有数据进行加锁，此时就会升级为表锁 可以通过一下SQL，查看意向锁以及行锁的加锁情况：\n1 mysql\u0026gt; select object_schema, object_name, index_name, lock_type, lock_mode, lock_data from performance_schema.data_locks; 2. 间隙锁 Gap Lock: 锁定索引记录间隙（不包含该记录），确保索引记录间隙不变，防止其他事务在这个间隙进行insert，产生幻读，在RR隔离级别下都支持。\n3. 临键锁 Next-Key Lock: 行锁和间隙锁组合，同时锁住数据，并锁住数据前面的间隙Gap。在RR隔离级别下支持。\n4. 间隙锁/临键锁 演示 默认情况下，InnoDB在Repeatable read 事务隔离级别运行，InnoDB使用 Next-key 锁进行搜索和索引扫描，以防止幻读。\n索引上的等值查询（唯一索引），给不存在的记录加锁时优化为间隙锁。 索引上的等值查询（普通索引），向右遍历时最后一个值不满足查询需求时，next-key lock 退化为间隙锁。 索引上的范围查询（唯一索引），会访问到不满足条件的第一个值为止。 注意：间隙锁的唯一目的是防止其他事务插入间隙，间隙锁可以共存，一个事务采用的间隙锁不会阻止另一个事务在同一间隙上使用间隙锁 ","date":"2022-12-16T17:54:33+08:00","permalink":"https://x-xkang.com/p/mysql--%E9%94%81%E5%88%86%E6%9E%90/","title":"MySQL -- 锁分析"},{"content":"环境准备 OS: win10 docker engine: v20.10.14 创建MySQL服务容器 创建 master 节点服务 编辑数据库配置文件，将配置文件以及数据存储目录挂载到宿主机上 master节点配置文件mysql-master.cnf如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 [mysqld] # 定义服务id server-id=1 #启用二进制日志 log-bin=mysql-bin # 设置不要复制的数据库(可设置多个) # binlog-ignore-db=information_schema # 设置需要复制的数据库 需要复制的主数据库名字 binlog-do-db=testdb #设置logbin格式 binlog_format=STATEMENT # 开启gtid enforce-gtid-consistency=on gtid-mode=on 创建master节点服务的容器mysql-master 1 docker run -d -p 3310:3306 -v D:\\workspace\\docker-volumes\\mysql\\master\\mysql-master.cnf:/etc/mysql/conf.d/mysql-master.cnf -v D:\\workspace\\docker-volumes\\mysql\\master\\data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 --name mysql-master mysql:5.7 创建 slave 节点服务 创建配置文件 mysql-slave-1.cnf 如下： 1 2 3 4 5 6 7 8 [mysqld] # 设置 server-id 注意不要与 master 节点重复 server-id=2 binlog-format=STATEMENT relay-log=mysql-relay gtid-mode=ON enforce-gtid-consistency=true read-only=1 创建 slave 节点服务 1 docker run -d -p 3311:3306 -v D:\\workspace\\docker-volumes\\mysql\\slave-1\\mysql-slave-1.cnf:/etc/mysql/conf.d/mysql-slave-1.cnf -v D:\\workspace\\docker-volumes\\mysql\\slave-1\\data\\:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 --name mysql-slave-1 mysql:5.7 创建同步数据使用的用户 进入master节点 1 docker exec -it mysql-master mysql -uroot -p123456 创建用户，记住此时创建的用户密码 1 2 CREATE USER \u0026#39;slave\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;123456\u0026#39;; GRANT REPLICATION SLAVE ON *.* TO \u0026#39;slave\u0026#39;@\u0026#39;%\u0026#39;; 查看 master 节点状态，在master 节点执行 1 mysql\u0026gt; show master status; File Position Binlog_Do_DB Binlog_Ignore_DB Executed_Gtid_Set mysql-bin.000002 157 testdb mysql,information_schema 记住File和Position，后面关联主从节点的时候会用\n查看 master 节点的IP地址，关联主从节点的时候要用 1 docker inspect mysql-master 进入 slave 节点 1 docker exec -it mysql-slave-1 mysql -uroot -p123456 执行SQL关联主从复制节点\n1 2 3 4 5 6 7 mysql\u0026gt; CHANGE MASTER TO MASTER_HOST=\u0026#39;172.17.0.2\u0026#39;, # master 节点IP MASTER_PORT=3306, # master 节点端口 MASTER_USER=\u0026#39;slave\u0026#39;, # 上面创建的主从同步用户 MASTER_PASSWORD=\u0026#39;123456\u0026#39;, # 用户密码 MASTER_LOG_FILE=\u0026#39;mysql-bin.000003\u0026#39;, # master 节点的日志文件 MASTER_LOG_POS=0; 开启 slave，\n1 2 mysql\u0026gt; start slave; Query OK, 0 rows affected, 1 warning (0.03 sec) 查看 slave开启状态\n1 mysql\u0026gt; show slave status\\G; 创建连接用户并授权 进入 master 节点，执行 1 2 3 4 5 # 创建用户 CREATE USER \u0026#39;test\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;123456\u0026#39;; # 授权 grant select,insert,update,delete on testdb.* to \u0026#39;test\u0026#39;@\u0026#39;%\u0026#39;; [注]测试过程中不要使用root账号测试从库的read-only权限，因为拥有 super 权限的账号会忽略限制\n在 master 节点上的testdb 库新建数据表 user 查看 slave 节点数据库，user 已经同步成功，至此mysql 的主从架构集群就已经部署完成，如果想增加 slave节点的话，重复2.2\n[注意]\n关联主从节点时，确保IP和端口是通的，不然在从节点执行 start slave 时，查看结果 show slave status 连接结果是失败的; 测试账号不要用 root 权限，会忽略从节点的读写限制，从节点也一样可以插入、更新、删除数据; slave 节点服务一定要将 read_only 设置成1，可以在.cnf配置文件中配置，也可以在mysql终端中设置set global read_only=1;不然有写权限的账号在 slave 节点也一样可以插入或更新数据; ","date":"2022-12-02T11:52:10+08:00","permalink":"https://x-xkang.com/p/mysql--docker%E9%83%A8%E7%BD%B2%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E6%96%B9%E6%A1%88/","title":"MySQL -- docker部署集群之主从复制方案"},{"content":" 分布式 将一个完整的系统，按照业务功能拆分成一些独立的子系统，它们独立运行在不同的服务器上。\n集群 集群就是单机的多实例，在多个服务器上部署同一个系统，这些服务器的集合叫做集群\n一、单机架构出现的问题 性能问题 单个服务器无论是计算资源还是存储资源都非常有限，当请求量非常大的时候，会出现响应变慢等情况，严重影响用户体验 单体架构的多个服务中，某个服务占用资源多，时间长，导致总体响应显慢 可用性问题 当出现单机故障（进程崩溃、断电、磁盘损坏等）时，整个应用不可用。 开发问题 单体架构下，我们将各个模块放在一个系统中，系统过于庞大臃肿，维护成本高，各个功能模块之间的耦合度偏高，难以针对单个模块进行优化，出现 BUG 较难定位。 交付周期长，所有功能得一起上线，一起构建，一起部署。任何一个环节出错，都可能影响交付。 二、为什么引入分布式 性能提高 相较于单体架构，在请求量非常大的时候，整个系统的响应速度不会有明显的下降，因为每个服务都占有各自独有的服务器资，资源竞争小，任务处理较快 提高可用性 在单体架构中，一旦出现了例如服务器宕机，磁盘损坏等故障问题，会导致整个服务不可用，但分布式架构将各个功能模块拆分独立部署后，即使部分服务器出现故障，也不会影响其他的功能 提高开发效率 1、针对传统的单体架构，必须将整个系统的功能开发完毕才可以上线部署，分布式可以将各个功能模块拆开，独立开发，独立测试，独立部署，同时也降低了系统之间的耦合度，系统与系统之间的边界也会更加清晰 2、服务的复用性更高，系统中某些比较通用的功能拆分出来作为独立的服务部署后，可以作为通用服务调用，不用在每一个系统中开发重复且耗时的功能，比如用户系统，支付系统等 三、为什么引入集群 性能提高 传统的单体架构，在请求量非常大的时候，整个系统的响应速度都会下降，对于集群来说，通过负载均衡将请求分发至集群中的不同节点中，这样可以有效降低每个服务节点的压力。 提高可用性 若集群中的某个节点出现故障时，可将请求转发至其他可用的服务节点上，这样可以保证服务的高可用。 四、分布式与集群的区别 从提升效率的角度比较\n分布式是以缩短单个任务的执行时间来提升效率 集群是以提高单位时间内执行的任务数来提高效率 从服务部署的角度比较\n分布式是将一个大的服务拆分成独立的小的服务并部署在不同的服务器上 集群则是将几台服务器集中在一起提供相同的服务 分布式中的每一个节点都可以做集群，反之则不一定可以 五、分布式与集群的关系 根据分布式的特点可以总结出，其主要作用是将系统模块化，将系统进行解耦，有利于开发和维护，但并不能解决大并发的问题，也无法保证服务在运行期间出现宕机等问题后的正常运转， 而集群刚好弥补了这个缺陷，集群，通俗来说就是多个服务器处理相同的业务，这里可以改善一些并发量大的问题，保证了高性能；另一方面，如果集群中的某个节点出现故障导致不可用，但对于整个集群来说，服务还是可用的，即保证了服务的高可用。 六、分布式集群的优点 系统可用性的提升\n一个系统全年可用时间在99.999%，5个9的服务可用率在设计合理的分布式集群系统中不是一个触不可及的数字 传统的集中式计算或集中式存储在遇见单点故障时很容易造成整个服务的不可用，分布式集群下的服务体系，单台机器有故障，不至于造成整个服务不可用 系统并发能力的提升\n请求通过负载均衡被分发到不同的服务器上，执行同样服务的服务器可以有1台或者N台，通常情况下可以根据用户访问量增加或减少服务器的数量，可以做到随时水平扩展 系统容错能力的提升\n同一组服务器分别部署在不同的城市，若其中一个机房发生故障（火灾或者停电），其他城市的服务器还是可用状态，将故障机房的流量转发到正常的城市机房中，可以有效提高服务的可用性 低延时\n服务器部署在不同城市之后，可以根据用户的IP将请求分发至最近的机房，可以达到降低延时的效果 低耦合\n（低耦度低）系统之间的耦合度大大降低，可以独立开发、独立部署、独立测试，系统与系统之间的边界非常明确，排错也变得相当容易，开发效率大大提升 （扩展性强）系统更易于扩展。我们可以针对性地扩展某些服务，换句话说，就是对子系统进行集群。例如在双十一时，订单子系统、支付子系统需要集群，账户管理子系统不需要集群。 （复用性高）服务的复用性更高。比如，当我们将用户系统作为单独的服务后，该公司所有的产品都可以使用该系统作为用户系统，无需重复开发 七、分布式集群带来的挑战 依赖网络，会因为网络问题导致系统数据丢失或者不一致 服务器之间的通信依赖网络，不可靠网络包括网络延时，丢包，中断，异步，一个完整的请求依赖于一连串的服务调用，任意一个服务节点出现网络故障都可能造成本次服务调用的失败\n系统复杂化，系统监控维护，版本迭代发布变的相对复杂，成本高 传统单体架构的服务只需要维护一个站点即可，分布式服务系统被拆分成若干个小服务，服务从1变成几十上百个服务之后，增加运维成本\n一致性，可用性，分区容错性无法同时满足 这个是最主要的，就是常说的CAP理论，在分布式系统中，这三种特性最多同时满足两种，无法同时满足全部，需要根据实际情况调整策略\n","date":"2022-11-28T11:02:35+08:00","permalink":"https://x-xkang.com/p/%E5%88%86%E5%B8%83%E5%BC%8F%E5%92%8C%E9%9B%86%E7%BE%A4%E7%9A%84%E5%8C%BA%E5%88%AB/","title":"分布式和集群的区别"},{"content":"一、索引的分类 1、根据数据结构可分为 Btree索引（B+tree, b-tree） 哈希索引 full-text 全文索引 2、根据物理存储可分为 聚簇索引 二级索引(辅助索引) 3、根据字段特性可分为 主键索引 普通索引 前缀索引 4、根据字段个数可分为 单列索引 联合索引（符合索引，组合索引） 二、根据数据结构分类 MySQL按数据结构分类可分为：B+tree索引，Hash索引，Full-text索引\n- InnoDB MyIsam Memory B+tree索引 √ √ √ Hash索引 × × √ B-tree索引 √（MySQL5.6+） √ × 注：InnoDB 实际上也支持Hash索引，但是InnoDB中Hash索引的创建由存储引擎自动优化创建，不能人为干预 是否为表创建Hash索引，\nB+tree是MySQL中被存储引擎采用最多的索引类型，B+tree中的B代表平衡（balance），而不是二叉（binary），因为B+tree是从最早的平衡二叉树演化而来的，下面演示B+tree数据结构与其他数据结构的对比。\n1. B+tree 和 B-tree的对比 [网络图片] B-tree示意图\nB+tree非叶子结点只存储键值信息，数据记录都存放在叶子节点中，而B-tree的非叶子节点也存储数据，B+tree单个节点的存储量更小，在相同的磁盘IO情况下可以查到更多节点数据。 B+tree 所有叶子结点之间都采用单链表连接，适合MySQL中常见的基于范围的顺序检索场景，而B-tree无法做到这一点。 [网络图片] B+tree示意图\n2. B+tree 和 红黑树的对比 [网络图片] 红黑树示意图\n红黑树是一种弱平衡二叉查找树，通过任何一条从根到叶子的路径上各个节点着色方式的限制，红黑树确保没有一条路径会比其他路径长出两倍\n对于有N个节点的B+tree，其搜索的时间复杂度为O(logdN)，其中**d(Degree)**为B+tree的度，表示节点允许的最大子节点数为d个，在实际应用当中，d值一般是大于100的，即使数据量达到千万级别时B+tree的高度依然维持在3-4左右，保证了3-4次磁盘I/O操作就能查询到目标数据。\n红黑树是二叉树，节点子节点个数为两个，意味着其搜索的算法时间复杂度为 O(logN)，树的高度也会比 B+tree 高出不少，因此红黑树检索到目标数据所需经历的磁盘I/O次数更多。\n3. B+tree 和 Hash的对比 Hash 索引结构的特殊性，其检索效率非常高，索引的检索可以一次定位，不像B-Tree 索引需要从根节点到枝节点，最后才能访问到页节点这样多次的IO访问，所以 Hash 索引的查询效率要远高于 B-Tree 索引。虽然 Hash 索引效率高，但是 Hash 索引本身由于其特殊性也带来了很多限制和弊端，主要有以下这些。\nHash 索引仅仅能满足 = , IN 和 \u0026lt;=\u0026gt;(表示NULL安全的等价) 查询，不能使用范围查询 由于 Hash 索引比较的是进行 Hash 运算之后的 Hash值，所以它只能用于等值的过滤，不能用于基于范围的过滤，因为经过相应的 Hash算法处理之后的 Hash 值的大小关系，并不能保证和Hash运算前完全一样\nHash 索引无法适用数据的排序操作 由于 Hash 索引中存放的是经过 Hash 计算之后的 Hash值，而且Hash值的大小关系并不一定和 Hash运算前的键值完全一样，所以数据库无法利用索引的数据来避免任何排序运算；\nHash 索引不能利用部分索引键查询 对于组合索引，Hash 索引在计算 Hash 值的时候是组合索引键合并后再一起计算 Hash 值，而不是单独计算 Hash值，所以通过组合索引的前面一个或几个索引键进行查询的时候，Hash 索引也无法被利用\nHash 索引依然需要回表扫描 Hash 索引是将索引键通过 Hash 运算之后，将 Hash运算结果的 Hash值和所对应的行指针信息存放于一个 Hash 表中，由于不同索引键可能存在相同 Hash 值，所以即使取满足某个 Hash 键值的数据的记录条数，也无法从 Hash索引中直接完成查询，还是要通过访问表中的实际数据进行相应的比较，并得到相应的结果。\nHash索引遇到大量Hash值相等的情况后性能并不一定就会比B-Tree索引高 选择性比较低的索引键，如果创建 Hash 索引，那么将会存在大量记录指针信息存于同一个Hash值相关联。这样要定位某一条记录时就会非常麻烦，会浪费多次表数据的访问，而造成整体性能低下\n由于范围查询是MySQL数据库查询中常见的场景，Hash表不适合做范围查询，它更适合做等值查询。另外Hash表还存在Hash函数选择和Hash值冲突等问题。因此，B+tree索引要比Hash表索引有更广的适用场景\n三、根据物理存储分类 MySQL根据叶子节点是否存储的是完整数据将索引分为：聚簇索引，二级索引（辅助索引）。\n1.聚簇索引 聚簇索引的每个叶子节点都存储了一行完整的表数据，叶子节点之间按照id列升序连接，可以方便的进行顺序检索。 [网络图片] MySQL中的InnoDB 存储引擎要求必须有聚簇索引，默认在主键字段上建立聚簇索引，在没有主键的情况下，数据表的第一个非空唯一索引将被建立为聚簇索引， 在前两者都没有的情况下，InnoDB将自动生成一个隐式的自增id列，并在此列上建立聚簇索引。\nMyISAM存储引擎不存在聚簇索引，主键索引和非主键索引的结构是一样的，索引的叶子节点不存储表数据，存放的是表数据的地址，所以MyISAM存储引擎的表可以没有主键。MyISAM表的索引和数据是分开存储的，\n2.二级索引 [网络图片]\n二级索引的叶子节点并不存储一行完整的数据，而是存储了聚簇索引所在的列。\n回表查询：由于二级索引的叶子节点不存储完整的表数据，索引当通过二级索引查询聚簇索引列值后，还需要回到聚簇索引也就是表本身进一步获取数据 [网络图片]\n回表查询需要额外的B+tree搜索过程，必然增加查询消耗, 需要注意的是，通过二级索引查询时，回表不是必须的过程，当select 的所有字段在单个二级索引中都能找到时，就不需要回表，MySQL称此时的二级索引为覆盖索引或者触发了索引覆盖。 可以用explain 命令查看SQL语句的执行计划，执行计划的extra字段中若出现Using index，表示查询触发了索引覆盖。\n四、按字段特性分类 MySQL索引按照字段特性可以分为：主键索引，普通索引，前缀索引\n主键索引 建立在主键上的索引称为主键索引，一张数据表只能有一个主键索引，索引列值不允许有空值，通常在创建表时一起创建。\n唯一索引 建立在 Unique字段上的索引称为唯一索引，一张表可以有多个唯一索引，索引列值允许为空，列值中出现多个空值不会出现重复冲突。\n普通索引 建立在普通字段上的索引被称为普通索引\n前缀索引 前缀索引是指对字符类型字段的前几个字符或对二进制类型字段的前几个bytes建立的索引，而不是在整个字段上建索引。前缀索引可以建立在类型为char、varchar、binary、varbinary的列上，可以大大减少索引占用的存储空间，也能提升索引的查询效率\n","date":"2022-11-26T11:17:45+08:00","permalink":"https://x-xkang.com/p/mysql--%E7%B4%A2%E5%BC%95%E6%B5%85%E6%9E%90/","title":"MySQL -- 索引浅析"},{"content":"Slice的数据结构 切片是对数组的一个连续片段的引用，所以切片是一个引用类型，一个长度可变的数组。 源码中的数据结构定义如下：\n源码位置： src/runtime/slice.go#L22\n1 2 3 4 5 type slice struct { array unsafe.Pointer // 引用的底层数组指针 len int // 切片的长度 cap int // 切片的容量 } 创建切片 创建方式 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 func main() { // 1. 通过关键字 var 声明，此时未初始化，无法对s1[i]进行取值、赋值 var s1 []int // 2. 使用 new() 创建，由于 new()返回的是一个指针，通过 *获取指针的值，未初始化，同样也无法对s2[i]进行取值赋值 s2 := *new([]int) // 3. 直接创建并初始化，已初始化，因此可以对s3[i]进行取值赋值 s3 := []int{1, 2, 3} // 4. make() 创建，第一个参数指定类型，第二个参数为长度，第三个参数为容量（可不指定，默认为长度值） s4 := make([]int, 4, 5) // 5. 通过截取数组或切片 array := [7]int{1, 2, 3, 4, 5, 6, 7} s5 := array[3:5] // (左闭右开)，输出 [4,5]，引用数组 array 的部分片段 } 创建逻辑 src/runtime/slice.go#L88\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // input: et: slice类型元信息，slice长度，slice容量 func makeslice(et *_type, len, cap int) unsafe.Pointer { // 调用MulUintptr函数：获取创建该切片需要的内存，是否溢出(超过2^64) // 2^64是64位机能够表示的最大内存地址 mem, overflow := math.MulUintptr(et.size, uintptr(cap)) if overflow || mem \u0026gt; maxAlloc || len \u0026lt; 0 || len \u0026gt; cap { mem, overflow := math.MulUintptr(et.size, uintptr(len)) // 如果溢出或超过能够分配的最大内存(2^32 - 1) | 非法输入, 报错并返回 if overflow || mem \u0026gt; maxAlloc || len \u0026lt; 0 { panicmakeslicelen() } panicmakeslicecap() } // 调用mallocgc函数分配一块连续内存并返回该内存的首地址 // 该函数实现涉及到了go语言内存管理，比较复杂，不是本文的主题 // 关于 mallocgc 逻辑后续再讲 return mallocgc(mem, et, true) } 根据slice的类型和长度以及容量，计算出所需要的内存空间大小， 如果合法则调用mallocgc函数申请相应的连续内存并返回首地址， 下面来看一下是如何计算所需要的内存大小的\nsrc\\runtime\\internal\\math\\math.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 // MulUintptr returns a * b and whether the multiplication overflowed. // On supported platforms this is an intrinsic lowered by the compiler. func MulUintptr(a, b uintptr) (uintptr, bool) { // 如果slice类型大小为0(如struct{}类型) 或 (a | b) \u0026lt; 2 ^ 32，肯定不会发生溢出 // sys.PtrSize = 8 if a|b \u0026lt; 1\u0026lt;\u0026lt;(4*sys.PtrSize) || a == 0 { return a * b, false } // 如果a * b \u0026gt; MaxUintptr，即类型大小 * 切片容量 \u0026gt; MaxUintPtr，则说明发生了溢出 // MaxUintptr = ^uintptr(0) = 2 ^ 64 overflow := b \u0026gt; MaxUintptr/a return a * b, overflow } 追加元素 append() src/reflect/value.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 // Append appends the values x to a slice s and returns the resulting slice. // As in Go, each x\u0026#39;s value must be assignable to the slice\u0026#39;s element type. func Append(s Value, x ...Value) Value { s.mustBe(Slice) s, i0, i1 := grow(s, len(x)) for i, j := i0, 0; i \u0026lt; i1; i, j = i+1, j+1 { s.Index(i).Set(x[j]) } return s } // grow grows the slice s so that it can hold extra more values, allocating // more capacity if needed. It also returns the old and new slice lengths. func grow(s Value, extra int) (Value, int, int) { i0 := s.Len() i1 := i0 + extra if i1 \u0026lt; i0 { panic(\u0026#34;reflect.Append: slice overflow\u0026#34;) } m := s.Cap() if i1 \u0026lt;= m { return s.Slice(0, i1), i0, i1 } if m == 0 { m = extra } else { // const threshold = 256 // 1.18增加 for m \u0026lt; i1 { if i0 \u0026lt; 1024 { m += m } else { m += m / 4 } } } t := MakeSlice(s.Type(), i1, m) Copy(t, s) return t, i0, i1 } 追加元素流程如下：\n计算当前slice长度i0与追加元素的长度总和i1，若i1 \u0026lt; i0（追加后的slice长度小于当前的slice长度），则直接抛出异常 若当前切片容量为0，则将容量改为追加元素的长度 循环遍历增加容量， 若追加的元素长度小于阈值（v1.18之前是1024，从v1.18开始改为256），则容量成倍增长， 否则容量每次增加1/4,(注：v1.18开始计算逻辑改为 m = (m + 3 * threshold) / 4) ， 创建新的切片，并将当前切片拷贝到新的切片中，返回新的切片 将追加的元素存储到新的切片指定位置 切片的扩容逻辑：\n原cap扩充一倍，即doublecap 如果指定cap(追加元素后的切片长度) 大于 doublecap，则取cap，否则如下： cap 小于1024，取doublecap 每次增长 1/4，直至不小于cap ","date":"2022-09-05T09:01:56+08:00","permalink":"https://x-xkang.com/p/golang--slice/","title":"Golang -- Slice"},{"content":"一、删除策略 惰性删除\n每次获取 key 的时候会在 expire 字典中查询是否有当前key，如果有的话会校验当前key的过期时间，过期则删除，缺点是如果存在键已过期，但长期不使用的情况，实际上数据还是存在内存中的\n以下是 get 命令的部分源码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // src/t_string.c line 78 void setGenericCommand(client *c, int flags, robj *key, robj *val, robj *expire, int unit, robj *ok_reply, robj *abort_reply) { long long milliseconds = 0; /* initialized to avoid any harmness warning */ int found = 0; int setkey_flags = 0; if (expire \u0026amp;\u0026amp; getExpireMillisecondsOrReply(c, expire, flags, unit, \u0026amp;milliseconds) != C_OK) { return; } if (flags \u0026amp; OBJ_SET_GET) { if (getGenericCommand(c) == C_ERR) return; } found = (lookupKeyWrite(c-\u0026gt;db,key) != NULL); // 重点，开始查找当前key // ... 省略部分源码 } 顺着 lookupKeyWrite 找下去\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 // src/db.c line 158 robj *lookupKeyWrite(redisDb *db, robj *key) { return lookupKeyWriteWithFlags(db, key, LOOKUP_NONE); // 继续跳转 } // src/db.c line 154 robj *lookupKeyWriteWithFlags(redisDb *db, robj *key, int flags) { return lookupKey(db, key, flags | LOOKUP_WRITE); // 继续跳转 } // src/db.c line 81 robj *lookupKey(redisDb *db, robj *key, int flags) { dictEntry *de = dictFind(db-\u0026gt;dict,key-\u0026gt;ptr); robj *val = NULL; if (de) { val = dictGetVal(de); /* Forcing deletion of expired keys on a replica makes the replica * inconsistent with the master. We forbid it on readonly replicas, but * we have to allow it on writable replicas to make write commands * behave consistently. * * It\u0026#39;s possible that the WRITE flag is set even during a readonly * command, since the command may trigger events that cause modules to * perform additional writes. */ int is_ro_replica = server.masterhost \u0026amp;\u0026amp; server.repl_slave_ro; int force_delete_expired = flags \u0026amp; LOOKUP_WRITE \u0026amp;\u0026amp; !is_ro_replica; // 这里开始判断是否过期 if (expireIfNeeded(db, key, force_delete_expired)) { /* The key is no longer valid. */ val = NULL; } } } // src/db.c line 1666 // 判断逻辑 int expireIfNeeded(redisDb *db, robj *key, int force_delete_expired) { if (!keyIsExpired(db,key)) return 0; if (server.masterhost != NULL) { if (server.current_client == server.master) return 0; if (!force_delete_expired) return 1; } if (checkClientPauseTimeoutAndReturnIfPaused()) return 1; /* Delete the key */ deleteExpiredKeyAndPropagate(db,key); return 1; } 定期删除\n每隔一段时间检查一次数据库，随机删除一些过期键，需要注意的是：Redis 每次扫描并不是遍历过期字典中的所有键，而是采用随机抽取判断并删除过期键的形式执行的 Redis 默认每秒进行10次过期扫描，此配置可以通过 redis.conf 的 hz配置项进行配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // src/server.c line 971 void databasesCron(void) { /* Expire keys by random sampling. Not required for slaves * as master will synthesize DELs for us. */ if (server.active_expire_enabled) { // 区分主从 if (iAmMaster()) { activeExpireCycle(ACTIVE_EXPIRE_CYCLE_SLOW); // Master 节点开始处理过期键 } else { expireSlaveKeys(); } } // 省略部分源码... } Redis使用使用的是惰性删除加定期删除的策略\n二、淘汰策略 只有在 Redis 的运行内存达到了某个阀值，才会触发内存淘汰机制，这个阀值就是我们设置的最大运行内存，此值在 Redis 的配置文件中可以找到，配置项为 maxmemory。\n查看最大运行内存\n1 2 3 127.0.0.1:6379\u0026gt; config get maxmemory 1) \u0026#34;maxmemory\u0026#34; 2) \u0026#34;0\u0026#34; Redis在32位系统下默认阈值为3G（最大运行内存为4G，为保证系统正常运行，预留1G资源），64位系统下默认阈值为0，表示没有大小限制\n查看内存淘汰策略\n1 2 3 127.0.0.1:6379\u0026gt; config get maxmemory-policy 1) \u0026#34;maxmemory-policy\u0026#34; 2) \u0026#34;noeviction\u0026#34; noeviction 表示当运行内存超过最大设置内存时，不淘汰任何数据，但新增数据操作会报错\n内存淘汰策略分类\n1. noeviction: 不淘汰任何数据，当运行内存不足时，新增数据会直接报错 2. allkeys-lru: 淘汰所有键中最长时间未使用的 3. allkeys-random: 随机淘汰任意键 4. allkeys-lfu: 淘汰所有键中最少使用的； 5. volatile-lru: 淘汰所有设置了过期时间的键中最长时间未使用的 6. volatile-random: 随机淘汰设置了过期时间的键 7. volatile-ttl: 优先淘汰更快过期的键 8. volatile-lfu: 优先淘汰设置了过期时间的键中最少使用的\nLRU算法\nLRU 算法全称是Least Recently Used 译为最近最少使用 LRU 算法基于链表结构，链表中的元素按照操作顺序从前往后排列，最新操作的会被移动到表头，当需要内存淘汰时，只需要删除链表尾部的元素即可 Redis使用的是一种近似 LRU 算法，目的是为了更好的节约内存，它的实现方式是给现有的数据结构添加一个额外的字段，用于记录此键的最后一次访问时间，Redis内存淘汰时，会使用随机采样的方式淘汰数据，他是随机取N个值（可配置），然后淘汰醉酒没有使用的那个\nLFU算法\nLFU 全称是 Least Frequently Used 翻译为最不常用，最不常用的算法是根据总访问次数来淘汰数据的，它的核心思想是”如果数据过去被访问的次数很多，那么将来被访问的频率也会很高“。 LFU 解决了偶尔访问一次之后，数据就不会被淘汰的问题，相比于 LRU 算法也更合理一些\n","date":"2022-06-30T10:01:40+08:00","permalink":"https://x-xkang.com/p/redis-%E7%9A%84%E8%BF%87%E6%9C%9F%E5%88%A0%E9%99%A4%E7%AD%96%E7%95%A5%E4%BB%A5%E5%8F%8A%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/","title":"Redis 的过期删除策略以及淘汰策略"},{"content":"Redis提供两种持久化方式：一种是默认的RDB持久化方式，另一种是AOF（append only file）持久化方式\n一、 RDB 是什么？\nRDB是通过创建快照的方式进行持久化，保存某个时间点的全部数据，RDB是Redis默认的持久化方案，原理是Redis会通过单独创建（fork）一个与当前进程一模一样的子进程来进行持久化，这个子进程的所有数据（变量、环境变量、程序计数器等）都和原进程一模一样，会先将数据写入到一个临时文件中，待持久化结束了，再用这个临时文件替换上次持久化好的文件，整个过程冲，主进程不进行任何的io操作，这就确保了极高的性能。\n1、持久化文件在哪 启动redis-server 的目录下\n2、什么时候fork子进程，或者什么时候触发 rdb持久化机制 RDB 方式持久化数据是通过 fork 子进程，在子进程中进行数据同步\nshutdown时，如果没有开启aof，会触发配置文件中默认的快照配置 执行命令 save 或者 bgsave save是只管保存，不管其他，全部阻塞，使用主进程进行持久化 bgsave redis 会在后台异步进行快照操作，同时可以响应客户端的请求\n3、优点 存储紧凑，节省内存空间 恢复速度快 适合全量备份，全量复制的场景，经常用于灾难恢复（对数据的完整性和一致性要求较低）\n4、缺点 容易丢失数据，两次数据快照备份之间如果出现故障，则会丢失期间产生或修改的数据 通过 fork 子进程对内存快照进行全量备份，频繁执行性能消耗较大\n手动触发\nsave， 在命令行执行save命令，将以同步的方式创建rdb文件保存快照，会阻塞服务器的主进程，生产环境中不要用 bgsave, 在命令行执行bgsave命令，将通过fork一个子进程以异步的方式创建rdb文件保存快照，除了fork时有阻塞，子进程在创建rdb文件时，主进程可继续处理请求 自动触发\n在redis.conf中配置​​save m n​​​ 定时触发，如​​save 900 1​​表示在900s内至少存在一次更新就触发 主从复制时，如果从节点执行全量复制操作，主节点自动执行bgsave生成RDB文件并发送给从节点 执行debug reload命令重新加载Redis时 执行shutdown且没有开启AOF持久化 二、AOF 将Redis的操作日志以追加的方式写入文件，读操作是不记录的\n为什么会出现AOF持久化方式\n1、这个持久化文件在哪 启动 redis-server 的目录下会生成 appendonly.aof文件\n2、触发机制（根据配置文件的配置项\u0026ndash;appendfsync） no: 表示操作系统进行数据缓存同步到磁盘（快，持久化没保证：写满缓冲区才会同步，若在缓冲区未写满前 shutdown 或其他意外关机，则这部分数据会丢失） always: 同步持久化，每次发生数据变更时（增删改操作），立即记录到磁盘（慢，安全） everysec: 表示每秒同步一次（默认值，很快，但可能会丢失1秒的数据）\n3、AOF 重写机制 重写 AOF 文件会 fork 子进程去执行，会将内存中的数据写入新的 AOF 文件，并且是以RDB 的方式写入，重写结束后会替代旧的AOF 文件，后续的客户端命令操作又重新以 AOF的格式写入，redis.conf 中配置触发 AOF 文件重写的文件大小值auto-aof-rewrite-percentage 不宜太小，因为会频繁触发重写\n触发时机 redis.conf 的配置项 auto-aof-rewrite-min-size 默认值是 64mb， 当 AOF 文件大小超过这个配置值时会自动开启重写 `。 redis.conf 的配置项 auto-aof-rewrite-percentage 默认值是100， 当 AOF 文件大小的增长率大于配置值时会自动开启重写。 4、优点 保证数据安全\n5、缺点 数据恢复慢\n","date":"2022-06-29T15:34:24+08:00","permalink":"https://x-xkang.com/p/redis-%E6%8C%81%E4%B9%85%E5%8C%96%E7%AD%96%E7%95%A5/","title":"Redis 持久化策略"},{"content":"一、Redis 缓存雪崩 出现场景 如果使用redis记录大量的热点数据，且过期时间为同一个常量，那么可能会出现大批的缓存数据会在同一时间或较短的时间区间内失效，redis会根据淘汰策略进行优化，如果数据量比较大会导致线程出现短暂的阻塞；另外，因为大量的缓存失效，会导致请求直接落在DB上，请求数较大情况下会直接导致数据库瘫痪，然后整个业务系统变为不可用状态\n解决方案 针对这种情况，我们可以在设置过期时间时加上一个随机值，类似 redis.set(key, value, expiredTime + Math.random()*10000)，这样设置就不会出现在短时间内大量缓存key失效的情况。\n二、Redis 缓存穿透 出现场景 如果用户请求的热点数据本身是不存在的，比如id为-1，或者id=\u0026lsquo;\u0026lsquo;的数据，查询缓存不存在后会将请求直接打到DB上，最终在DB中也没有查到此数据，此时Redis缓存就是去了作用，搞并发的情况下会降低数据库性能，甚至瘫痪\n解决方案 增加参数校验，拦截掉大量的非法参数请求； 缓存空值，因为数据库中本来就不存在这些数据，因此可以在第一次重建缓存时将value 记录为 null，下次请求时从Redis获取到 null 值直接返回（注意，要对redis查询的返回值进行严格校验，区分key不存在返回的空值和主动设置的空值null）； 布隆过滤器，将DB中的热点数据加载至布隆过滤器中（布隆过滤器的特性：若在过滤器中存在，不一定真是存在；若不存在时，一定不存在），每次请求前先校验布隆过滤器是否存在该key，不存在的话直接return； 三、Redis 缓存击穿 出现场景 高并发请求同一个热点数据，在热点数据失效的瞬间，大量请求在缓存中没有命中会直接落在DB上进行查询，导致DB压力瞬间增加\n解决方案 增加互斥锁，在第一个请求没有在缓存命中开始在DB进行查询并重加缓存时加上一个互斥锁，在缓存重建完成之前，对这同一热点数据的请求将会被互斥锁拦截，被拦截的这些请求根据业务需求，可以延时重试直到拿到数据或直接返回失败等； 热点数据不设置过期时间（不建议，随着热点数据的增加，无过期时间的key也越来越多，或导致Redis的存储压力增加）\n","date":"2022-06-28T14:58:25+08:00","permalink":"https://x-xkang.com/p/redis%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E4%B9%8B%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9%E5%87%BB%E7%A9%BF%E5%92%8C%E7%A9%BF%E9%80%8F/","title":"Redis基础知识之缓存雪崩、击穿和穿透"},{"content":"HTTP/0.9 - 单行协议\n最初版本的HTTP协议并没有版本号，后来它的版本号被定位在0.9以区分后来的版本。HTTP/0.9极其简单：请求由单行指令构成，以唯一可用方法GET开头，其后跟目标资源的路径（一旦连接到服务器，协议、服务器、端口号这些都不是必须的）。 跟后来的版本不同，HTTP/0.9的响应内容并不包含HTTP头，这意味着只有 HTML 文件可以传送，无法传送其他类型的文件；也没有状态码或者错误代码；一旦出现问题，一个特殊的包含问题描述信息的HTML文件将被发回，供人们查看。\nHTTP/1.0 - 构建可扩展性\n由于 HTTP/0.9协议的应用十分有限，浏览器和服务器迅速扩展内容使其用途更广：\n协议版本信息现在会随着每个请求发送（HTTP/1.0被追加到了 GET行） 状态码会在响应开始时发送，使浏览器能了解请求执行成功或失败，并响应调整行为（如更新或使用本地缓存） 引入了 HTTP 头的概念，无论是对于请求还是响应，允许传输元数据，使协议变得非常灵活，更具扩展性。 在新 HTTP 头的的帮助下，具备了传输纯文本HTML文件以外其他类型文档的能力（凭借Content-Type头） 一个典型的请求看起来就像这样：\n1 2 3 4 5 6 7 8 9 10 11 GET /mypage.html HTTP/1.0 User-Agent: NCSA_Mosaic/2.0 (Windows 3.1) 200 OK Date: Tue, 15 Nov 1994 08:12:31 GMT Server: CERN/3.0 libwww/2.17 Content-Type: text/html \u0026lt;HTML\u0026gt; 一个包含图片的页面 \u0026lt;IMG SRC=\u0026#34;/myimage.gif\u0026#34;\u0026gt; \u0026lt;/HTML\u0026gt; 接下来是第二个请求：\n1 2 3 4 5 6 7 8 GET /myimage.gif HTTP/1.0 User-Agent: NCSA_Mosaic/2.0 (Windows 3.1) 200 OK Date: Tue, 15 Nov 1994 08:12:32 GMT Server: CERN/3.0 libwww/2.17 Content-Type: text/gif (这里是图片内容) 在 1991-1995年，这些新扩展并没有被引入到标准中以促进下注工作，而仅仅作为一种尝试：服务器和浏览器天界这些新扩展功能，但出现了大量的互操作问题。知道 1996年11月，为了解决这些问题，一份新文档（RFC 1945） 被发表出来，泳衣描述如何操作实践这些新扩展功能，文档 RFC 1945 定义了 HTTP/1.0，但它是狭义的，并不是官方标准\nHTTP/1.1 - 标准化的协议\nHTTP/1.0 多种不同的实现方式在实际运用中显得有些混乱，自 1995 年开始，即 HTTP/1.0 文档发布的下一年，就开始修订 HTTP 的第一个标准化版本。在 1997 年初，HTTP1.1 标准发布，就在 HTTP/1.0 发布的几个月后。\nHTTP/1.1 消除了大量歧义内容并引入了多项改进：\n连接可以复用，节省了多次打开 TCP 连接加载网页文档资源的时间。 增加管线化技术，允许在第一个应答被完全发送之前就发送第二个请求，以降低通信延迟。 支持响应分块。 引入额外的缓存控制机制。 引入内容协商机制，包括语言，编码，类型等，并允许客户端和服务器之间约定以最合适的内容进行交换。 凭借Host头，能够使不同域名配置在同一个 IP 地址的服务器上。 一个典型的请求流程， 所有请求都通过一个连接实现，看起来就像这样：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 GET /en-US/docs/Glossary/Simple_header HTTP/1.1 Host: developer.mozilla.org User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:50.0) Gecko/20100101 Firefox/50.0 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8 Accept-Language: en-US,en;q=0.5 Accept-Encoding: gzip, deflate, br Referer: https://developer.mozilla.org/en-US/docs/Glossary/Simple_header 200 OK Connection: Keep-Alive Content-Encoding: gzip Content-Type: text/html; charset=utf-8 Date: Wed, 20 Jul 2016 10:55:30 GMT Etag: \u0026#34;547fa7e369ef56031dd3bff2ace9fc0832eb251a\u0026#34; Keep-Alive: timeout=5, max=1000 Last-Modified: Tue, 19 Jul 2016 00:59:33 GMT Server: Apache Transfer-Encoding: chunked Vary: Cookie, Accept-Encoding (content) HTTP/1.1 在 1997 年 1 月以 RFC 2068 文件发布。\nHTTP 用于安全传输\nHTTP 最大的变化发生在 1994 年底。HTTP 在基本的 TCP/IP 协议栈上发送信息，网景公司（Netscape Communication）在此基础上创建了一个额外的加密传输层：SSL 。SSL 1.0 没有在公司以外发布过，但 SSL 2.0 及其后继者 SSL 3.0 和 SSL 3.1 允许通过加密来保证服务器和客户端之间交换消息的真实性，来创建电子商务网站。SSL 在标准化道路上最终成为 TLS，随着版本 1.0, 1.1, 1.2 的出现成功地关闭漏洞。TLS 1.3 目前正在形成。\n与此同时，人们对一个加密传输层的需求也愈发高涨：因为 Web 最早几乎是一个学术网络，相对信任度很高，但如今不得不面对一个险恶的丛林：广告客户、随机的个人或者犯罪分子争相劫取个人信息，将信息占为己有，甚至改动将要被传输的数据。随着通过 HTTP 构建的应用程序变得越来越强大，可以访问越来越多的私人信息，如地址簿，电子邮件或用户的地理位置，即使在电子商务使用之外，对 TLS 的需求也变得普遍。\nHTTP 用于复杂应用\nTim Berners-Lee 对于 Web 的最初设想不是一个只读媒体。 他设想一个 Web 是可以远程添加或移动文档，是一种分布式文件系统。 大约 1996 年，HTTP 被扩展到允许创作，并且创建了一个名为 WebDAV 的标准。 它进一步扩展了某些特定的应用程序，如 CardDAV 用来处理地址簿条目，CalDAV 用来处理日历。 但所有这些 *DAV 扩展有一个缺陷：它们必须由要使用的服务器来实现，这是非常复杂的。并且他们在网络领域的使用必须保密。\n在 2000 年，一种新的使用 HTTP 的模式被设计出来：representational state transfer (或者说 REST)。 由 API 发起的操作不再通过新的 HTTP 方法传达，而只能通过使用基本的 HTTP / 1.1 方法访问特定的 URI。 这允许任何 Web 应用程序通过提供 API 以允许查看和修改其数据，而无需更新浏览器或服务器：所有需要的内容都被嵌入到由网站通过标准 HTTP/1.1 提供的文件中。 REST 模型的缺点在于每个网站都定义了自己的非标准 RESTful API，并对其进行了全面的控制；不同于 *DAV 扩展，客户端和服务器是可互操作的。 RESTful API 在 2010 年变得非常流行。\n自2005年以来，可用于 Web 页面的API大大增加，其中几个API为特定目的扩展了HTTP协议，大部分是新的特定HTTP头：\nServer-sent events 服务器可以偶尔推送消息到浏览器 Websocket 一个新协议，可以通过升级现有 HTTP 协议来建立 放松安全措施-基于当前的web模型\nHTTP 和 Web安全模型 \u0026ndash; 同源策略是互不相关的。事实上，当前的Web安全模型是在HTTP被创造出来后才被发展的！这些年来，已经在证实了它如果能通过在特定的约束下移除一些这个策略的限制来管的宽松些的话，将会更有用。这些策略导致大量的成本和时间被话费在通过转交到服务端来添加一些新的HTTP头来发送。这些被定义在了 Cross-Origin Resource Sharing(CORS) or the Content Security Policy(CSP)规范里。\n不只是这大量的扩展，很多的其他的头也被加了进来，有些只是实验性的，比较著名的有 Do Not Track(DNT)来控制隐私， X-Frame-Option，还有很多\nHTTP/2 - 为了更优异的表现\n这些年来，网页愈渐变得的复杂，甚至演变成了独有的应用，可见媒体的播放量，增进交互的脚本大小也增加了许多：更多的数据通过 HTTP 请求被传输。HTTP/1.1 链接需要请求以正确的顺序发送，理论上可以用一些并行的链接（尤其是 5 到 8 个），带来的成本和复杂性堪忧。比如，HTTP 管线化（pipelining）就成为了 Web 开发的负担。\n在 2010 年到 2015 年，谷歌通过实践了一个实验性的 SPDY 协议，证明了一个在客户端和服务器端交换数据的另类方式。其收集了浏览器和服务器端的开发者的焦点问题。明确了响应数量的增加和解决复杂的数据传输，SPDY 成为了 HTTP/2 协议的基础。\nHTTP/2 和 HTTP/1.1 有几处基本的不同：\nHTTP/2 是二进制协议而不是文本协议。不在可读，也不是无障碍的手动创建，改善的优化技术现在可被实施。 这是一个复用协议。并行的请求能在同一个链接中处理，移除了HTTP/1.x 中顺序和阻塞的约束。 压缩了headers，因为 headers 在一系列请求中常常是相似的，其移除了重复和传输重复数据的成本。 其允许服务器在客户端缓存中填充数据，通过一个叫服务器推送的机制来提前请求。 HTTP/3 - 基于UDP的QUIC协议实现\n在HTTP/3中，将启用TCP协议，改为使用基于UDP协议的QUIC协议实现，此改变是为了解决HTTP/2中存在的队头阻塞问题，由于HTTP/2在单个TCP连接上使用了多路复用，收到TCP拥塞控制的影响，少量的丢包就可能导致整个TCP连接上的所有流被阻塞。\nQUIC（快速UDP网络连接）是一种实验性的网络传输协议，由Google开发，该协议旨在使网页传输更快。\n","date":"2022-06-17T10:09:23+08:00","permalink":"https://x-xkang.com/p/http-%E5%8D%8F%E8%AE%AE%E7%89%88%E6%9C%AC%E8%BF%9B%E5%8C%96/","title":"Http 协议版本进化"},{"content":"了解域名结构\n以mail.qq.com域名为例，com为顶级域名，qq.com 为二级域名， mail.qq.com为三级域名\n顶级域名服务器\n顶级域名为最后一个.右侧部分的内容，如mail.qq.com的com就是顶级域名，顶级域名对应的服务器称之为顶级域名服务器\n二级域名服务器\n域名 mail.qq.com中的倒数第二个.右侧部分qq.com成为二级域名\n根域名服务器\n在2016年之前全球一共拥有13台根服务器，1台主根服务器在美国，其他12台为辅根服务器，其中美国9台，英国1台，瑞典1台，日本1台，这13台根服务器主要管理互联网的主目录，主要作用IPV4。 2016年，中国下一代互联网工程中心领衔发起雪人计划，旨在为下一代互联网(IPV6)提供更多的根服务器解决方案，该计划于2017年完成，其中包含3台主根服务器，中国1台，美国1台，日本1台，22台辅根服务器，中国3台，美国2台，印度和法国分别有3台，德国2台，俄罗斯、意大利、西班牙、奥地利、智利、南非、澳大利亚、瑞士、荷兰各1台，共22台，从此形成了13台原有根加25台IPV6根服务器的新格局\n本地域名服务器\n本地域名服务器的范围非常广，没有一个详细的定位，可能是某个运营商部署在该城市的一台服务器，也可能是某个公司的一台服务器，甚至可能是某个学校的服务器\n存放地址\n浏览器缓存 系统缓存 本地域名服务器 根域名服务器 顶级域名服务器 二级域名服务器 \u0026hellip; \u0026hellip; 查询顺序\n检查浏览器缓存中是否存在改域名与IP地址的映射关系，如果有则解析结束，没有则继续 到系统本地查找映射关系，一般存在 hosts 文件中，如果有则解析结束，否则继续 到本地域名服务器去查询，有则结束，否则继续 本地域名服务器去查询 根域名服务器，该过程不会返回映射关系，只会告诉你去下级服务器（顶级域名服务器）查询 本地域名服务器查询顶级域名服务器（即 com 服务器），同样不会返回映射关系，只会引导你去二级域名服务器查询 本地域名服务器查询二级域名服务器（即 qq.com 服务器），引导去三级域名服务器 本地域名服务器查询三级域名服务器（即 mail.qq.com）， 此时已经是最后一级，如果有则返回映射关系，并且本地服务器加入自身的映射表中，方便下次查询，同时返回给用户的计算机，没有找到则网页报错 ","date":"2022-06-16T17:00:53+08:00","permalink":"https://x-xkang.com/p/dns-%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E6%B5%81%E7%A8%8B/","title":"DNS 域名解析流程"},{"content":"OSI七层模型与TCP/IP四层模型对比\nOSI 七层模型 TCP/IP 四层模型 对应网络协议 应用层 应用层 HTTP、TFTP, FTP, NFS, WAIS、SMTP 表示层 Telnet, Rlogin, SNMP, Gopher 会话层 SMTP, DNS 传输层 传输层 TCP, UDP 网络层 网络层 IP, ICMP, ARP, RARP, AKP, UUCP 数据链路层 数据链路层 FDDI, Ethernet, Arpanet, PDN, SLIP, PPP 物理层 IEEE 802.1A, IEEE 802.2到IEEE 802.11 应用层：为应用程序或用户请求提供请求服务。OSI参考模型最高层，也是最靠近用户的一层，为计算机用户、各种应用程序以及网络提供端口，也为用户直接提供各种网络服务。\n表示层：数据编码、格式转换、数据加密。提供各种用于应用层的编码和转换功能，确保一个系统的应用层发送的数据能被另一个系统的应用层识别。如果必要，该层可提供一种标准表示形式，用于讲计算机内部的各种数据格式转换成通信中采用的标准表示形式。数据压缩和加密也是表示层可提供的转换功能之一。\n会话层：创建、管理和维护会话。接收来自传输层的数据，负责建立、管理和终止表示层实体之间的通信会话，支持它们之间的数据交换。该层的通信由不同设备中的应用程序之间的服务请求和响应组成。\n传输层：数据通信。建立主机端到端的链接，为会话层和网络层提供端到端可靠的和透明的数据传输服务，确保数据能完整的传输到网络层。\n网络层：IP选址及路由选择。通过路由选择算法，为报文或通信子网选择最适当的路径。控制数据链路层与传输层之间的信息转发，建立、维持和终止网络的连接。数据链路层的数据在这一层被转换为数据包，然后通过路径选择、分段组合、顺序、进/出路由等控制，将信息从一个网络设备传送到另一个网络设备。\n数据链路层：提供介质访问和链路管理。接收来自物理层的位流形式的数据，封装成帧，传送到网络层；将网络层的数据帧，拆装为位流形式的数据转发到物理层；负责建立和管理节点间的链路，通过各种控制协议，将有差错的物理信道变为无差错的、能可靠传输数据帧的数据链路。\n物理层：管理通信设备和网络媒体之间的互联互通。传输介质为数据链路层提供物理连接，实现比特流的透明传输。实现相邻计算机节点之间比特流的透明传送，屏蔽具体传输介质和物理设备的差异。\n","date":"2022-06-15T15:57:49+08:00","permalink":"https://x-xkang.com/p/osi-%E4%B8%83%E5%B1%82%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/","title":"OSI 七层网络模型"},{"content":"一、HTTP状态码分类 HTTP 状态码由三个十进制数字组成，第一个十进制数字定义了状态码的类型，响应分为五类：消息响应（100-199），成功响应（200-299），重定向消息（300-399），客户端错误响应（400-499）和服务器错误响应（500-599）\n分类 分来描述 1** 信息，服务器收到请求，需要请求者继续执行操作 2** 操作被成功接收并处理 3** 重定向，需要进一步操作以完成请求 4** 客户端错误，请求包含语法错误或无法完成请求 5** 服务器错误，服务器在处理请求的过程发生了错误 二、状态码分类描述 状态码 英文状态码 中文描述 100 Continue 这个临时响应表明，迄今为止的所有内容都是可行的，客户端应该继续请求，如果以完成，请忽略它。 101 Switching Protocols 该代码是响应客户端 Upgrade(en-US). 请求头发送的，指明服务器即将切换的协议 102 Processing 此代码表示服务器以收到并且正在处理该请求，但当前没有响应可用 103 Early Hints 次状态码主要用于与 Link 链接头一起使用，以允许用户代理在服务器响应阶段时开始预加载 preloading 资源 200 OK 请求成功 201 Created 该请求已成功，并因此创建了一个新的资源。这通常是在 POST 请求，或是某些 PUT 请求之后返回的响应 202 Accepted 请求已经接收到，但还未响应，没有结果。意味着不会有一个异步的响应去表明当前请求的结果，预期另外的进程和服务去处理请求，或者批处理。 203 Non-Authoritative Information 服务器已成功处理了请求，但返回的实体头部元信息不是在原始服务器上有效的确定集合，而是来自本地或者第三方的拷贝。当前的信息可能是原始版本的子集或者超集。例如，包含资源的元数据可能导致原始服务器知道元信息的超集。使用此状态码不是必须的，而且只有在响应不使用此状态码便会返回200 OK的情况下才是合适的。 204 No Content 对于该请求没有的内容可发送，但头部字段可能有用。用户代理可能会用此时请求头部信息来更新原来资源的头部缓存字段。 205 Reset Content 告诉用户代理重置发送此请求的文档 206 Partial Content 当从客户端发送Range范围标头以只请求资源的一部分时，将使用此响应代码。 207 Multi Status 对于多个状态代码都可能合适的情况，传输有关多个资源的信息。 208 Already Reported 在 DAV 里面使用 dav:propstat 响应元素以避免重复枚举多个绑定的内部成员到同一个集合。 226 IM Used 服务器已经完成了对资源的GET请求，并且响应是对当前实例应用的一个或多个实例操作结果的表示。 300 Multiple Choice 请求拥有不只一个的可鞥响应。用户带来或者用户应当从中选择一个。 (没有标准化的方法来选择其中一个响应，但是建议使用指向可能性的 HTML 链接，以便用户可以选择。) 301 Moved Permanently 请求资源的 URL 已永久更改。在响应中给出了新的 URL。 302 Found 此响应代码表示所请求资源的 URI 已 暂时 更改。未来可能会对 URI 进行进一步的改变。因此，客户机应该在将来的请求中使用这个相同的 URI。 303 See Other 服务器发送此响应，以指示客户端通过一个 GET 请求在另一个 URI 中获取所请求的资源 304 Not Modified 这是用于缓存的目的。它告诉客户端响应还没有被修改，因此客户端可以继续使用相同的缓存版本的响应。 305 Use Proxy 在 HTTP 规范中定义，以指示请求的响应必须被代理访问。由于对代理的带内配置的安全考虑，它已被弃用 306 unused 此响应代码不再使用；它只是保留。它曾在 HTTP/1.1 规范的早期版本中使用过。 307 Temporary Redirect 服务器发送此响应，以指示客户端使用在前一个请求中使用的相同方法在另一个 URI 上获取所请求的资源。这与 302 Found HTTP 响应代码具有相同的语义，但用户代理 不能 更改所使用的 HTTP 方法：如果在第一个请求中使用了 POST，则在第二个请求中必须使用 POST 308 Permanent Redirect 这意味着资源现在永久位于由Location: HTTP Response 标头指定的另一个 URI。 这与 301 Moved Permanently HTTP 响应代码具有相同的语义，但用户代理不能更改所使用的 HTTP 方法：如果在第一个请求中使用 POST，则必须在第二个请求中使用 POST。 400 Bad Request 由于被认为是客户端错误（例如，错误的请求语法、无效的请求消息帧或欺骗性的请求路由），服务器无法或不会处理请求。 401 Unauthorized 虽然 HTTP 标准指定了\u0026quot;unauthorized\u0026quot;，但从语义上来说，这个响应意味着\u0026quot;unauthenticated\u0026quot;。也就是说，客户端必须对自身进行身份验证才能获得请求的响应。 402 Payment Required 此响应代码保留供将来使用。创建此代码的最初目的是将其用于数字支付系统，但是此状态代码很少使用，并且不存在标准约定。 403 Forbidden 客户端没有访问内容的权限；也就是说，它是未经授权的，因此服务器拒绝提供请求的资源。与 401 Unauthorized 不同，服务器知道客户端的身份。 404 Not Found 服务器找不到请求的资源。在浏览器中，这意味着无法识别 URL。在 API 中，这也可能意味着端点有效，但资源本身不存在。服务器也可以发送此响应，而不是 403 Forbidden，以向未经授权的客户端隐藏资源的存在。这个响应代码可能是最广为人知的，因为它经常出现在网络上。 405 Method Not Allowed 服务器知道请求方法，但目标资源不支持该方法。例如，API 可能不允许调用DELETE来删除资源。 406 Not Acceptale 当 web 服务器在执行 服务端驱动型内容协商机制](/zh-CN/docs/Web/HTTP/Content_negotiation#服务端驱动型内容协商机制)后，没有发现任何符合用户代理给定标准的内容时，就会发送此响应。 407 Proxy Authentication Required 类似于 401 Unauthorized 但是认证需要由代理完成。 408 Request Timeout 此响应由一些服务器在空闲连接上发送，即使客户端之前没有任何请求。这意味着服务器想关闭这个未使用的连接。由于一些浏览器，如 Chrome、Firefox 27+ 或 IE9，使用 HTTP 预连接机制来加速冲浪，所以这种响应被使用得更多。还要注意的是，有些服务器只是关闭了连接而没有发送此消息。 409 Conflict 当请求与服务器的当前状态冲突时，将发送此响应。 410 Gone 当请求的内容已从服务器中永久删除且没有转发地址时，将发送此响应。客户端需要删除缓存和指向资源的链接。HTTP 规范打算将此状态代码用于“有限时间的促销服务”。API 不应被迫指出已使用此状态代码删除的资源。 411 Length Required 服务端拒绝该请求因为 Content-Length 头部字段未定义但是服务端需要它。 412 Precondition Failed 客户端在其头文件中指出了服务器不满足的先决条件。 413 Payload Too Large 请求实体大于服务器定义的限制。服务器可能会关闭连接，或在标头字段后返回重试 Retry-After。 414 URI Too Long 客户端请求的 URI 比服务器愿意接收的长度长。 415 Unsupported Media Type 服务器不支持请求数据的媒体格式，因此服务器拒绝请求。 416 Rabge Not Satisfiale 无法满足请求中 Range 标头字段指定的范围。该范围可能超出了目标 URI 数据的大小。 417 Expectation Failed 此响应代码表示服务器无法满足 Expect 请求标头字段所指示的期望。 418 I'm Teapot 服务端拒绝用茶壶煮咖啡。笑话，典故来源茶壶冲泡咖啡 421 Misdirected Request 请求被定向到无法生成响应的服务器。这可以由未配置为针对请求 URI 中包含的方案和权限组合生成响应的服务器发送。 422 Unproccessable Entity 请求格式正确，但由于语义错误而无法遵循。 423 Locked 正在访问的资源已锁定。 424 Failed Dependency 由于前一个请求失败，请求失败。 425 Too Early 表示服务器不愿意冒险处理可能被重播的请求。 426 Upgrade Required 服务器拒绝使用当前协议执行请求，但在客户端升级到其他协议后可能愿意这样做。 服务端发送带有 Upgrade (en-US) 字段的 426 响应 来表明它所需的协议（们）。 428 Precondition Required 用户在给定的时间内发送了太多请求（\u0026ldquo;限制请求速率\u0026rdquo;） 431 Request Header Fields Too Large 服务器不愿意处理请求，因为其头字段太大。在减小请求头字段的大小后，可以重新提交请求。 451 Unavailable For Legal Reasons 用户代理请求了无法合法提供的资源，例如政府审查的网页。 500 Internal Server Error 服务器遇到了不知道如何处理的情况。 501 Not Implement 服务器不支持请求方法，因此无法处理。服务器需要支持的唯二方法（因此不能返回此代码）是 GET and HEAD. 502 Bad Gateway 此错误响应表明服务器作为网关需要得到一个处理这个请求的响应，但是得到一个错误的响应。 503 Service Unavailable 服务器没有准备好处理请求。常见原因是服务器因维护或重载而停机。请注意，与此响应一起，应发送解释问题的用户友好页面。这个响应应该用于临时条件和如果可能的话，HTTP 标头 Retry-After 字段应该包含恢复服务之前的估计时间。网站管理员还必须注意与此响应一起发送的与缓存相关的标头，因为这些临时条件响应通常不应被缓存。 504 Gateway Timeout 当服务器充当网关且无法及时获得响应时，会给出此错误响应。 505 HTTP Version Not Supported 服务器不支持请求中使用的 HTTP 版本。 506 Variant Also Negotiates 服务器存在内部配置错误：所选的变体资源被配置为参与透明内容协商本身，因此不是协商过程中的适当终点 507 Insufficient Storage 无法在资源上执行该方法，因为服务器无法存储成功完成请求所需的表示。 508 Loop Detected 服务器在处理请求时检测到无限循环。 510 Not Extented 服务器需要对请求进行进一步扩展才能完成请求。 511 Network Authentication Required 指示客户端需要进行身份验证才能获得网络访问权限。 ","date":"2022-06-14T21:33:58+08:00","permalink":"https://x-xkang.com/p/http-%E7%8A%B6%E6%80%81%E7%A0%81/","title":"Http 状态码"},{"content":" 常用正则表达式 字符 描述 \\ 将下一个字符标记为一个特殊字符、或一个原义字符、或一个向后引用、或一个八进制转义符。例如，“n”匹配字符“n”。“\\n”匹配一个换行符。串行“\\”匹配“\\”而“(”则匹配“(”。 ^ 匹配输入字符串的开始位置。如果设置了RegExp对象的Multiline属性，^也匹配“\\n”或“\\r”之后的位置。 $ 匹配输入字符串的结束位置。如果设置了RegExp对象的Multiline属性，$也匹配“\\n”或“\\r”之前的位置。 * 匹配前面的子表达式零次或多次。例如，zo*能匹配“z”以及“zoo”。*等价于{0,}。 + 匹配前面的子表达式一次或多次。例如，“zo+”能匹配“zo”以及“zoo”，但不能匹配“z”。+等价于{1,}。 ? 匹配前面的子表达式零次或一次。例如，“do(es)?”可以匹配“does”或“does”中的“do”。?等价于{0,1}。 {n} n是一个非负整数。匹配确定的n次。例如，“o{2}”不能匹配“Bob”中的“o”，但是能匹配“food”中的两个o。 {n,} n是一个非负整数。至少匹配n次。例如，“o{2,}”不能匹配“Bob”中的“o”，但能匹配“foooood”中的所有o。“o{1,}”等价于“o+”。“o{0,}”则等价于“o*”。 {n,m} m和n均为非负整数，其中n\u0026lt;=m。最少匹配n次且最多匹配m次。例如，“o{1,3}”将匹配“fooooood”中的前三个o。“o{0,1}”等价于“o?”。请注意在逗号和两个数之间不能有空格。 ? 当该字符紧跟在任何一个其他限制符（*,+,?，{n}，{n,}，{n,m}）后面时，匹配模式是非贪婪的。非贪婪模式尽可能少的匹配所搜索的字符串，而默认的贪婪模式则尽可能多的匹配所搜索的字符串。例如，对于字符串“oooo”，“o+?”将匹配单个“o”，而“o . 匹配除“\\n”之外的任何单个字符。要匹配包括“\\n”在内的任何字符，请使用像“(. (pattern) 匹配pattern并获取这一匹配。所获取的匹配可以从产生的Matches集合得到，在VBScript中使用SubMatches集合，在JScript中则使用$0…$9属性。要匹配圆括号字符，请使用“(”或“)”。 (?:pattern) 匹配pattern但不获取匹配结果，也就是说这是一个非获取匹配，不进行存储供以后使用。这在使用或字符“( (?=pattern) 正向肯定预查，在任何匹配pattern的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用。例如，“Windows(?=95 (?!pattern) 正向否定预查，在任何不匹配pattern的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用。例如“Windows(?!95 (?\u0026lt;=pattern) 反向肯定预查，与正向肯定预查类拟，只是方向相反。例如，“(?\u0026lt;=95 (?\u0026lt;!pattern) 反向否定预查，与正向否定预查类拟，只是方向相反。例如“(?\u0026lt;!95 x|y 匹配x或y。例如，“z [xyz] 字符集合。匹配所包含的任意一个字符。例如，“[abc]”可以匹配“plain”中的“a”。 [^xyz] 负值字符集合。匹配未包含的任意字符。例如，“[^abc]”可以匹配“plain”中的“p”。 [a-z] 字符范围。匹配指定范围内的任意字符。例如，“[a-z]”可以匹配“a”到“z”范围内的任意小写字母字符。 [^a-z] 负值字符范围。匹配任何不在指定范围内的任意字符。例如，“[^a-z]”可以匹配任何不在“a”到“z”范围内的任意字符。 \\b 匹配一个单词边界，也就是指单词和空格间的位置。例如，“er\\b”可以匹配“never”中的“er”，但不能匹配“verb”中的“er”。 \\B 匹配非单词边界。“er\\B”能匹配“verb”中的“er”，但不能匹配“never”中的“er”。 \\cx 匹配由x指明的控制字符。例如，\\cM匹配一个Control-M或回车符。x的值必须为A-Z或a-z之一。否则，将c视为一个原义的“c”字符。 \\d 匹配一个数字字符。等价于[0-9]。 \\D 匹配一个非数字字符。等价于[^0-9]。 \\f 匹配一个换页符。等价于\\x0c和\\cL。 \\n 匹配一个换行符。等价于\\x0a和\\cJ。 \\r 匹配一个回车符。等价于\\x0d和\\cM。 \\s 匹配任何空白字符，包括空格、制表符、换页符等等。等价于[ \\f\\n\\r\\t\\v]。 \\S 匹配任何非空白字符。等价于[^ \\f\\n\\r\\t\\v]。 \\t 匹配一个制表符。等价于\\x09和\\cI。 \\v 匹配一个垂直制表符。等价于\\x0b和\\cK。 \\w 匹配包括下划线的任何单词字符。等价于“[A-Za-z0-9_]”。 \\W 匹配任何非单词字符。等价于“[^A-Za-z0-9_]”。 \\xn 匹配n，其中n为十六进制转义值。十六进制转义值必须为确定的两个数字长。例如，“\\x41”匹配“A”。“\\x041”则等价于“\\x04\u0026amp;1”。正则表达式中可以使用ASCII编码。. \\num 匹配num，其中num是一个正整数。对所获取的匹配的引用。例如，“(.)\\1”匹配两个连续的相同字符。 \\n 标识一个八进制转义值或一个向后引用。如果\\n之前至少n个获取的子表达式，则n为向后引用。否则，如果n为八进制数字（0-7），则n为一个八进制转义值。 \\nm 标识一个八进制转义值或一个向后引用。如果\\nm之前至少有nm个获得子表达式，则nm为向后引用。如果\\nm之前至少有n个获取，则n为一个后跟文字m的向后引用。如果前面的条件都不满足，若n和m均为八进制数字（0-7），则\\nm将匹配八进制转义值nm。 \\nml 如果n为八进制数字（0-3），且m和l均为八进制数字（0-7），则匹配八进制转义值nml。 \\un 匹配n，其中n是一个用四个十六进制数字表示的Unicode字符。例如，\\u00A9匹配版权符号（©）。 ","date":"2022-01-11T09:29:51+08:00","permalink":"https://x-xkang.com/p/%E5%B8%B8%E7%94%A8%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/","title":"常用正则表达式"},{"content":"go:build go:linkname 1 //go:linkname localname importpath.name 该指令指示编译器使用 importpath.name 作为源码中声明为 localname 的变量的或函数的目标文件符号名称，但是由于这个伪指令可以破坏类型系统和包模块化，只有引用了 unsafe 包才可以使用。 简单来讲，就是 importpath.name 是 localname 的符号别名，编译器实际上会调用 localname，使用的前提是引入了 unsafe 包才能使用。\n例如 time/time.go\n1 2 // Provided by package runtime. func now() (sec int64, nsec int32, mono int64) runtime/timestub.go\n1 2 3 4 5 6 7 import _ \u0026#34;unsafe\u0026#34; // for go:linkname //go:linkname time_now time.now func time_now() (sec int64, nsec int32, mono int64) { sec, nsec = walltime() return sec, nsec, nanotime() } now 方法并没有具体实现，注释上也描述具体实现由 runtime 包完成，看一下 runtime 包中的代码，先引入了 unsafe 包，再定义了 //go:lickname time_now time.now。\n第一个参数time_now 代表本地变量或方法，第二个参数time.now标识需要建立链接的变量、方法路径。也就是说，//go:lickname 是可以跨包使用的。\n另外 go build 是无法编译 go:linkname的，必须使用单独的 compile 命令进行编译，因为 go build 会加上 -complete 参数，这个参数会检查到没有方法体的方法，并且不通过。\ngo:noscape 1 //go:noscape 该指令指定下一个有声明但没有主体（意味着实现有可能不是 Go）的函数，不允许编译器对其做逃逸分析。\n一般情况下，该指令用于内存分配优化。编译器默认会进行逃逸分析，会通过规则判定一个变量是分配到堆上还是栈上。\n但凡事有意外，一些函数虽然逃逸分析其是存放到堆上。但是对于我们来说，它是特别的。我们就可以使用 go:noescape 指令强制要求编译器将其分配到函数栈上。\n例如 1 2 3 4 // memmove copies n bytes from \u0026#34;from\u0026#34; to \u0026#34;to\u0026#34;. // in memmove_*.s //go:noescape func memmove(to, from unsafe.Pointer, n uintptr) 我们观察一下这个案例，它满足了该指令的常见特性。如下：\nmemmove_*.s: 只有声明，没有主体，其主体是由底层汇编实现的。\nmemmove: 函数功能，在栈上处理性能会更好。\ngo:noslip 1 //go:noslip 该指令指定文件中声明的下一个函数不得包含堆栈溢出检查。\n简单来讲，就是这个函数跳过堆栈溢出的检查。\n例如 1 2 3 4 //go:nosplit func key32(p *uintptr) *uint32 { return (*uint32)(unsafe.Pointer(p)) } go:nowritebarrierrec 1 //go:nowritebarrierrec 该指令表示编译器遇到写屏障时就会产生一个错误，并且允许递归。也就是这个函数调用的其他函数如果有写屏障也会报错。\n简单来讲，就是针对写屏障的处理，防止其死循环。\n例如 1 2 3 4 //go:nowritebarrierrec func gcFlushBgCredit(scanWork int64) { ... } go:yeswritebarrierrec 1 //go:yeswritebarrierrec 该指令与 go:nowritebarrierrec 相对，在标注 go:nowritebarrierrec 指令的函数上，遇到写屏障会产生错误。\n而当编译器遇到 go:yeswritebarrierrec 指令时将会停止。\n例如 1 2 3 4 //go:yeswritebarrierrec func gchelper() { ... } go:noinline 1 //go:noinline 该指令表示该函数禁止进行内联。\n1 2 3 4 //go:noinline func unexportedPanicForTesting(b []byte, i int) byte { return b[i] } 例如 我们观察一下这个案例，是直接通过索引取值，逻辑比较简单。如果不加上 go:noinline 的话，就会出现编译器对其进行内联优化。\n显然，内联有好有坏。该指令就是提供这一特殊处理。\ngo:norace 1 //go:norace 该指令表示禁止进行竞态检测。\n常见的形式就是在启动时执行 go run -race，能够检测应用程序中是否存在双向的数据竞争，非常有用。\n例如 1 2 3 4 //go:norace func forkAndExecInChild(argv0 *byte, argv, envv []*byte, chroot, dir *byte, attr *ProcAttr, sys *SysProcAttr, pipe int) (pid int, err Errno) { ... } go:notinheap 1 //go:notinheap 该指令常用于类型声明，它表示这个类型不允许从 GC 堆上进行申请内存。\n在运行时中常用其来做较低层次的内部结构，避免调度器和内存分配中的写屏障，能够提高性能。\n例如 1 2 3 4 5 6 7 8 9 // notInHeap is off-heap memory allocated by a lower-level allocator // like sysAlloc or persistentAlloc. // // In general, it\u0026#39;s better to use real types marked as go:notinheap, // but this serves as a generic type for situations where that isn\u0026#39;t // possible (like in the allocators). // //go:notinheap type notInHeap struct{} 每日一算 描述 2个逆序的链表，要求从低位开始相加，得出结果也逆序输出，返回值是逆序结果链表的头结点\n解题思路 需要注意的是进位的问题，极端情况如下：\nInput: (9 -\u0026gt; 9 -\u0026gt; 9 -\u0026gt; 9) + (1 -\u0026gt; ) Output 0 -\u0026gt; 0 -\u0026gt; 0 -\u0026gt; 0 -\u0026gt; 1\n为了处理方法统一，可以先建立一个虚拟头结点，这个虚拟头结点的Next指向真正的head，这样head不需要单独处理，直接wihle循环即可。另外判断循环终止的条件不用是 p.Next != nil，这样最后一位还需要额外计算，终止条件应该是 p != nil。\n代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 type ListNode struct { Val int Next *ListNode } func addTwoNumbers(l1 *ListNode, l2 *ListNode) *ListNode { if l1 == nil || l2 == nil{ return nil } // 虚拟头结点 head := \u0026amp;ListNode{ Val: 0, Next: nil, } current := head carry := 0\t// 是否需要进位 // 遍历 for l1 != nil || l2 != nil { var x, y int if l1 == nil { x = 0 }else{ x = l1.Val } if l2 == nil { y = 0 }else { y = l2.Val } current.Next = \u0026amp;ListNode{ Val: (x + y + carry) % 10, Next: nil, } current = current.Next carry = (x+y+carry) / 10 if l1 != nil { l1 = l1.Next } if l2 != nil { l2 = l2.Next } fmt.Println(\u0026#34;carry:\u0026#34;, carry) } if carry \u0026gt; 0 {\t// 最后一位相加又进位，要在尾结点再加一个结点 current.Next = \u0026amp;ListNode{ Val: carry % 10, Next: nil, } } return head.Next } ","date":"2021-12-16T09:35:02+08:00","permalink":"https://x-xkang.com/p/golang-%E6%BA%90%E7%A0%81%E9%87%8C%E7%9A%84-/go-%E6%8C%87%E4%BB%A4%E9%83%BD%E4%BB%A3%E8%A1%A8%E4%BB%80%E4%B9%88%E6%84%8F%E6%80%9D/","title":"Golang 源码里的 //go: 指令，都代表什么意思？"},{"content":"ASCII ascii码使用指定的7位或8位二进制数组合表示128或256种可能的字符，标准ASCII码也叫基础ASCII码，使用7位二进制数（剩下的1位二进制为0）来表示所有的大写和小写字母，数字0到9、标点符号，以及再美式英语中使用的特殊字符，其中：\n0 ~ 31及127（共33个）是控制字符或通信专用字符（其余为可显示字符），如控制字符：LF（换行）, CR（回车）；通信专用字符：SOH（文头）、ACK（确认）等；ASCII值为8、9、10和13分别转换为退格、制表、换行和回车，它们并没有特定的图形显示，但会依不同的应用程序，而对文本显示不同的影响。 32 ~ 126（共95个）是字符（32是空格），其中48~57位0到9十个阿拉伯数字，65 ~ 90为26个大写英文字母，97 ~ 122为26个小写英文字母，其余为一些标点符号、运算符号等。 同时还要注意，在标准ASCII中，其中最高位（b7）用作奇偶校验。所谓奇偶校验，是指在代码传送过程中用来校验是否出现错误的一种方法，一般分为奇校验和偶校验两种。\n奇校验规定：正确的代码一个字节中1的个数必须是1，若非技术，则在最高位b7添1. 偶校验规定：正确的代码一个字节中1的个数必须是偶数，若非偶数，则在最高位（b7）添1. 后128个称为扩展ASCII码。许多基于x86的系统都支持使用扩展（或“高”ASCII）。扩展ASCII码允许将每个字符的第8位用于确定附加的128个特殊符号字符、外来语字母和图形符号\nUnicode Unicode是国际组织制定的可以容纳世界上所有文字和符号的字符编码方案。Unicode用数字0-0x10FFFF来映射这些字符，最多可以容纳1114112个字符，或者说有1114112个码位。码位就是可以分配给字符的数字。UTF-8、UTF-16、UTF-32都是将数字转换到程序数据的编码方案。\nUnicode 源于一个很简单的想法：将全世界所有的字符包含在一个集合里，计算机只要支持这一个字符集，就能显示所有的字符，再也不会有乱码了。\n它从 0 开始，为每个符号指定一个编号，这叫做”码点”（code point）。比如，码点 0 的符号就是 null（表示所有二进制位都是 0）。 这么多符号，Unicode 不是一次性定义的，而是分区定义。每个区可以存放 65536 个（2^16）字符，称为一个平面（plane）。目前，一共有 17 个平面，也就是说，整个 Unicode 字符集的大小现在是 2^21。 最前面的 65536 个字符位，称为基本平面（缩写 BMP），它的码点范围是从 0 一直到 2^16-1，写成 16 进制就是从 U+0000 到 U+FFFF。所有最常见的字符都放在这个平面，这是 Unicode 最先定义和公布的一个平面。 剩下的字符都放在辅助平面（缩写 SMP），码点范围从 U+010000 一直到 U+10FFFF。 Unicode 只规定了每个字符的码点，到底用什么样的字节序表示这个码点，就涉及到编码方法。 Unicode 编码方案 之前提到，Unicode 没有规定字符对应的二进制码如何存储。以汉字“汉”为例，它的 Unicode 码点是 0x6c49，对应的二进制数是 110110001001001，二进制数有 15 位，这也就说明了它至少需要 2 个字节来表示。可以想象，在 Unicode 字典中往后的字符可能就需要 3 个字节或者 4 个字节，甚至更多字节来表示了。 这就导致了一些问题，计算机怎么知道你这个 2 个字节表示的是一个字符，而不是分别表示两个字符呢？这里我们可能会想到，那就取个最大的，假如 Unicode 中最大的字符用 4 字节就可以表示了，那么我们就将所有的字符都用 4 个字节来表示，不够的就往前面补 0。这样确实可以解决编码问题，但是却造成了空间的极大浪费，如果是一个英文文档，那文件大小就大出了 3 倍，这显然是无法接受的。 于是，为了较好的解决 Unicode 的编码问题， UTF-8 和 UTF-16 两种当前比较流行的编码方式诞生了。当然还有一个 UTF-32 的编码方式，也就是上述那种定长编码，字符统一使用 4 个字节，虽然看似方便，但是却不如另外两种编码方式使用广泛。 UTF8 UTF-8 是一个非常惊艳的编码方式，漂亮的实现了对 ASCII 码的向后兼容，以保证 Unicode 可以被大众接受。\nUTF-8 是目前互联网上使用最广泛的一种 Unicode 编码方式，它的最大特点就是可变长。它可以使用 1 - 4 个字节表示一个字符，根据字符的不同变换长度。编码规则如下： 1.对于单个字节的字符，第一位设为 0，后面的 7 位对应这个字符的 Unicode 码点。因此，对于英文中的 0 - 127 号字符，与 ASCII 码完全相同。这意味着 ASCII 码那个年代的文档用 UTF-8 编码打开完全没有问题。 2.对于需要使用 N 个字节来表示的字符（N \u0026gt; 1），第一个字节的前 N 位都设为 1，第 N + 1 位设为 0，剩余的 N - 1 个字节的前两位都设位 10，剩下的二进制位则使用这个字符的 Unicode 码点来填充。 编码规则如下：\nUnicode 十六进制码点范围 UTF-8 二进制 0000 0000 - 0000 007F 0xxxxxxx 0000 0080 - 0000 07FF 110xxxxx 10xxxxxx 0000 0800 - 0000 FFFF 1110xxxx 10xxxxxx 10xxxxxx 0001 0000 - 0010 FFFF 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx 根据上面编码规则对照表，进行 UTF-8 编码和解码就简单多了。下面以汉字“汉”为利，具体说明如何进行 UTF-8 编码和解码。\n“汉”的 Unicode 码点是 0x6c49（110 1100 0100 1001），通过上面的对照表可以发现，0x0000 6c49 位于第三行的范围，那么得出其格式为 1110xxxx 10xxxxxx 10xxxxxx。接着，从“汉”的二进制数最后一位开始，从后向前依次填充对应格式中的 x，多出的 x 用 0 补上。这样，就得到了“汉”的 UTF-8 编码为 11100110 10110001 10001001，转换成十六进制就是 0xE6 0xB7 0x89。\n解码的过程也十分简单：如果一个字节的第一位是 0 ，则说明这个字节对应一个字符；如果一个字节的第一位 1，那么连续有多少个 1，就表示该字符占用多少个字节。\n每日一算 描述 在数组中找到 2 个数之和等于给定值的数字，结果返回 2 个数字在数组中的下标\n解题思路 利用map的特性，遍历数组预算计算出和给定值的差值，也就是目标元素，若值已经在map中，说明已找到，若没有，则把当前元素以 值=\u0026gt;下标 的形式存到map中\n代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func twoSum(arr []int, target int)[]int{ var m = map[int]int{} for i := 0; i \u0026lt; len(arr); i++ { another := target - arr[i] if _, ok := m[another]; ok == true { return []int{m[another], i} }else{ m[arr[i]] = i } } return []int{-1, -1} } ","date":"2021-12-02T09:31:17+08:00","permalink":"https://x-xkang.com/p/asciiunicode%E5%92%8Cutf8/","title":"ASCII、Unicode和UTF8"},{"content":"问题描述 生产环境每隔一段时间会出现mysql数据库的死锁日志:\n同一条update语句，where 条件不同，但是会触发死锁，于是查看阿里云的数据库死锁日志，如下：\nupdate 语句如果使用的是主键索引，会将主键索引锁住，如果是普通索引，会先将普通索引锁住，然后根据普通索引的主键id再锁住主键索引，同一条update语句的索引执行应该是一样的，不应该存在互相等待释放的情况，于是有点陷入僵局，google一下有遇到相似问题的帖子，查看了执行计划，之前也看了执行计划，但是忽略了type字段和extra字段，结果如下：\n索引合并查询，会同时使用idx_status_vmstatus 和 uniq_instance 扫描记录并给普通索引加锁，然后通过普通索引中的主键ID去锁定主键索引，问题就出现在这里，由于 idx_status_vmstatus 索引扫描和 uniq_instance索引扫描是同时的，如果两条update语句同时执行，则 事务2 先锁定 锁定 uniq_instance 成功后锁定对应的主键，然后事务1 锁定idx_status_vmstatus 成功后也去锁定主键,此时主键已被事务2锁定，于是阻塞等待primary释放，接着事务2开始扫描 idx_status_vmstatus 发现普通索引被事务1锁住，于是阻塞等待idx_status_vmstatus，于是出现最终的 事务2等待 事务2释放idx_status_vmstatus，事务1等待事务1释放primary，即出现死锁。\n解决方案也比较简单，先查出主键ID，使用主键ID再更新记录，因为使用主键ID直接加锁的话，锁粒度更小，及时同时更新一条记录，也不会出现同时等待对方将锁释放的场景。问题描述的比较简单，但在排查过程中还是走了不少弯路的。\n","date":"2021-11-22T10:48:54+08:00","permalink":"https://x-xkang.com/p/mysql-deadlock/","title":"Mysql Deadlock"},{"content":"一、GMT 1、什么是GMT GMT (Greenwich Mean Time) 格林威治标准时间\n它规定太阳每天经过位于英国伦敦郊区的皇家格林威治天文台的时间为中午12点。 2、GMT的历史 格林威治皇家天文台为了海上霸权的扩张计划，在十七世纪就开始进行天体观测。为了天文观测，选择了穿过英国伦敦格林威治天文台子午仪中心的一条经线作为零度参考线，这条线，简称格林威治子午线。\n1884年10月在美国华盛顿召开了一个国际子午线会议，该会议将格林威治子午线设定为本初子午线，并将格林威治平时 (GMT, Greenwich Mean Time) 作为世界时间标准（UT, Universal Time）。由此也确定了全球24小时自然时区的划分，所有时区都以和 GMT 之间的偏移量做为参考。\n1972年之前，格林威治时间（GMT）一直是世界时间的标准。1972年之后，GMT 不再是一个时间标准了。\n由于地球在它的椭圆轨道里的运动速度不均匀，这个时间可能和实际的太阳时相差16分钟。\n二、UTC 1、什么是UTC ？ UTC (Coodinated Universal Time)，协调世界时\n又称世界统一时间、世界标准时间、国际协调时间。由于英文（CUT）和法文（TUC）的缩写不同，作为妥协，简称UTC\nUTC是现在全球通用的时间标准，全球各地都同意将各自的时间进行同步协调。UTC时间是经过平均太阳时（以格林威治时间GMT为准）、地轴运动修正后的新时标以及以秒为单位的国际原子时所综合精算而成\n在军事中协调世界时会用”Z“来表示。又由于Z在无线电联络中使用”Zulu“作代称，协调世界时也会被称为”Zulu time“。\n2、UTC的组成 原子时间（TAI，International Atomic Time）：结合了劝阻400个所有的原子钟而得到的时间，它决定了我们每个人的中标中，时间流动的速度。 世界时间（UT，Universal Time）： 也称天文时间，或太阳时，它的依据是地球的自转，我们用它来确定多少原子时，对应于一个地球日的时间长度。 3、UTC的历史 1960年，国际无线电咨询委员会规范统一了UTC的概念，并在次年投入实际使用。1967年以前，UTC被数次调整过，原因是要使用润秒（leap second）来将UTC和地球自转时间进行统一。\n三、GMT与UTC GMT是前世界标准时，UTC是现世界标准时； UTC比GMT更准确，以原子时计时，适应现代社会的精准计时，但在不需要精确到秒的情况下，二者可视为等同； 每年格林尼治天文台会发调时信息，给予UTC。 ","date":"2021-09-29T13:38:27+08:00","permalink":"https://x-xkang.com/p/gmt-%E4%B8%8E-utc-%E6%97%B6%E9%97%B4%E6%A0%BC%E5%BC%8F/","title":"GMT 与 UTC 时间格式"},{"content":"我们平时使用的查询 sql 基本格式如下：\n1 2 3 4 5 6 7 8 9 SELECT DISTINCT \u0026lt;select_list\u0026gt; FROM \u0026lt;left_table\u0026gt; \u0026lt;join_type\u0026gt; JOIN \u0026lt;right_table\u0026gt; ON \u0026lt;join_condition\u0026gt; WHERE \u0026lt;where_condition\u0026gt; GROUP BY \u0026lt;group_by_condition\u0026gt; HAVING \u0026lt;having_condition\u0026gt; ORDER BY \u0026lt;order_by\u0026gt; LIMIT \u0026lt;limit_number\u0026gt;; 实际的执行顺序并不是如上书写顺序一样的：\nFROM: 对 from 左右的表计算笛卡尔积，产生虚拟表VT1； ON: 对笛卡尔积进行筛选，只有符合条件的行才会被记录到虚拟表VT2中； JOIN: 如果是 OUT JOIN，那么将保留表中（如左表或者右表）未匹配的行作为外部行添加到虚拟表VT2中，从而产生了虚拟表VT3； WHERE: 对 JOIN 之后的虚拟表VT3进行进一步的筛选，满足条件的留下生成虚拟表VT4； GROUP BY: 对虚拟表VT4进行分组，生成VT5； HAVING: 对分组后的VT5进行筛选，生成虚拟表VT6； SELECT: 选择 SELECT 指定的列，插入到虚拟表VT7中； DISTINCT: 对虚拟表VT7中的数据进行去重，产生VT8； ORDER BY: 对虚拟表VT8的中的数据进行排序生成VT9； LIMIT: 取出VT9中指定行的数据，产生虚拟表VT10，并返回数据 ","date":"2021-09-25T17:18:01+08:00","permalink":"https://x-xkang.com/p/mysql--%E5%85%B3%E9%94%AE%E5%AD%97%E7%9A%84%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F/","title":"Mysql -- 关键字的执行顺序"},{"content":"一、Int 1、string 转 int 1 2 3 4 5 6 var str = \u0026#34;1001\u0026#34; n, _ := strconv.Atoi(str) // 官方 \u0026#34;strconv\u0026#34; 包 fmt.Printf(\u0026#34;type:%T, value:%v\\n\u0026#34;, n, n) // type:int, value:1001 2、string 转 int64 1 2 3 4 5 6 var str = \u0026#34;1001\u0026#34; n, _ := strconv.ParseInt(str, 10, 64) fmt.Printf(\u0026#34;type:%T, value:%v\\n\u0026#34;, n, n) // type:int64, value:1001 3、string 转浮点数 1 2 3 4 5 6 var str = \u0026#34;3.1415926\u0026#34; n, _ := strconv.ParseFloat(str, 10) fmt.Printf(\u0026#34;type:%T, value:%v\\n\u0026#34;, n, n) // type:float64, value:3.1415926 二、String 1、int 转 string 1 2 3 4 5 6 var n int = 100 str := strconv.Itoa(n) fmt.Printf(\u0026#34;type:%T, value:%v\\n\u0026#34;, str, str) // type:string, value:100 2、int64 转 string 1 2 3 4 5 6 var n int64 = 100 str := strconv.FormatInt(n, 10) fmt.Printf(\u0026#34;type:%T, value:%v\\n\u0026#34;, str, str) // type:string, value:100 3、uint32 转 string 1 2 3 4 5 6 var n uint32 = 10 str := strconv.ParseUint(uint64(n), 10) fmt.Printf(\u0026#34;type:%T, value:%v\\n\u0026#34;, str, str) // type:string, value:100 三、Struct 1、json 转 struct 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 type Person struct { Name string `json:\u0026#34;name\u0026#34;` Age int `json:\u0026#34;age\u0026#34;` } var jsonStr = []byte(`{\u0026#34;name\u0026#34;: \u0026#34;neil\u0026#34;, \u0026#34;age\u0026#34;: 21}`) var p = Person{} err := json.Unmarshal(jsonStr, \u0026amp;p) // 官方 “json” 包 if err != nil { fmt.Println(\u0026#34;Unmarshal failed:\u0026#34;, err) return } fmt.Printf(\u0026#34;p:%#v\\n\u0026#34;, p) // p:main.Person{Name:\u0026#34;neil\u0026#34;, Age:21} 2、map 转 struct 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 type Person struct { Name string `json:\u0026#34;name\u0026#34;` Age int `json:\u0026#34;age\u0026#34;` } var m = map[string]interface{}{ \u0026#34;name\u0026#34;: \u0026#34;king\u0026#34;, \u0026#34;age\u0026#34;: 19, } jsonStr, err := json.Marshal(m) if err != nil { fmt.Println(\u0026#34;Marshal failed:\u0026#34;, err) return } err = json.Unmarshal(jsonStr, \u0026amp;p) if err != nil { fmt.Println(\u0026#34;Unmarshal failed too:\u0026#34;, err) return } fmt.Printf(\u0026#34;p:%#v\\n\u0026#34;, p) // p:main.Person{Name:\u0026#34;king\u0026#34;, Age:19} Map 1、json 转 map 1 2 3 4 5 6 7 8 9 10 11 12 13 14 var jsonStr = []byte(`{\u0026#34;name\u0026#34;: \u0026#34;neil\u0026#34;, \u0026#34;age\u0026#34;: 21}`) var m = map[string]interface{}{} err := json.Unmarshal(jsonStr, \u0026amp;m) if err != nil { fmt.Println(\u0026#34;Unmarshal failed:\u0026#34;, err) return } fmt.Printf(\u0026#34;m:%#v\\n\u0026#34;, m) // m:map[string]interface {}{\u0026#34;age\u0026#34;:21, \u0026#34;name\u0026#34;:\u0026#34;neil\u0026#34;} 2、 struct 转 map 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 type Person struct { Name string `json:\u0026#34;name\u0026#34;` Age int `json:\u0026#34;age\u0026#34;` } var p = Person{ Name: \u0026#34;king\u0026#34;, Age: 17, } var jsonStr, _ = json.Marshal(p) var m = map[string]interface{}{} _ = json.Unmarshal(jsonStr, \u0026amp;m) fmt.Printf(\u0026#34;m:%#v\\n\u0026#34;, m) // m:map[string]interface {}{\u0026#34;age\u0026#34;:17, \u0026#34;name\u0026#34;:\u0026#34;king\u0026#34;} ","date":"2021-06-24T13:58:33+08:00","permalink":"https://x-xkang.com/p/golang-%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2/","title":"Golang 常用数据类型转换"},{"content":"一、Mysql 事务 MySQL 事务主要用于处理操作量大、复杂度高的数据\nMySQL 数据库中只有 Innodb 存储引擎支持事务操作 事务处理可以用来维护数据库的完整性，保证成批的 SQL 要么全部执行，要么全部不执行 事务用来管理insert，update，delete语句 二、事务特性 一般来说，事务必须满足 4 个条件（ACID），即原子性、一致性、持久性、隔离性，具体如下：\n1、原子性 (Atomicity) 一个事务（Transaction）中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节，事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像事务没有执行过一样。\n2、一致性（Consistency） 在事务开始之前以及事务结束之后，数据库的完整性没有被破坏。这标识写入的数据必须完全符合所有的预设规则，这包含数据的精确度，串联性以及后续数据库可以自发的完成预定的工作。\n3、隔离性（Isolation） 数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致，事务隔离分为以下不同级别：\n读未提交（Read uncommited）: 允许脏读，也就是可能读到其他会话中未提交事务修改的数据。\n读已提交（Read commited）: 只能读取到已提交的数据。\n可重复读（Repeatable read）: 在同一个事务内的查询都是从开始时刻一致的，InnoDB 存储引擎默认的事务隔离级别就是可重复读，在 SQL 标准中，该隔离级别消除了不可重复读，但还是存在幻读。\n串行化（Serializable）: 完全串行化的读，每次读都需要获得表级的共享锁，读写相互都会阻塞。\n4、持久性（Durability） 事务处理结束后，对数据的修改就是永久的，几遍系统故障也不会丢失。\n三、事务的并发处理 准备工作：创建数据表，插入一条数据\n1 2 3 4 5 6 7 8 create table user( id int(10) not null auto_increment comment \u0026#39;主键ID\u0026#39;, name varchar(30) not null default \u0026#39;\u0026#39; comment \u0026#39;用户名\u0026#39;, primary key(id) ) engine=innodb charset=utf8mb4; # 插入数据 insert into `user`(`name`) values(\u0026#39;老王01\u0026#39;); 事务并发可能出现的情况：\n脏读 一个事务读到了另一个未提交事务修改过的数据\n1、会话 B 开启一个事务，把id=1的name改为老王01；\n2、会话 A 也开启一个事务，读取id=1的name，次时的查询结果为老王02；\n3、会话 B 的事务回滚了修改的操作，这样会话 A 读到的数据就是不存在的；\n这个现象就是脏读。（脏读只会在读未提交的隔离级别中才会出现）。\n不可重复读 一个事务只能读到另一个已经提交的事务修改过的数据，并且其他事务每对该数据进行一次修改并提交后，该事务都能查询得到最新值。（不可重复读在读未提交和读已提交隔离级别都可能会出现）\n1、会话 A 开启事务，查询id=1的 name 是老王01；\n2、会话 B 将id=1的 name 更新为老王02（隐式事务，autocommit=1，执行完 sql 会自动 commit）；\n3、会话 A 再查询时id=1的 name 为老王02；\n4、会话 B 又将id=1的 name 更新为老王03；\n5、会话 A 再查询id=1的 name 时，结果为老王03。\n这种现象就是不可重复读。\n幻读 一个事务先根据某些条件查出一些记录，之后另一个事务又想表中插入了符合这些条件的记录，原先的事务再次按照该条件查询时，能够把另一个事务插入的数据也查出来。 （幻读在读未提交、读已提交、可重复读隔离级别中都可能会出现）\n1、会话 A 开始事务，查询id\u0026gt;0的数据，结果只有 name=老王 01 的一条数据\n2、会话 B 像数据表中插入了一条name=老王02的数据（隐式事务，执行 sql 后自动 commit）\n3、会话 A 的事务再次查询 id\u0026gt;0的数据\n不同隔离级别下出现事务并发问题的可能 隔离级别 脏读 不可重复读 幻读 读未提交 可能 可能 可能 读已提交 不可能 可能 可能 可重复读 不可能 不可能 可能 串行化 不可能 不可能 不可能 四、事务的实现原理 首先了解一下 redo log 和 undo log\n1、redo log(重做日志) MySQL 为了提升性能不会把每次的修改都实时同步到磁盘，而是会优先存储到 Buffer Pool（缓冲池）里面，把这个当做缓存来用，然后使用后台线程去做缓冲池和磁盘之间的同步\n如果还没来得及同步数据就出现宕机或者断电，就会导致丢失部分已提交事务的修改信息，\n所以引入了redo log来记录已成功提交事务的修改信息，并且把 redo log 持久化到磁盘，系统重启之后读取 redo log 恢复最新数据\nredo log 是用来恢复数据的，用于保障已提交事务的持久化特性。\n2、undo log undo log 叫做回滚日志，用于记录数据被修改前的信息，与 redo log 记录的数据相反，redo log 是记录修改后的数据，undo log 记录的是数据的逻辑变化，为了发生错误时回滚之前的操作，需要将之前的操作都记录下来，然后在发生错误时才可以回滚\n每次写入数据或者修改数据之前都会把修改前的信息记录到 undo log\n3、事务特性的具体实现原理 事务的原子性通过 undo log 来实现的 事务的持久性是通过 redo log 实现的 事务的隔离性是通过 读写锁 + MVCC 实现的 事务的一致性是通过 **原子性、持久性、隔离性**来实现的 3.1、原子性的实现 每条数据变更（insert/update/delete）操作都会记录一条undo log，并且undo log必须先于数据持久化到磁盘上。\n所谓的回滚就是根据undo log做逆向操作，比如delete的逆向操作是insert，insert的逆向操作是delete，update的逆向操作是update等。\n为了做到同时成功或者同时失败，当系统发生错误或者执行rollback时需根据undo log进行回滚\n3.2、持久性的实现 Mysql 的数据存储机制是将数据最终持久化到磁盘上，并且频繁的进行磁盘 IO 是非常消耗性能的，为了提升性能，InnoDB 提供了缓冲池（Buffer Pool），缓冲池中包含了磁盘数据也的映射，可以当做缓存来使用\n读数据：会首先从缓冲池中读取，若没有，则从磁盘读取并放入缓冲池中\n写数据：会首先写入缓冲池中，缓冲池中的数据会定期同步到磁盘中\n那么问题来了，如果在缓冲池的数据还没有同步到磁盘上时，出现了机器宕机或者断电，可能会出现数据丢失的问题，因此我们需要记录已提交事务的数据，于是，redo log登场了， redo log 在执行数据变更（insert/update/delete）操作的时候，会变更后的结果记录在缓冲区，待commit事务之后同步到磁盘\n至于redo log也要进行磁盘 IO，为什么还要用\n(1)、redo log是顺序存储，而缓存同步是随机操作\n(2)、缓存同步是以数据页为单位，每次传输的数据大小小于redo log\n3.3、隔离性的实现 读未提交： 读写并行，读的操作不能排斥写的操作，因此会出现脏读,不可重复读,幻读的问题\n读已提交： 使用排他锁X，更新数据需要获取排他锁，已经获取排他锁的数据，不可以再获取共享锁S以及排他锁X，读取数据使用了MVCC（Mutil-Version Concurrency Control）多版本并发控制机制（后续单独展开）以及Read view的概念，每次读取都会产生新的Read view，因此可以解决脏读问题，但解决不了不可重复读和幻读的问题\n可重复读： 同上也是利用MVCC机制实现，但是只在第一次查询的时候创建Read view，后续的查询还是沿用之前的Read view，因此可以解决不可重复读的问题，具体不在这展开，但还是有可能出现幻读\n串行化 ：读操作的时候加共享锁，其他事务可以并发读，但是不能并发写，执行写操作的时候加排他锁，其他事务既不能并发写，也不能并发读，串行化可以解决事务并发中出现的脏读、不可重复读、幻读问题，但是并发性能却因为加锁的开销变得很差\n3.4、一致性的实现 一致性的实现其实是通过原子性、持久性，隔离性共同完成的\n五、结束语 了解 MySQL 的事务机制，以及实现原理，对于使用或者优化都有很大的帮助，要保持知其然和知其所以然的心态和持续学习的劲头，了解更多关于 Mysql 相关的知识！\n","date":"2021-03-24T11:18:58+08:00","permalink":"https://x-xkang.com/p/mysql--%E4%BA%8B%E5%8A%A1%E6%B5%85%E6%9E%90/","title":"MySQL -- 事务浅析"},{"content":"一、问题描述 1、生产环境主站每隔一段时间就会出现卡顿，接口响应慢，甚至超时的情况、\n2、测试环境重现不了（一抹诡异的光）\n二、问题排查 针对响应慢的接口进行优化，之前的代码风格也存在问题，还是有些滥用sync/await，一些没有依赖关系的操作，全部分开每行await同步执行，分析后把部分DB操作合并一个Promise执行 阿里云查了一下mysql的slow_log，有挺多的慢查询，优化了一部分SQL，业务逻辑太复杂，但是！没有解决问题，主站还是隔一段时间就卡 找到部分接口日志，超时的接口返回的是Knex.js数据库管理工具抛出的异常，KnexTimeoutError: Knex: Timeout acquiring a connection. The pool is probably full.，可能是连接池已满，获取连接失败导致的 看了一下数据库连接池的配置，最大连接数是30，获取连接的超时时间是60s，可能是并发量大加上部分操作未释放连接导致后续的操作无法正常获取数据库连接池的连接，大概又定位了一下可能的问题点\n1，主站的信息列表会隔几秒钟轮询，获取最新的数据，如果1000个用户在线，轮询周期内就会有1000个查询，中间也没有做缓存处理，导致并发到DB的请求会比较多 2，为了维护DB的状态统一，用户的部分操作用了事务，一些事务内包含了太多操作（感觉是长期占用连接未释放的罪魁祸首） 3，测试环境重现不了是因为没有经过压测，只测试了功能，没有测试性能，日常测试也没有大并发 三、问题验证 将数据库连接池的数量改成了1，使用事务的接口中做了延时的transaction.commit()操作，然后另外一个请求再去正常查询，\n结果显示，如果一个用户调用了事务操作的接口，然后再调用查询接口，查询会一直阻塞在获取连接的步骤，直至事务commit之后释放连接，如果在配置的timeout时间之前没有获取到，Knex就会抛出Timeout acquiring a connection. The pool is probably full的异常，\n也侧面印证了为什么测试环境复现不了这种情况，毕竟在没有压测的前提下。两个测试通过手动操作，并发量是达不到配置的数量的，也就不会出现卡顿的情况\n四、解决方案 1，优化长事务的操作，减少不必要的事务，提高处理效率（难度较大，业务逻辑比较复杂）；\n2，合理范围内增加数据库连接池的最大连接数配置，线上的mysql可连接300个，后端3个服务，现在配置是10、30、30，先把主站改50看看，连接数太大也会导致磁盘I/O效率大幅降低又会导致其他问题；\n3，轮询获取列表的操作，可以改成服务端主动去推（使用socket.io），然后加一个中间缓存（redis），毕竟列表数据变化的频率不是很高；\n4，数据库扩展成读写分离，update和insert的操作直接操作主库，大部分select操作转移到从库，即使有部分的事务操作慢，也不会导致主站的基本查询卡住\n五、写在最后 系统的业务逻辑比较复杂，从业务代码层面下手成本还是比较高，接口的耦合都比较高，重构都比改的成本低，开始的设计，可能也没有考虑扩展的问题，并发的问题等， 包括每个服务之间的通信问题，后期再慢慢优化吧，不怕有问题，就怕一直没遇到过问题！加油~。\n","date":"2021-03-12T17:42:20+08:00","permalink":"https://x-xkang.com/p/mysql--%E8%BF%9E%E6%8E%A5%E6%B1%A0%E9%97%AE%E9%A2%98/","title":"Mysql -- 连接池问题"},{"content":" 资源转自[B站]，如侵删\n","date":"2021-02-04T13:35:56+08:00","permalink":"https://x-xkang.com/p/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%89/","title":"计算机网络学习笔记（三）"},{"content":" 资源转自[B站]，如侵删\n","date":"2021-02-04T09:55:34+08:00","permalink":"https://x-xkang.com/p/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%BA%8C/","title":"计算机网络学习笔记（二）"},{"content":" 资源转自[B站]，如侵删\n","date":"2021-01-22T16:13:30+08:00","permalink":"https://x-xkang.com/p/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%80/","title":"计算机网络学习笔记（一）"},{"content":"一、物理层基本概念 物理层解决如何在连接各种计算机的传输媒体上传输数据比特流，而不是指具体的传输媒体，物理层的主要任务是确定与传输媒体接口有关的一些特性\n1、机械特性 定义物理连接的特性，规定物理连接时所采用的规格，接口形状，引线数目，引脚数量和排列情况\n2、电气特性 规定传输二进制位时，线路上信号的电压范围，阻抗匹配，传输速率和距离限制\n3、功能特性 知名某条线上出现的某一电平标识何种意义，接口不见的信号线的用途\n4、规程特性 (过程特性)定义各条物理线路的工作规程和时序关系\n二、数据通信基础知识 1、典型的数据通信模型 2、数据通信相关术语 通信的目的是传送消息\n数据：传送消息的实体，通常是有意义的符号序列\n信号：数据的电气/电磁表现，是数据在传输过程中的存在形式\n数字信号：代表消息的参数取值是离散的 模拟信号：代表消息的参数是连续的 信源：产生和发送数据的源头\n信宿：接受数据的终点\n信道：信号的传输媒介，一般用来表示向某一个方向传送信息的介质，因此一条通信线路往往包含一条发送信道和一条接受信道\n3、三种通信方式 从通信双方信息的交互方式来看，可以有三种基本方式：\n单工通信：只有一个方向的通信而没有反方向的交互，仅需一条信道\n半双工信道：通信的双方都可以发送或接受信息，但任何一方都不能同时发送和接受，需要两条信道\n全双工信道：通信双方可以同时发送和接受信息，也需要两条信道\n4、两种数据传输方式 串行传输：速度慢、费用低、适合远距离\n并行传输：速度快、费用高、适合近距离\n三、码元、波特、速率、带宽 1、码元 码元： 是指用给一个固定时长的信号波形（信号波形），代表不同离散数值的基本波形，是数字通信中数字信号的计量单位，这个时长内的信号称为k进制码元，而该时长称为码元宽度，当码元的离散状态有M个时（M大于2）此时码元为M进制码元\n1码元可以携带多个比特的信息量， 例如： 在使用二进制编码时，只有两种不同的码元，一种代表0状态，另一种代表1状态\n2、速率、波特、带宽 速率： 也叫 数据率 是指数据的传输速率，表示单位时间内传输的数据量，可以用码元传输速率和信息传输速率表示\n1）码元传输速率： 别名码元速率、波形速率、调制速率、符号速率等，它标识单位时间内数字通讯系统所传输的码元个数（也可称为脉冲个数或信号变化的次数），单位是 波特(Baud)。1波特表示数字通讯系统每秒传输一个码元，这里的码元可以是多进制的，但码元速率与进制无关。\n2）信息传输速率： 别名信息速率、比特率等，表示单位时间内数字通讯系统传输的二进制码元个数（即比特数），单位是 比特/秒（b/s）\n关系： 若一个码元携带 n bit的信息量，则M Baud的码元传输速率所对应的信息传输速率为 M x n bit/s\n带宽： 表示在单位时间内从网络中的某一点到另一点所能通过的 “最高数据率”，常用来表示网络的通信线路所能传输数据的能力，单位是b/s。\n四、奈氏准则和香农定理 1、失真 2、码间串扰 3、奈氏准则 4、香农定理 五、编码与调制 1、基带信号和宽带信号 2、编码与调制 1）非归零编码\n2）归零编码\n3）反向不归零编码\n4）曼彻斯特编码\n5）差分曼彻斯特编码\n6）4B/5B编码\n7）数字数据调制为模拟信号\n8）模拟数据编码为数字信号\n","date":"2020-01-25T17:21:56+08:00","permalink":"https://x-xkang.com/p/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C--%E7%89%A9%E7%90%86%E5%B1%82/","title":"计算机网络--物理层"},{"content":"一、模式切换 i 切换到输入模式，以输入字符。\nx 删除当前光标所在处的字符。\n: 切换到底线命令模式，以在最底一行输入命令。\n二、输入模式 在命令模式下按下i就进入了输入模式。 在输入模式中，可以使用以下按键：\n字符按键以及Shift组合，输入字符\nENTER，回车键，换行\nBACK SPACE，退格键，删除光标前一个字符\nDEL，删除键，删除光标后一个字符\n↑/↓/←/→ 方向键，在文本中移动光标\nHOME/END，移动光标到行首/行尾\nPage Up/Page Down，上/下翻页\nInsert，切换光标为输入/替换模式，光标将变成竖线/下划线\nESC，退出输入模式，切换到命令模式\n三、命令模式 1、移动光标 命令 作用 h 或 向左箭头键(←) 光标向左移动一个字符 j 或 向下箭头键(↓) 光标向下移动一个字符 k 或 向上箭头键(↑) 光标向上移动一个字符 l 或 向右箭头键(→) 光标向右移动一个字符 [Ctrl] + [f] 屏幕『向下』移动一页，相当于 [Page Down]按键 (常用) [Ctrl] + [b] 屏幕『向上』移动一页，相当于 [Page Up] 按键 (常用) [Ctrl] + [d] 屏幕『向下』移动半页 [Ctrl] + [u] 屏幕『向上』移动半页 + 光标移动到非空格符的下一行 - 光标移动到非空格符的上一行 n\u0026lt;space\u0026gt; 那个 n 表示『数字』，例如 20 。按下数字后再按空格键，光标会向右移动这一行的 n 个字符。例如 20\u0026lt;space\u0026gt; 则光标会向后面移动 20 个字符距离。 0 或功能键[Home] 这是数字『 0 』：移动到这一行的最前面字符处 (常用) $ 或功能键[End] 移动到这一行的最后面字符处(常用) H 光标移动到这个屏幕的最上方那一行的第一个字符 M 光标移动到这个屏幕的中央那一行的第一个字符 L 光标移动到这个屏幕的最下方那一行的第一个字符 G 移动到这个档案的最后一行(常用) nG n 为数字。移动到这个档案的第 n 行。例如 20G 则会移动到这个档案的第 20 行(可配合 :set nu) gg 移动到这个档案的第一行，相当于 1G 啊！ (常用) n\u0026lt;Enter\u0026gt; n 为数字。光标向下移动 n 行(常用) 2、搜索替换 命令 作用 /word 向光标之下寻找一个名称为 word 的字符串。例如要在档案内搜寻 vbird 这个字符串，就输入 /vbird 即可！ (常用) ?word 向光标之上寻找一个字符串名称为 word 的字符串。 n 这个 n 是英文按键。代表重复前一个搜寻的动作。举例来说， 如果刚刚我们执行 /vbird 去向下搜寻 vbird 这个字符串，则按下 n 后，会向下继续搜寻下一个名称为 vbird 的字符串。如果是执行 ?vbird 的话，那么按下 n 则会向上继续搜寻名称为 vbird 的字符串！ N 这个 N 是英文按键。与 n 刚好相反，为『反向』进行前一个搜寻动作。 例如 /vbird 后，按下 N 则表示『向上』搜寻 vbird 。 :n1,n2s/word1/word2/g n1 与 n2 为数字。在第 n1 与 n2 行之间寻找 word1 这个字符串，并将该字符串取代为 word2 ！举例来说，在 100 到 200 行之间搜寻 vbird 并取代为 VBIRD 则：『:100,200s/vbird/VBIRD/g』。(常用) :1,$s/word1/word2/g或 :%s/word1/word2/g 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！(常用) :1,$s/word1/word2/gc 或 :%s/word1/word2/gc 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！且在取代前显示提示字符给用户确认 (confirm) 是否需要取代！(常用) 【注】：使用 /word 配合 n 及 N 是非常有帮助的！可以让你重复的找到一些你搜寻的关键词！\n3、删除、复制与粘贴 命令 作用 x, X 在一行字当中，x 为向后删除一个字符 (相当于 [del] 按键)， X 为向前删除一个字符(相当于 [backspace] 亦即是退格键) (常用) nx n 为数字，连续向后删除 n 个字符。举例来说，我要连续删除 10 个字符， 『10x』。 dd 删除游标所在的那一整行(常用) ndd n 为数字。删除光标所在的向下 n 行，例如 20dd 则是删除 20 行 (常用) d1G 删除光标所在到第一行的所有数据 dG 删除光标所在到最后一行的所有数据 d$ 删除游标所在处，到该行的最后一个字符 d0 那个是数字的 0 ，删除游标所在处，到该行的最前面一个字符 yy 复制游标所在的那一行(常用) nyy n 为数字。复制光标所在的向下 n 行，例如 20yy 则是复制 20 行(常用) y1G 复制游标所在行到第一行的所有数据 yG 复制游标所在行到最后一行的所有数据 y0 复制光标所在的那个字符到该行行首的所有数据 y$ 复制光标所在的那个字符到该行行尾的所有数据 p, P p 为将已复制的数据在光标下一行贴上，P 则为贴在游标上一行！ 举例来说，我目前光标在第 20 行，且已经复制了 10 行数据。则按下 p 后， 那 10 行数据会贴在原本的 20 行之后，亦即由 21 行开始贴。但如果是按下 P 呢？ 那么原本的第 20 行会被推到变成 30 行。 (常用) J 将光标所在行与下一行的数据结合成同一行 c 重复删除多个数据，例如向下删除 10 行，[ 10cj ] u 复原前一个动作。(常用) [Ctrl]+r 重做上一个动作。(常用) . 不要怀疑！这就是小数点！意思是重复前一个动作的意思。 如果你想要重复删除、重复贴上等等动作，按下小数点『.』就好了！ (常用) 【注】：这个 u 与 [Ctrl]+r 是很常用的指令！一个是复原，另一个则是重做一次～ 利用这两个功能按键，你的编辑，嘿嘿！很快乐的啦！\n4、进入输入或取代的编辑模式 命令 作用 i, I 进入输入模式(Insert mode)： i 为『从目前光标所在处输入』， I 为『在目前所在行的第一个非空格符处开始输入』。 (常用) a, A 进入输入模式(Insert mode)： a 为『从目前光标所在的下一个字符处开始输入』， A 为『从光标所在行的最后一个字符处开始输入』。(常用) o, O 进入输入模式(Insert mode)： 这是英文字母 o 的大小写。o 为『在目前光标所在的下一行处输入新的一行』； O 为在目前光标所在处的上一行输入新的一行！(常用) r, R 进入取代模式(Replace mode)： r 只会取代光标所在的那一个字符一次； R 会一直取代光标所在的文字，直到按下 ESC 为止；(常用) [Esc] 退出编辑模式，回到一般模式中(常用) 【注】：上面这些按键中，在 vi 画面的左下角处会出现『\u0026ndash;INSERT\u0026ndash;』或『\u0026ndash;REPLACE\u0026ndash;』的字样。 由名称就知道该动作了吧！！特别注意的是，我们上面也提过了，你想要在档案里面输入字符时， 一定要在左下角处看到 INSERT 或 REPLACE 才能输入喔！\n5、指令行的储存、离开等指令 命令 作用 :w 将编辑的数据写入硬盘档案中(常用) :w! 若文件属性为『只读』时，强制写入该档案。不过，到底能不能写入， 还是跟你对该档案的档案权限有关啊！ :q 离开 vi (常用) :q! 若曾修改过档案，又不想储存，使用 ! 为强制离开不储存档案。注意一下啊，那个惊叹号 (!) 在 vi 当中，常常具有『强制』的意思～ :wq 储存后离开，若为 :wq! 则为强制储存后离开 (常用) ZZ 这是大写的 Z 喔！若档案没有更动，则不储存离开，若档案已经被更动过，则储存后离开！ :w [filename] 将编辑的数据储存成另一个档案（类似另存新档） :r [filename] 在编辑的数据中，读入另一个档案的数据。亦即将 『filename』 这个档案内容加到游标所在行后面 :! command 暂时离开 vi 到指令行模式下执行 command 的显示结果！例如『:! ls /home』即可在 vi 当中察看 /home 底下以 ls 输出的档案信息！ 6、vim 环境的变更 命令 作用 :set nu 显示行号，设定之后，会在每一行的前缀显示该行的行号 :set nonu 与 set nu 相反，为取消行号！ ","date":"2020-01-06T09:42:51+08:00","permalink":"https://x-xkang.com/p/vim%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","title":"Vim常用命令"},{"content":" 构建镜像 在项目根目录下新建 Dockerfile 文件并编辑保存\n1 2 3 4 5 6 7 8 FROM golang:latest # 依赖的镜像:镜像版本 ADD . /var/www/go-aimaster # 将当前工作目录copy到镜像的/var/www/go-aimaster 目录下 WORKDIR /var/www/go-aimaster # 设置镜像内的工作目录 RUN GOPROXY=\u0026#34;https://goproxy.cn,direct\u0026#34; go build -o main /var/www/go-aimaster/main.go # 运行命令(当前为golang 项目demo) CMD [\u0026#34;/var/www/go-aimaster/main\u0026#34;] # 可执行文件目录，上一步build生成的main可执行文件 EXPOSE 8080 # 暴露端口，最终暴露的端口不一定是当前的8080 端口 ENTRYPOINT [\u0026#34;./main\u0026#34;] # 入口文件 执行命令：\n1 docker image build -t 镜像名称[:版本号] . # (注意最后有个点 .) 指定Dockerfile:\n1 docker image build -t 镜像名称[:版本号] -f Dcokerfile . 上面代码中，-t 参数用来指定 image 文件的名字，后面还可以用冒号指定标签。如果不指定，默认的标签就是latest。最后的那个点表示 Dockerfile 文件所在的路径，上例是当前路径，所以是一个点。 执行结果如下：\n出现上图的Successfully成功标识表示已构建成功，执行 docker images 查看，列表中出现刚刚构建的go-aimaster镜像\n下载远端镜像 命令：docker pull 仓库名称\n1 docker pull nginx 推送本地镜像至远端仓库 命令： 1 2 3 docker image tag go-docker:v1.0 devxiaokang/go-docker:v1.0 docker push devxiaokang/go-docker:v1.0 查看镜像列表 命令：docker image ls | docker images\n1 docker images 删除本地镜像 命令：docker rmi 镜像标识|镜像名称:版本号\n【注意】若有容器正在依赖该镜像，则无法删除\n1 docker rmi go-aimamster:v0.01 生成容器 命令： docker [container] run 镜像标识 /bin/bash（简单操作）| docker [container] run -d -p 宿主机端口:容器端口 -it --name 容器名称 镜像标识 /bin/bash （常用操作）\n1 docker run nginx 或者：\n1 docker run -d -p 8080:80 -it --name nginx nginx:latest /bin/bash 以上代码中-d 代表后台运行， -p 代表宿主机端口与容器端口的映射关系，-it 代表容器的 shell 映射到当前的 shell，然后再本机窗口输入命令，就会传入容器中，--name nginx 代表定义容器名称nginx 为自定义名称，。\n执行结果如下： 查看容器列表 命令：docker ps -a[q]， -a 表示显示所有容器（包括已停止的），-q 列表值显示容器的唯一标识\n1 docker ps -a 1 docker ps -aq 进入容器 命令：docker exec -it 容器ID|容器名称 /bin/bash\n1 docker exec -it nginx /bin/bash 启动容器 命令：docker start 容器ID|容器名称\n1 docker start nginx 重启容器 命令：docker restart 容器ID|容器名称\n1 docker restart nginx 停止容器 命令：docker stop 容器ID|容器名称\n1 docker stop nginx 停止全部容器\n1 docker stop $(docker ps -qa) 删除容器 命令：docker rm 容器ID|容器名称\n删除指定容器\n1 docker rm nginx 删除全部容器\n1 docker rm $(docker ps -qa) 数据卷 数据卷：将宿主机的一个目录映射到容器的一个目录中，可以在宿主机中操作目录中的内容，那么容器内部映射的文件，也会跟着一起改变,创建数据卷之后，默认会存在一个目录下 /var/lib/docker/volumes/数据卷名称/_data\n创建数据卷\n1 docker volume create volume_name 查看数据卷\n1 docker volume inspect volume_name 查看全部数据卷\n1 docker volume ls 删除数据卷：docker volume rm 数据卷名称\n1 docker volume rm volume_name 管理多容器 .yml文件以key: value 方式来指定配置信息，多个配置信息以换行+锁紧的方式来区分，在docker-compose文件中，不要使用制表符\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # yml version: ‘3.1’ service: mysql: restart: always # 代表只要docker启动，这个容器就会跟着启动 image: daoclound.io/lib/mysql:5.7.4 # 镜像路径 container_name: mysql # 指定容器名称 ports: - 3306:3306 environment: MYSQL_ROOT_PASSWORD: 123456 TZ: Asia/Shanghai # 时区 volumes: # 数据卷 - /opt/docker_mysql/data:/var/lib/mysql tomcat: restart: always Image: daocloud.io/library/tomcat:8.5.15-jre8 # 镜像 container_name: tomcat ports: - 8080:8080 environment: TZ: Asia/Shanghai volumes: - /opt/docker_mysql_tomcat/tomcat_webapps:/usr/local/tomcat/webapps - /opt/docker_mysql_tomcat/tomcat_logs:/usr/local/tomcat/logs Docker-Compose 配置Dockerfile使用 使用docker-compose.yml文件以及Dockerfile文件在生成自定义镜像的同时启动当前镜像，并且由docker-compose去管理容器\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # yml version: ‘3.1’ services: mysql: restart: always build: context: ../ # 指定dockerfile文件所在路径 dockerfile: Dockerfile #指定dockerfile文件名称 container_name: mysql ports: - 3306:3306 environment: TZ: Asia/Shanghai 可以直接启动基于 docker-compose.yml以及Dockerfile文件构建的自定义镜像 docker-compose up -d 如果自定义镜像不存在，会帮助我们构建出自定义镜像，如果自定义镜像已存在，会直接运行这个自定义镜像，重新构建的话需执行 docker-compose build , 运行前重新构建 docker-compose up -d —build\nDocker-compose 命令：\n后台启动： docker-compose up -d 关闭并删除容器： docker-compose down 开启|关闭|重启已经存在的有docker-compose维护的容器： docker-compose start | stop | restart 查看docker-compose管理的容器： docker-compose ps 查看日志： docker-compose logs -f ","date":"2019-09-18T02:01:58+05:30","permalink":"https://x-xkang.com/p/docker-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","title":"Docker 常用命令"}]