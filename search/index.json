[{"content":"一、删除策略 惰性删除\n每次获取 key 的时候会在 expire 字典中查询是否有当前key，如果有的话会校验当前key的过期时间，过期则删除，缺点是如果存在键已过期，但长期不使用的情况，实际上数据还是存在内存中的\n以下是 get 命令的部分源码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // src/t_string.c line 78 void setGenericCommand(client *c, int flags, robj *key, robj *val, robj *expire, int unit, robj *ok_reply, robj *abort_reply) { long long milliseconds = 0; /* initialized to avoid any harmness warning */ int found = 0; int setkey_flags = 0; if (expire \u0026amp;\u0026amp; getExpireMillisecondsOrReply(c, expire, flags, unit, \u0026amp;milliseconds) != C_OK) { return; } if (flags \u0026amp; OBJ_SET_GET) { if (getGenericCommand(c) == C_ERR) return; } found = (lookupKeyWrite(c-\u0026gt;db,key) != NULL); // 重点，开始查找当前key // ... 省略部分源码 } 顺着 lookupKeyWrite 找下去\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 // src/db.c line 158 robj *lookupKeyWrite(redisDb *db, robj *key) { return lookupKeyWriteWithFlags(db, key, LOOKUP_NONE); // 继续跳转 } // src/db.c line 154 robj *lookupKeyWriteWithFlags(redisDb *db, robj *key, int flags) { return lookupKey(db, key, flags | LOOKUP_WRITE); // 继续跳转 } // src/db.c line 81 robj *lookupKey(redisDb *db, robj *key, int flags) { dictEntry *de = dictFind(db-\u0026gt;dict,key-\u0026gt;ptr); robj *val = NULL; if (de) { val = dictGetVal(de); /* Forcing deletion of expired keys on a replica makes the replica * inconsistent with the master. We forbid it on readonly replicas, but * we have to allow it on writable replicas to make write commands * behave consistently. * * It\u0026#39;s possible that the WRITE flag is set even during a readonly * command, since the command may trigger events that cause modules to * perform additional writes. */ int is_ro_replica = server.masterhost \u0026amp;\u0026amp; server.repl_slave_ro; int force_delete_expired = flags \u0026amp; LOOKUP_WRITE \u0026amp;\u0026amp; !is_ro_replica; // 这里开始判断是否过期 if (expireIfNeeded(db, key, force_delete_expired)) { /* The key is no longer valid. */ val = NULL; } } } // src/db.c line 1666 // 判断逻辑 int expireIfNeeded(redisDb *db, robj *key, int force_delete_expired) { if (!keyIsExpired(db,key)) return 0; if (server.masterhost != NULL) { if (server.current_client == server.master) return 0; if (!force_delete_expired) return 1; } if (checkClientPauseTimeoutAndReturnIfPaused()) return 1; /* Delete the key */ deleteExpiredKeyAndPropagate(db,key); return 1; } 定期删除\n每隔一段时间检查一次数据库，随机删除一些过期键，需要注意的是：Redis 每次扫描并不是遍历过期字典中的所有键，而是采用随机抽取判断并删除过期键的形式执行的 Redis 默认每秒进行10次过期扫描，此配置可以通过 redis.conf 的 hz配置项进行配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // src/server.c line 971 void databasesCron(void) { /* Expire keys by random sampling. Not required for slaves * as master will synthesize DELs for us. */ if (server.active_expire_enabled) { // 区分主从 if (iAmMaster()) { activeExpireCycle(ACTIVE_EXPIRE_CYCLE_SLOW); // Master 节点开始处理过期键 } else { expireSlaveKeys(); } } // 省略部分源码... } Redis使用使用的是惰性删除加定期删除的策略\n二、淘汰策略 只有在 Redis 的运行内存达到了某个阀值，才会触发内存淘汰机制，这个阀值就是我们设置的最大运行内存，此值在 Redis 的配置文件中可以找到，配置项为 maxmemory。\n查看最大运行内存\n1 2 3 127.0.0.1:6379\u0026gt; config get maxmemory 1) \u0026#34;maxmemory\u0026#34; 2) \u0026#34;0\u0026#34; Redis在32位系统下默认阈值为3G（最大运行内存为4G，为保证系统正常运行，预留1G资源），64位系统下默认阈值为0，表示没有大小限制\n查看内存淘汰策略\n1 2 3 127.0.0.1:6379\u0026gt; config get maxmemory-policy 1) \u0026#34;maxmemory-policy\u0026#34; 2) \u0026#34;noeviction\u0026#34; noeviction 表示当运行内存超过最大设置内存时，不淘汰任何数据，但新增数据操作会报错\n内存淘汰策略分类\n1. noeviction: 不淘汰任何数据，当运行内存不足时，新增数据会直接报错 2. allkeys-lru: 淘汰所有键中最长时间未使用的 3. allkeys-random: 随机淘汰任意键 4. allkeys-lfu: 淘汰所有键中最少使用的； 5. volatile-lru: 淘汰所有设置了过期时间的键中最长时间未使用的 6. volatile-random: 随机淘汰设置了过期时间的键 7. volatile-ttl: 优先淘汰更快过期的键 8. volatile-lfu: 优先淘汰设置了过期时间的键中最少使用的\nLRU算法\nLRU 算法全称是Least Recently Used 译为最近最少使用 LRU 算法基于链表结构，链表中的元素按照操作顺序从前往后排列，最新操作的会被移动到表头，当需要内存淘汰时，只需要删除链表尾部的元素即可 Redis使用的是一种近似 LRU 算法，目的是为了更好的节约内存，它的实现方式是给现有的数据结构添加一个额外的字段，用于记录此键的最后一次访问时间，Redis内存淘汰时，会使用随机采样的方式淘汰数据，他是随机取N个值（可配置），然后淘汰醉酒没有使用的那个\nLFU算法\nLFU 全称是 Least Frequently Used 翻译为最不常用，最不常用的算法是根据总访问次数来淘汰数据的，它的核心思想是”如果数据过去被访问的次数很多，那么将来被访问的频率也会很高“。 LFU 解决了偶尔访问一次之后，数据就不会被淘汰的问题，相比于 LRU 算法也更合理一些\n","date":"2022-06-30T10:01:40+08:00","permalink":"https://x-xkang.com/p/redis-%E7%9A%84%E8%BF%87%E6%9C%9F%E5%88%A0%E9%99%A4%E7%AD%96%E7%95%A5%E4%BB%A5%E5%8F%8A%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/","title":"Redis 的过期删除策略以及淘汰策略"},{"content":"一、删除策略 惰性删除\n每次获取 key 的时候会在 expire 字典中查询是否有当前key，如果有的话会校验当前key的过期时间，过期则删除，缺点是如果存在键已过期，但长期不使用的情况，实际上数据还是存在内存中的\n以下是 get 命令的部分源码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // src/t_string.c line 78 void setGenericCommand(client *c, int flags, robj *key, robj *val, robj *expire, int unit, robj *ok_reply, robj *abort_reply) { long long milliseconds = 0; /* initialized to avoid any harmness warning */ int found = 0; int setkey_flags = 0; if (expire \u0026amp;\u0026amp; getExpireMillisecondsOrReply(c, expire, flags, unit, \u0026amp;milliseconds) != C_OK) { return; } if (flags \u0026amp; OBJ_SET_GET) { if (getGenericCommand(c) == C_ERR) return; } found = (lookupKeyWrite(c-\u0026gt;db,key) != NULL); // 重点，开始查找当前key // ... 省略部分源码 } 顺着 lookupKeyWrite 找下去\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 // src/db.c line 158 robj *lookupKeyWrite(redisDb *db, robj *key) { return lookupKeyWriteWithFlags(db, key, LOOKUP_NONE); // 继续跳转 } // src/db.c line 154 robj *lookupKeyWriteWithFlags(redisDb *db, robj *key, int flags) { return lookupKey(db, key, flags | LOOKUP_WRITE); // 继续跳转 } // src/db.c line 81 robj *lookupKey(redisDb *db, robj *key, int flags) { dictEntry *de = dictFind(db-\u0026gt;dict,key-\u0026gt;ptr); robj *val = NULL; if (de) { val = dictGetVal(de); /* Forcing deletion of expired keys on a replica makes the replica * inconsistent with the master. We forbid it on readonly replicas, but * we have to allow it on writable replicas to make write commands * behave consistently. * * It\u0026#39;s possible that the WRITE flag is set even during a readonly * command, since the command may trigger events that cause modules to * perform additional writes. */ int is_ro_replica = server.masterhost \u0026amp;\u0026amp; server.repl_slave_ro; int force_delete_expired = flags \u0026amp; LOOKUP_WRITE \u0026amp;\u0026amp; !is_ro_replica; // 这里开始判断是否过期 if (expireIfNeeded(db, key, force_delete_expired)) { /* The key is no longer valid. */ val = NULL; } } } // src/db.c line 1666 // 判断逻辑 int expireIfNeeded(redisDb *db, robj *key, int force_delete_expired) { if (!keyIsExpired(db,key)) return 0; if (server.masterhost != NULL) { if (server.current_client == server.master) return 0; if (!force_delete_expired) return 1; } if (checkClientPauseTimeoutAndReturnIfPaused()) return 1; /* Delete the key */ deleteExpiredKeyAndPropagate(db,key); return 1; } 定期删除\n每隔一段时间检查一次数据库，随机删除一些过期键，需要注意的是：Redis 每次扫描并不是遍历过期字典中的所有键，而是采用随机抽取判断并删除过期键的形式执行的 Redis 默认每秒进行10次过期扫描，此配置可以通过 redis.conf 的 hz配置项进行配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // src/server.c line 971 void databasesCron(void) { /* Expire keys by random sampling. Not required for slaves * as master will synthesize DELs for us. */ if (server.active_expire_enabled) { // 区分主从 if (iAmMaster()) { activeExpireCycle(ACTIVE_EXPIRE_CYCLE_SLOW); // Master 节点开始处理过期键 } else { expireSlaveKeys(); } } // 省略部分源码... } Redis使用使用的是惰性删除加定期删除的策略\n二、淘汰策略 只有在 Redis 的运行内存达到了某个阀值，才会触发内存淘汰机制，这个阀值就是我们设置的最大运行内存，此值在 Redis 的配置文件中可以找到，配置项为 maxmemory。\n查看最大运行内存\n1 2 3 127.0.0.1:6379\u0026gt; config get maxmemory 1) \u0026#34;maxmemory\u0026#34; 2) \u0026#34;0\u0026#34; Redis在32位系统下默认阈值为3G（最大运行内存为4G，为保证系统正常运行，预留1G资源），64位系统下默认阈值为0，表示没有大小限制\n查看内存淘汰策略\n1 2 3 127.0.0.1:6379\u0026gt; config get maxmemory-policy 1) \u0026#34;maxmemory-policy\u0026#34; 2) \u0026#34;noeviction\u0026#34; noeviction 表示当运行内存超过最大设置内存时，不淘汰任何数据，但新增数据操作会报错\n内存淘汰策略分类\n1. noeviction: 不淘汰任何数据，当运行内存不足时，新增数据会直接报错 2. allkeys-lru: 淘汰所有键中最长时间未使用的 3. allkeys-random: 随机淘汰任意键 4. allkeys-lfu: 淘汰所有键中最少使用的； 5. volatile-lru: 淘汰所有设置了过期时间的键中最长时间未使用的 6. volatile-random: 随机淘汰设置了过期时间的键 7. volatile-ttl: 优先淘汰更快过期的键 8. volatile-lfu: 优先淘汰设置了过期时间的键中最少使用的\nLRU算法\nLRU 算法全称是Least Recently Used 译为最近最少使用 LRU 算法基于链表结构，链表中的元素按照操作顺序从前往后排列，最新操作的会被移动到表头，当需要内存淘汰时，只需要删除链表尾部的元素即可 Redis使用的是一种近似 LRU 算法，目的是为了更好的节约内存，它的实现方式是给现有的数据结构添加一个额外的字段，用于记录此键的最后一次访问时间，Redis内存淘汰时，会使用随机采样的方式淘汰数据，他是随机取N个值（可配置），然后淘汰醉酒没有使用的那个\nLFU算法\nLFU 全称是 Least Frequently Used 翻译为最不常用，最不常用的算法是根据总访问次数来淘汰数据的，它的核心思想是”如果数据过去被访问的次数很多，那么将来被访问的频率也会很高“。 LFU 解决了偶尔访问一次之后，数据就不会被淘汰的问题，相比于 LRU 算法也更合理一些\n","date":"2022-06-30T10:01:40+08:00","permalink":"https://x-xkang.com/p/redis-%E7%9A%84%E8%BF%87%E6%9C%9F%E5%88%A0%E9%99%A4%E7%AD%96%E7%95%A5%E4%BB%A5%E5%8F%8A%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5-%E4%B8%AD%E6%96%87/","title":"Redis 的过期删除策略以及淘汰策略-中文"},{"content":"Redis提供两种持久化方式：一种是默认的RDB持久化方式，另一种是AOF（append only file）持久化方式\n一、 RDB 是什么？\n原理是Redis会通过单独创建（fork）一个与当前进程一模一样的子进程来进行持久化，这个子进程的所有数据（变量、环境变量、程序计数器等）都和原进程一模一样，会先将数据写入到一个临时文件中，待持久化结束了，再用这个临时文件替换上次持久化好的文件，整个过程冲，主进程不进行任何的io操作，这就确保了极高的性能。\n1、持久化文件在哪 启动redis-server 的目录下\n2、什么时候fork子进程，或者什么时候触发 rdb持久化机制 RDB 方式持久化数据是通过 fork 子进程，在子进程中进行数据同步\nshutdown时，如果没有开启aof，会触发配置文件中默认的快照配置 执行命令 save 或者 bgsave save是只管保存，不管其他，全部阻塞，使用主进程进行持久化 bgsave redis 会在后台异步进行快照操作，同时可以响应客户端的请求\n3、优点 适合数据恢复\n4、缺点 数据丢失多\n原理是将Redis的操作日志以追加的方式写入文件，读操作是不记录的\n二、AOF 为什么会出现AOF持久化方式\n1、这个持久化文件在哪 启动 redis-server 的目录下会生成 appendonly.aof文件\n2、触发机制（根据配置文件的配置项\u0026ndash;appendfsync） no: 表示操作系统进行数据缓存同步到磁盘（快，持久化没保证：写满缓冲区才会同步，若在缓冲区未写满前 shutdown 或其他意外关机，则这部分数据会丢失） always: 同步持久化，每次发生数据变更时（增删改操作），立即记录到磁盘（慢，安全） everysec: 表示每秒同步一次（默认值，很快，但可能会丢失1秒的数据）\n3、AOF 重写机制 重写 AOF 文件会 fork 子进程去执行，会将内存中的数据写入新的 AOF 文件，并且是以RDB 的方式写入，重写结束后会替代旧的AOF 文件，后续的客户端命令操作又重新以 AOF的格式写入，redis.conf 中配置触发 AOF 文件重写的文件大小值auto-aof-rewrite-percentage 不宜太小，因为会频繁触发重写\n触发时机 redis.conf 的配置项 auto-aof-rewrite-min-size 默认值是 64mb， 当 AOF 文件大小超过这个配置值时会自动开启重写 `。 redis.conf 的配置项 auto-aof-rewrite-percentage 默认值是100， 当 AOF 文件大小的增长率大于配置值时会自动开启重写。 4、优点 保证数据安全\n5、缺点 数据恢复慢\n","date":"2022-06-29T15:34:24+08:00","permalink":"https://x-xkang.com/p/redis-%E6%8C%81%E4%B9%85%E5%8C%96%E7%AD%96%E7%95%A5/","title":"Redis 持久化策略"},{"content":"一、Redis 缓存雪崩 出现场景 如果使用redis记录大量的热点数据，且过期时间为同一个常量，那么可能会出现大批的缓存数据会在同一时间或较短的时间区间内失效，redis会根据淘汰策略进行优化，如果数据量比较大会导致线程出现短暂的阻塞；另外，因为大量的缓存失效，会导致请求直接落在DB上，请求数较大情况下会直接导致数据库瘫痪，然后整个业务系统变为不可用状态\n解决方案 针对这种情况，我们可以在设置过期时间时加上一个随机值，类似 redis.set(key, value, expiredTime + Math.random()*10000)，这样设置就不会出现在短时间内大量缓存key失效的情况。\n二、Redis 缓存穿透 出现场景 如果用户请求的热点数据本身是不存在的，比如id为-1，或者id=\u0026lsquo;\u0026lsquo;的数据，查询缓存不存在后会将请求直接打到DB上，最终在DB中也没有查到此数据，此时Redis缓存就是去了作用，搞并发的情况下会降低数据库性能，甚至瘫痪\n解决方案 增加参数校验，拦截掉大量的非法参数请求； 缓存空值，因为数据库中本来就不存在这些数据，因此可以在第一次重建缓存时将value 记录为 null，下次请求时从Redis获取到 null 值直接返回（注意，要对redis查询的返回值进行严格校验，区分key不存在返回的空值和主动设置的空值null）； 布隆过滤器，将DB中的热点数据加载至布隆过滤器中（布隆过滤器的特性：若在过滤器中存在，不一定真是存在；若不存在时，一定不存在），每次请求前先校验布隆过滤器是否存在该key，不存在的话直接return； 三、Redis 缓存击穿 出现场景 高并发请求同一个热点数据，在热点数据失效的瞬间，大量请求在缓存中没有命中会直接落在DB上进行查询，导致DB压力瞬间增加\n解决方案 增加互斥锁，在第一个请求没有在缓存命中开始在DB进行查询并重加缓存时加上一个互斥锁，在缓存重建完成之前，对这同一热点数据的请求将会被互斥锁拦截，被拦截的这些请求根据业务需求，可以延时重试直到拿到数据或直接返回失败等； 热点数据不设置过期时间（不建议，随着热点数据的增加，无过期时间的key也越来越多，或导致Redis的存储压力增加）\n","date":"2022-06-28T14:58:25+08:00","permalink":"https://x-xkang.com/p/redis%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E4%B9%8B%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9%E5%87%BB%E7%A9%BF%E5%92%8C%E7%A9%BF%E9%80%8F/","title":"Redis基础知识之缓存雪崩、击穿和穿透"},{"content":"HTTP/0.9 - 单行协议\n最初版本的HTTP协议并没有版本号，后来它的版本号被定位在0.9以区分后来的版本。HTTP/0.9极其简单：请求由单行指令构成，以唯一可用方法GET开头，其后跟目标资源的路径（一旦连接到服务器，协议、服务器、端口号这些都不是必须的）。 跟后来的版本不同，HTTP/0.9的响应内容并不包含HTTP头，这意味着只有 HTML 文件可以传送，无法传送其他类型的文件；也没有状态码或者错误代码；一旦出现问题，一个特殊的包含问题描述信息的HTML文件将被发回，供人们查看。\nHTTP/1.0 - 构建可扩展性\n由于 HTTP/0.9协议的应用十分有限，浏览器和服务器迅速扩展内容使其用途更广：\n协议版本信息现在会随着每个请求发送（HTTP/1.0被追加到了 GET行） 状态码会在响应开始时发送，使浏览器能了解请求执行成功或失败，并响应调整行为（如更新或使用本地缓存） 引入了 HTTP 头的概念，无论是对于请求还是响应，允许传输元数据，使协议变得非常灵活，更具扩展性。 在新 HTTP 头的的帮助下，具备了传输纯文本HTML文件以外其他类型文档的能力（凭借Content-Type头） 一个典型的请求看起来就像这样：\n1 2 3 4 5 6 7 8 9 10 11 GET /mypage.html HTTP/1.0 User-Agent: NCSA_Mosaic/2.0 (Windows 3.1) 200 OK Date: Tue, 15 Nov 1994 08:12:31 GMT Server: CERN/3.0 libwww/2.17 Content-Type: text/html \u0026lt;HTML\u0026gt; 一个包含图片的页面 \u0026lt;IMG SRC=\u0026#34;/myimage.gif\u0026#34;\u0026gt; \u0026lt;/HTML\u0026gt; 接下来是第二个请求：\n1 2 3 4 5 6 7 8 GET /myimage.gif HTTP/1.0 User-Agent: NCSA_Mosaic/2.0 (Windows 3.1) 200 OK Date: Tue, 15 Nov 1994 08:12:32 GMT Server: CERN/3.0 libwww/2.17 Content-Type: text/gif (这里是图片内容) 在 1991-1995年，这些新扩展并没有被引入到标准中以促进下注工作，而仅仅作为一种尝试：服务器和浏览器天界这些新扩展功能，但出现了大量的互操作问题。知道 1996年11月，为了解决这些问题，一份新文档（RFC 1945） 被发表出来，泳衣描述如何操作实践这些新扩展功能，文档 RFC 1945 定义了 HTTP/1.0，但它是狭义的，并不是官方标准\nHTTP/1.1 - 标准化的协议\nHTTP/1.0 多种不同的实现方式在实际运用中显得有些混乱，自 1995 年开始，即 HTTP/1.0 文档发布的下一年，就开始修订 HTTP 的第一个标准化版本。在 1997 年初，HTTP1.1 标准发布，就在 HTTP/1.0 发布的几个月后。\nHTTP/1.1 消除了大量歧义内容并引入了多项改进：\n连接可以复用，节省了多次打开 TCP 连接加载网页文档资源的时间。 增加管线化技术，允许在第一个应答被完全发送之前就发送第二个请求，以降低通信延迟。 支持响应分块。 引入额外的缓存控制机制。 引入内容协商机制，包括语言，编码，类型等，并允许客户端和服务器之间约定以最合适的内容进行交换。 凭借Host头，能够使不同域名配置在同一个 IP 地址的服务器上。 一个典型的请求流程， 所有请求都通过一个连接实现，看起来就像这样：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 GET /en-US/docs/Glossary/Simple_header HTTP/1.1 Host: developer.mozilla.org User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:50.0) Gecko/20100101 Firefox/50.0 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8 Accept-Language: en-US,en;q=0.5 Accept-Encoding: gzip, deflate, br Referer: https://developer.mozilla.org/en-US/docs/Glossary/Simple_header 200 OK Connection: Keep-Alive Content-Encoding: gzip Content-Type: text/html; charset=utf-8 Date: Wed, 20 Jul 2016 10:55:30 GMT Etag: \u0026#34;547fa7e369ef56031dd3bff2ace9fc0832eb251a\u0026#34; Keep-Alive: timeout=5, max=1000 Last-Modified: Tue, 19 Jul 2016 00:59:33 GMT Server: Apache Transfer-Encoding: chunked Vary: Cookie, Accept-Encoding (content) HTTP/1.1 在 1997 年 1 月以 RFC 2068 文件发布。\nHTTP 用于安全传输\nHTTP 最大的变化发生在 1994 年底。HTTP 在基本的 TCP/IP 协议栈上发送信息，网景公司（Netscape Communication）在此基础上创建了一个额外的加密传输层：SSL 。SSL 1.0 没有在公司以外发布过，但 SSL 2.0 及其后继者 SSL 3.0 和 SSL 3.1 允许通过加密来保证服务器和客户端之间交换消息的真实性，来创建电子商务网站。SSL 在标准化道路上最终成为 TLS，随着版本 1.0, 1.1, 1.2 的出现成功地关闭漏洞。TLS 1.3 目前正在形成。\n与此同时，人们对一个加密传输层的需求也愈发高涨：因为 Web 最早几乎是一个学术网络，相对信任度很高，但如今不得不面对一个险恶的丛林：广告客户、随机的个人或者犯罪分子争相劫取个人信息，将信息占为己有，甚至改动将要被传输的数据。随着通过 HTTP 构建的应用程序变得越来越强大，可以访问越来越多的私人信息，如地址簿，电子邮件或用户的地理位置，即使在电子商务使用之外，对 TLS 的需求也变得普遍。\nHTTP 用于复杂应用\nTim Berners-Lee 对于 Web 的最初设想不是一个只读媒体。 他设想一个 Web 是可以远程添加或移动文档，是一种分布式文件系统。 大约 1996 年，HTTP 被扩展到允许创作，并且创建了一个名为 WebDAV 的标准。 它进一步扩展了某些特定的应用程序，如 CardDAV 用来处理地址簿条目，CalDAV 用来处理日历。 但所有这些 *DAV 扩展有一个缺陷：它们必须由要使用的服务器来实现，这是非常复杂的。并且他们在网络领域的使用必须保密。\n在 2000 年，一种新的使用 HTTP 的模式被设计出来：representational state transfer (或者说 REST)。 由 API 发起的操作不再通过新的 HTTP 方法传达，而只能通过使用基本的 HTTP / 1.1 方法访问特定的 URI。 这允许任何 Web 应用程序通过提供 API 以允许查看和修改其数据，而无需更新浏览器或服务器：所有需要的内容都被嵌入到由网站通过标准 HTTP/1.1 提供的文件中。 REST 模型的缺点在于每个网站都定义了自己的非标准 RESTful API，并对其进行了全面的控制；不同于 *DAV 扩展，客户端和服务器是可互操作的。 RESTful API 在 2010 年变得非常流行。\n自2005年以来，可用于 Web 页面的API大大增加，其中几个API为特定目的扩展了HTTP协议，大部分是新的特定HTTP头：\nServer-sent events 服务器可以偶尔推送消息到浏览器 Websocket 一个新协议，可以通过升级现有 HTTP 协议来建立 放松安全措施-基于当前的web模型\nHTTP 和 Web安全模型 \u0026ndash; 同源策略是互不相关的。事实上，当前的Web安全模型是在HTTP被创造出来后才被发展的！这些年来，已经在证实了它如果能通过在特定的约束下移除一些这个策略的限制来管的宽松些的话，将会更有用。这些策略导致大量的成本和时间被话费在通过转交到服务端来添加一些新的HTTP头来发送。这些被定义在了 Cross-Origin Resource Sharing(CORS) or the Content Security Policy(CSP)规范里。\n不只是这大量的扩展，很多的其他的头也被加了进来，有些只是实验性的，比较著名的有 Do Not Track(DNT)来控制隐私， X-Frame-Option，还有很多\nHTTP/2 - 为了更优异的表现\n这些年来，网页愈渐变得的复杂，甚至演变成了独有的应用，可见媒体的播放量，增进交互的脚本大小也增加了许多：更多的数据通过 HTTP 请求被传输。HTTP/1.1 链接需要请求以正确的顺序发送，理论上可以用一些并行的链接（尤其是 5 到 8 个），带来的成本和复杂性堪忧。比如，HTTP 管线化（pipelining）就成为了 Web 开发的负担。\n在 2010 年到 2015 年，谷歌通过实践了一个实验性的 SPDY 协议，证明了一个在客户端和服务器端交换数据的另类方式。其收集了浏览器和服务器端的开发者的焦点问题。明确了响应数量的增加和解决复杂的数据传输，SPDY 成为了 HTTP/2 协议的基础。\nHTTP/2 和 HTTP/1.1 有几处基本的不同：\nHTTP/2 是二进制协议而不是文本协议。不在可读，也不是无障碍的手动创建，改善的优化技术现在可被实施。 这是一个复用协议。并行的请求能在同一个链接中处理，移除了HTTP/1.x 中顺序和阻塞的约束。 压缩了headers，因为 headers 在一系列请求中常常是相似的，其移除了重复和传输重复数据的成本。 其允许服务器在客户端缓存中填充数据，通过一个叫服务器推送的机制来提前请求。 HTTP/3 - 基于UDP的QUIC协议实现\n在HTTP/3中，将启用TCP协议，改为使用基于UDP协议的QUIC协议实现，此改变是为了解决HTTP/2中存在的队头阻塞问题，由于HTTP/2在单个TCP连接上使用了多路复用，收到TCP拥塞控制的影响，少量的丢包就可能导致整个TCP连接上的所有流被阻塞。\nQUIC（快速UDP网络连接）是一种实验性的网络传输协议，由Google开发，该协议旨在使网页传输更快。\n","date":"2022-06-17T10:09:23+08:00","permalink":"https://x-xkang.com/p/http-%E5%8D%8F%E8%AE%AE%E7%89%88%E6%9C%AC%E8%BF%9B%E5%8C%96/","title":"Http 协议版本进化"},{"content":"了解域名结构\n以mail.qq.com域名为例，com为顶级域名，qq.com 为二级域名， mail.qq.com为三级域名\n顶级域名服务器\n顶级域名为最后一个.右侧部分的内容，如mail.qq.com的com就是顶级域名，顶级域名对应的服务器称之为顶级域名服务器\n二级域名服务器\n域名 mail.qq.com中的倒数第二个.右侧部分qq.com成为二级域名\n根域名服务器\n在2016年之前全球一共拥有13台根服务器，1台主根服务器在美国，其他12台为辅根服务器，其中美国9台，英国1台，瑞典1台，日本1台，这13台根服务器主要管理互联网的主目录，主要作用IPV4。 2016年，中国下一代互联网工程中心领衔发起雪人计划，旨在为下一代互联网(IPV6)提供更多的根服务器解决方案，该计划于2017年完成，其中包含3台主根服务器，中国1台，美国1台，日本1台，22台辅根服务器，中国3台，美国2台，印度和法国分别有3台，德国2台，俄罗斯、意大利、西班牙、奥地利、智利、南非、澳大利亚、瑞士、荷兰各1台，共22台，从此形成了13台原有根加25台IPV6根服务器的新格局\n本地域名服务器\n本地域名服务器的范围非常广，没有一个详细的定位，可能是某个运营商部署在该城市的一台服务器，也可能是某个公司的一台服务器，甚至可能是某个学校的服务器\n存放地址\n浏览器缓存 系统缓存 本地域名服务器 根域名服务器 顶级域名服务器 二级域名服务器 \u0026hellip; \u0026hellip; 查询顺序\n检查浏览器缓存中是否存在改域名与IP地址的映射关系，如果有则解析结束，没有则继续 到系统本地查找映射关系，一般存在 hosts 文件中，如果有则解析结束，否则继续 到本地域名服务器去查询，有则结束，否则继续 本地域名服务器去查询 根域名服务器，该过程不会返回映射关系，只会告诉你去下级服务器（顶级域名服务器）查询 本地域名服务器查询顶级域名服务器（即 com 服务器），同样不会返回映射关系，只会引导你去二级域名服务器查询 本地域名服务器查询二级域名服务器（即 qq.com 服务器），引导去三级域名服务器 本地域名服务器查询三级域名服务器（即 mail.qq.com）， 此时已经是最后一级，如果有则返回映射关系，并且本地服务器加入自身的映射表中，方便下次查询，同时返回给用户的计算机，没有找到则网页报错 ","date":"2022-06-16T17:00:53+08:00","permalink":"https://x-xkang.com/p/dns-%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E6%B5%81%E7%A8%8B/","title":"DNS 域名解析流程"},{"content":"OSI七层模型与TCP/IP四层模型对比\n应用层：为应用程序或用户请求提供请求服务。OSI参考模型最高层，也是最靠近用户的一层，为计算机用户、各种应用程序以及网络提供端口，也为用户直接提供各种网络服务。\n表示层：数据编码、格式转换、数据加密。提供各种用于应用层的编码和转换功能，确保一个系统的应用层发送的数据能被另一个系统的应用层识别。如果必要，该层可提供一种标准表示形式，用于讲计算机内部的各种数据格式转换成通信中采用的标准表示形式。数据压缩和加密也是表示层可提供的转换功能之一。\n会话层：创建、管理和维护会话。接收来自传输层的数据，负责建立、管理和终止表示层实体之间的通信会话，支持它们之间的数据交换。该层的通信由不同设备中的应用程序之间的服务请求和响应组成。\n传输层：数据通信。建立主机端到端的链接，为会话层和网络层提供端到端可靠的和透明的数据传输服务，确保数据能完整的传输到网络层。\n网络层：IP选址及路由选择。通过路由选择算法，为报文或通信子网选择最适当的路径。控制数据链路层与传输层之间的信息转发，建立、维持和终止网络的连接。数据链路层的数据在这一层被转换为数据包，然后通过路径选择、分段组合、顺序、进/出路由等控制，将信息从一个网络设备传送到另一个网络设备。\n数据链路层：提供介质访问和链路管理。接收来自物理层的位流形式的数据，封装成帧，传送到网络层；将网络层的数据帧，拆装为位流形式的数据转发到物理层；负责建立和管理节点间的链路，通过各种控制协议，将有差错的物理信道变为无差错的、能可靠传输数据帧的数据链路。\n物理层：管理通信设备和网络媒体之间的互联互通。传输介质为数据链路层提供物理连接，实现比特流的透明传输。实现相邻计算机节点之间比特流的透明传送，屏蔽具体传输介质和物理设备的差异。\n","date":"2022-06-15T15:57:49+08:00","permalink":"https://x-xkang.com/p/osi-%E4%B8%83%E5%B1%82%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/","title":"OSI 七层网络模型"},{"content":"一、HTTP状态码分类 HTTP 状态码由三个十进制数字组成，第一个十进制数字定义了状态码的类型，响应分为五类：消息响应（100-199），成功响应（200-299），重定向消息（300-399），客户端错误响应（400-499）和服务器错误响应（500-599）\n分类 分来描述 1** 信息，服务器收到请求，需要请求者继续执行操作 2** 操作被成功接收并处理 3** 重定向，需要进一步操作以完成请求 4** 客户端错误，请求包含语法错误或无法完成请求 5** 服务器错误，服务器在处理请求的过程发生了错误 二、状态码分类描述 状态码 英文状态码 中文描述 100 Continue 这个临时响应表明，迄今为止的所有内容都是可行的，客户端应该继续请求，如果以完成，请忽略它。 101 Switching Protocols 该代码是响应客户端 Upgrade(en-US). 请求头发送的，指明服务器即将切换的协议 102 Processing 此代码表示服务器以收到并且正在处理该请求，但当前没有响应可用 103 Early Hints 次状态码主要用于与 Link 链接头一起使用，以允许用户代理在服务器响应阶段时开始预加载 preloading 资源 200 OK 请求成功 201 Created 该请求已成功，并因此创建了一个新的资源。这通常是在 POST 请求，或是某些 PUT 请求之后返回的响应 202 Accepted 请求已经接收到，但还未响应，没有结果。意味着不会有一个异步的响应去表明当前请求的结果，预期另外的进程和服务去处理请求，或者批处理。 203 Non-Authoritative Information 服务器已成功处理了请求，但返回的实体头部元信息不是在原始服务器上有效的确定集合，而是来自本地或者第三方的拷贝。当前的信息可能是原始版本的子集或者超集。例如，包含资源的元数据可能导致原始服务器知道元信息的超集。使用此状态码不是必须的，而且只有在响应不使用此状态码便会返回200 OK的情况下才是合适的。 204 No Content 对于该请求没有的内容可发送，但头部字段可能有用。用户代理可能会用此时请求头部信息来更新原来资源的头部缓存字段。 205 Reset Content 告诉用户代理重置发送此请求的文档 206 Partial Content 当从客户端发送Range范围标头以只请求资源的一部分时，将使用此响应代码。 207 Multi Status 对于多个状态代码都可能合适的情况，传输有关多个资源的信息。 208 Already Reported 在 DAV 里面使用 dav:propstat 响应元素以避免重复枚举多个绑定的内部成员到同一个集合。 226 IM Used 服务器已经完成了对资源的GET请求，并且响应是对当前实例应用的一个或多个实例操作结果的表示。 300 Multiple Choice 请求拥有不只一个的可鞥响应。用户带来或者用户应当从中选择一个。 (没有标准化的方法来选择其中一个响应，但是建议使用指向可能性的 HTML 链接，以便用户可以选择。) 301 Moved Permanently 请求资源的 URL 已永久更改。在响应中给出了新的 URL。 302 Found 此响应代码表示所请求资源的 URI 已 暂时 更改。未来可能会对 URI 进行进一步的改变。因此，客户机应该在将来的请求中使用这个相同的 URI。 303 See Other 服务器发送此响应，以指示客户端通过一个 GET 请求在另一个 URI 中获取所请求的资源 304 Not Modified 这是用于缓存的目的。它告诉客户端响应还没有被修改，因此客户端可以继续使用相同的缓存版本的响应。 305 Use Proxy 在 HTTP 规范中定义，以指示请求的响应必须被代理访问。由于对代理的带内配置的安全考虑，它已被弃用 306 unused 此响应代码不再使用；它只是保留。它曾在 HTTP/1.1 规范的早期版本中使用过。 307 Temporary Redirect 服务器发送此响应，以指示客户端使用在前一个请求中使用的相同方法在另一个 URI 上获取所请求的资源。这与 302 Found HTTP 响应代码具有相同的语义，但用户代理 不能 更改所使用的 HTTP 方法：如果在第一个请求中使用了 POST，则在第二个请求中必须使用 POST 308 Permanent Redirect 这意味着资源现在永久位于由Location: HTTP Response 标头指定的另一个 URI。 这与 301 Moved Permanently HTTP 响应代码具有相同的语义，但用户代理不能更改所使用的 HTTP 方法：如果在第一个请求中使用 POST，则必须在第二个请求中使用 POST。 400 Bad Request 由于被认为是客户端错误（例如，错误的请求语法、无效的请求消息帧或欺骗性的请求路由），服务器无法或不会处理请求。 401 Unauthorized 虽然 HTTP 标准指定了\u0026quot;unauthorized\u0026quot;，但从语义上来说，这个响应意味着\u0026quot;unauthenticated\u0026quot;。也就是说，客户端必须对自身进行身份验证才能获得请求的响应。 402 Payment Required 此响应代码保留供将来使用。创建此代码的最初目的是将其用于数字支付系统，但是此状态代码很少使用，并且不存在标准约定。 403 Forbidden 客户端没有访问内容的权限；也就是说，它是未经授权的，因此服务器拒绝提供请求的资源。与 401 Unauthorized 不同，服务器知道客户端的身份。 404 Not Found 服务器找不到请求的资源。在浏览器中，这意味着无法识别 URL。在 API 中，这也可能意味着端点有效，但资源本身不存在。服务器也可以发送此响应，而不是 403 Forbidden，以向未经授权的客户端隐藏资源的存在。这个响应代码可能是最广为人知的，因为它经常出现在网络上。 405 Method Not Allowed 服务器知道请求方法，但目标资源不支持该方法。例如，API 可能不允许调用DELETE来删除资源。 406 Not Acceptale 当 web 服务器在执行 服务端驱动型内容协商机制](/zh-CN/docs/Web/HTTP/Content_negotiation#服务端驱动型内容协商机制)后，没有发现任何符合用户代理给定标准的内容时，就会发送此响应。 407 Proxy Authentication Required 类似于 401 Unauthorized 但是认证需要由代理完成。 408 Request Timeout 此响应由一些服务器在空闲连接上发送，即使客户端之前没有任何请求。这意味着服务器想关闭这个未使用的连接。由于一些浏览器，如 Chrome、Firefox 27+ 或 IE9，使用 HTTP 预连接机制来加速冲浪，所以这种响应被使用得更多。还要注意的是，有些服务器只是关闭了连接而没有发送此消息。 409 Conflict 当请求与服务器的当前状态冲突时，将发送此响应。 410 Gone 当请求的内容已从服务器中永久删除且没有转发地址时，将发送此响应。客户端需要删除缓存和指向资源的链接。HTTP 规范打算将此状态代码用于“有限时间的促销服务”。API 不应被迫指出已使用此状态代码删除的资源。 411 Length Required 服务端拒绝该请求因为 Content-Length 头部字段未定义但是服务端需要它。 412 Precondition Failed 客户端在其头文件中指出了服务器不满足的先决条件。 413 Payload Too Large 请求实体大于服务器定义的限制。服务器可能会关闭连接，或在标头字段后返回重试 Retry-After。 414 URI Too Long 客户端请求的 URI 比服务器愿意接收的长度长。 415 Unsupported Media Type 服务器不支持请求数据的媒体格式，因此服务器拒绝请求。 416 Rabge Not Satisfiale 无法满足请求中 Range 标头字段指定的范围。该范围可能超出了目标 URI 数据的大小。 417 Expectation Failed 此响应代码表示服务器无法满足 Expect 请求标头字段所指示的期望。 418 I'm Teapot 服务端拒绝用茶壶煮咖啡。笑话，典故来源茶壶冲泡咖啡 421 Misdirected Request 请求被定向到无法生成响应的服务器。这可以由未配置为针对请求 URI 中包含的方案和权限组合生成响应的服务器发送。 422 Unproccessable Entity 请求格式正确，但由于语义错误而无法遵循。 423 Locked 正在访问的资源已锁定。 424 Failed Dependency 由于前一个请求失败，请求失败。 425 Too Early 表示服务器不愿意冒险处理可能被重播的请求。 426 Upgrade Required 服务器拒绝使用当前协议执行请求，但在客户端升级到其他协议后可能愿意这样做。 服务端发送带有 Upgrade (en-US) 字段的 426 响应 来表明它所需的协议（们）。 428 Precondition Required 用户在给定的时间内发送了太多请求（\u0026ldquo;限制请求速率\u0026rdquo;） 431 Request Header Fields Too Large 服务器不愿意处理请求，因为其头字段太大。在减小请求头字段的大小后，可以重新提交请求。 451 Unavailable For Legal Reasons 用户代理请求了无法合法提供的资源，例如政府审查的网页。 500 Internal Server Error 服务器遇到了不知道如何处理的情况。 501 Not Implement 服务器不支持请求方法，因此无法处理。服务器需要支持的唯二方法（因此不能返回此代码）是 GET and HEAD. 502 Bad Gateway 此错误响应表明服务器作为网关需要得到一个处理这个请求的响应，但是得到一个错误的响应。 503 Service Unavailable 服务器没有准备好处理请求。常见原因是服务器因维护或重载而停机。请注意，与此响应一起，应发送解释问题的用户友好页面。这个响应应该用于临时条件和如果可能的话，HTTP 标头 Retry-After 字段应该包含恢复服务之前的估计时间。网站管理员还必须注意与此响应一起发送的与缓存相关的标头，因为这些临时条件响应通常不应被缓存。 504 Gateway Timeout 当服务器充当网关且无法及时获得响应时，会给出此错误响应。 505 HTTP Version Not Supported 服务器不支持请求中使用的 HTTP 版本。 506 Variant Also Negotiates 服务器存在内部配置错误：所选的变体资源被配置为参与透明内容协商本身，因此不是协商过程中的适当终点 507 Insufficient Storage 无法在资源上执行该方法，因为服务器无法存储成功完成请求所需的表示。 508 Loop Detected 服务器在处理请求时检测到无限循环。 510 Not Extented 服务器需要对请求进行进一步扩展才能完成请求。 511 Network Authentication Required 指示客户端需要进行身份验证才能获得网络访问权限。 ","date":"2022-06-14T21:33:58+08:00","permalink":"https://x-xkang.com/p/http-%E7%8A%B6%E6%80%81%E7%A0%81/","title":"Http 状态码"},{"content":" 常用正则表达式 字符 描述 \\ 将下一个字符标记为一个特殊字符、或一个原义字符、或一个向后引用、或一个八进制转义符。例如，“n”匹配字符“n”。“\\n”匹配一个换行符。串行“\\”匹配“\\”而“(”则匹配“(”。 ^ 匹配输入字符串的开始位置。如果设置了RegExp对象的Multiline属性，^也匹配“\\n”或“\\r”之后的位置。 $ 匹配输入字符串的结束位置。如果设置了RegExp对象的Multiline属性，$也匹配“\\n”或“\\r”之前的位置。 * 匹配前面的子表达式零次或多次。例如，zo*能匹配“z”以及“zoo”。*等价于{0,}。 + 匹配前面的子表达式一次或多次。例如，“zo+”能匹配“zo”以及“zoo”，但不能匹配“z”。+等价于{1,}。 ? 匹配前面的子表达式零次或一次。例如，“do(es)?”可以匹配“does”或“does”中的“do”。?等价于{0,1}。 {n} n是一个非负整数。匹配确定的n次。例如，“o{2}”不能匹配“Bob”中的“o”，但是能匹配“food”中的两个o。 {n,} n是一个非负整数。至少匹配n次。例如，“o{2,}”不能匹配“Bob”中的“o”，但能匹配“foooood”中的所有o。“o{1,}”等价于“o+”。“o{0,}”则等价于“o*”。 {n,m} m和n均为非负整数，其中n\u0026lt;=m。最少匹配n次且最多匹配m次。例如，“o{1,3}”将匹配“fooooood”中的前三个o。“o{0,1}”等价于“o?”。请注意在逗号和两个数之间不能有空格。 ? 当该字符紧跟在任何一个其他限制符（*,+,?，{n}，{n,}，{n,m}）后面时，匹配模式是非贪婪的。非贪婪模式尽可能少的匹配所搜索的字符串，而默认的贪婪模式则尽可能多的匹配所搜索的字符串。例如，对于字符串“oooo”，“o+?”将匹配单个“o”，而“o . 匹配除“\\n”之外的任何单个字符。要匹配包括“\\n”在内的任何字符，请使用像“(. (pattern) 匹配pattern并获取这一匹配。所获取的匹配可以从产生的Matches集合得到，在VBScript中使用SubMatches集合，在JScript中则使用$0…$9属性。要匹配圆括号字符，请使用“(”或“)”。 (?:pattern) 匹配pattern但不获取匹配结果，也就是说这是一个非获取匹配，不进行存储供以后使用。这在使用或字符“( (?=pattern) 正向肯定预查，在任何匹配pattern的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用。例如，“Windows(?=95 (?!pattern) 正向否定预查，在任何不匹配pattern的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用。例如“Windows(?!95 (?\u0026lt;=pattern) 反向肯定预查，与正向肯定预查类拟，只是方向相反。例如，“(?\u0026lt;=95 (?\u0026lt;!pattern) 反向否定预查，与正向否定预查类拟，只是方向相反。例如“(?\u0026lt;!95 x|y 匹配x或y。例如，“z [xyz] 字符集合。匹配所包含的任意一个字符。例如，“[abc]”可以匹配“plain”中的“a”。 [^xyz] 负值字符集合。匹配未包含的任意字符。例如，“[^abc]”可以匹配“plain”中的“p”。 [a-z] 字符范围。匹配指定范围内的任意字符。例如，“[a-z]”可以匹配“a”到“z”范围内的任意小写字母字符。 [^a-z] 负值字符范围。匹配任何不在指定范围内的任意字符。例如，“[^a-z]”可以匹配任何不在“a”到“z”范围内的任意字符。 \\b 匹配一个单词边界，也就是指单词和空格间的位置。例如，“er\\b”可以匹配“never”中的“er”，但不能匹配“verb”中的“er”。 \\B 匹配非单词边界。“er\\B”能匹配“verb”中的“er”，但不能匹配“never”中的“er”。 \\cx 匹配由x指明的控制字符。例如，\\cM匹配一个Control-M或回车符。x的值必须为A-Z或a-z之一。否则，将c视为一个原义的“c”字符。 \\d 匹配一个数字字符。等价于[0-9]。 \\D 匹配一个非数字字符。等价于[^0-9]。 \\f 匹配一个换页符。等价于\\x0c和\\cL。 \\n 匹配一个换行符。等价于\\x0a和\\cJ。 \\r 匹配一个回车符。等价于\\x0d和\\cM。 \\s 匹配任何空白字符，包括空格、制表符、换页符等等。等价于[ \\f\\n\\r\\t\\v]。 \\S 匹配任何非空白字符。等价于[^ \\f\\n\\r\\t\\v]。 \\t 匹配一个制表符。等价于\\x09和\\cI。 \\v 匹配一个垂直制表符。等价于\\x0b和\\cK。 \\w 匹配包括下划线的任何单词字符。等价于“[A-Za-z0-9_]”。 \\W 匹配任何非单词字符。等价于“[^A-Za-z0-9_]”。 \\xn 匹配n，其中n为十六进制转义值。十六进制转义值必须为确定的两个数字长。例如，“\\x41”匹配“A”。“\\x041”则等价于“\\x04\u0026amp;1”。正则表达式中可以使用ASCII编码。. \\num 匹配num，其中num是一个正整数。对所获取的匹配的引用。例如，“(.)\\1”匹配两个连续的相同字符。 \\n 标识一个八进制转义值或一个向后引用。如果\\n之前至少n个获取的子表达式，则n为向后引用。否则，如果n为八进制数字（0-7），则n为一个八进制转义值。 \\nm 标识一个八进制转义值或一个向后引用。如果\\nm之前至少有nm个获得子表达式，则nm为向后引用。如果\\nm之前至少有n个获取，则n为一个后跟文字m的向后引用。如果前面的条件都不满足，若n和m均为八进制数字（0-7），则\\nm将匹配八进制转义值nm。 \\nml 如果n为八进制数字（0-3），且m和l均为八进制数字（0-7），则匹配八进制转义值nml。 \\un 匹配n，其中n是一个用四个十六进制数字表示的Unicode字符。例如，\\u00A9匹配版权符号（©）。 ","date":"2022-01-11T09:29:51+08:00","permalink":"https://x-xkang.com/p/%E5%B8%B8%E7%94%A8%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/","title":"常用正则表达式"},{"content":"go:build go:linkname 1 //go:linkname localname importpath.name 该指令指示编译器使用 importpath.name 作为源码中声明为 localname 的变量的或函数的目标文件符号名称，但是由于这个伪指令可以破坏类型系统和包模块化，只有引用了 unsafe 包才可以使用。 简单来讲，就是 importpath.name 是 localname 的符号别名，编译器实际上会调用 localname，使用的前提是引入了 unsafe 包才能使用。\n例如 time/time.go\n1 2 // Provided by package runtime. func now() (sec int64, nsec int32, mono int64) runtime/timestub.go\n1 2 3 4 5 6 7 import _ \u0026#34;unsafe\u0026#34; // for go:linkname //go:linkname time_now time.now func time_now() (sec int64, nsec int32, mono int64) { sec, nsec = walltime() return sec, nsec, nanotime() } now 方法并没有具体实现，注释上也描述具体实现由 runtime 包完成，看一下 runtime 包中的代码，先引入了 unsafe 包，再定义了 //go:lickname time_now time.now。\n第一个参数time_now 代表本地变量或方法，第二个参数time.now标识需要建立链接的变量、方法路径。也就是说，//go:lickname 是可以跨包使用的。\n另外 go build 是无法编译 go:linkname的，必须使用单独的 compile 命令进行编译，因为 go build 会加上 -complete 参数，这个参数会检查到没有方法体的方法，并且不通过。\ngo:noscape 1 //go:noscape 该指令指定下一个有声明但没有主体（意味着实现有可能不是 Go）的函数，不允许编译器对其做逃逸分析。\n一般情况下，该指令用于内存分配优化。编译器默认会进行逃逸分析，会通过规则判定一个变量是分配到堆上还是栈上。\n但凡事有意外，一些函数虽然逃逸分析其是存放到堆上。但是对于我们来说，它是特别的。我们就可以使用 go:noescape 指令强制要求编译器将其分配到函数栈上。\n例如 1 2 3 4 // memmove copies n bytes from \u0026#34;from\u0026#34; to \u0026#34;to\u0026#34;. // in memmove_*.s //go:noescape func memmove(to, from unsafe.Pointer, n uintptr) 我们观察一下这个案例，它满足了该指令的常见特性。如下：\nmemmove_*.s: 只有声明，没有主体，其主体是由底层汇编实现的。\nmemmove: 函数功能，在栈上处理性能会更好。\ngo:noslip 1 //go:noslip 该指令指定文件中声明的下一个函数不得包含堆栈溢出检查。\n简单来讲，就是这个函数跳过堆栈溢出的检查。\n例如 1 2 3 4 //go:nosplit func key32(p *uintptr) *uint32 { return (*uint32)(unsafe.Pointer(p)) } go:nowritebarrierrec 1 //go:nowritebarrierrec 该指令表示编译器遇到写屏障时就会产生一个错误，并且允许递归。也就是这个函数调用的其他函数如果有写屏障也会报错。\n简单来讲，就是针对写屏障的处理，防止其死循环。\n例如 1 2 3 4 //go:nowritebarrierrec func gcFlushBgCredit(scanWork int64) { ... } go:yeswritebarrierrec 1 //go:yeswritebarrierrec 该指令与 go:nowritebarrierrec 相对，在标注 go:nowritebarrierrec 指令的函数上，遇到写屏障会产生错误。\n而当编译器遇到 go:yeswritebarrierrec 指令时将会停止。\n例如 1 2 3 4 //go:yeswritebarrierrec func gchelper() { ... } go:noinline 1 //go:noinline 该指令表示该函数禁止进行内联。\n1 2 3 4 //go:noinline func unexportedPanicForTesting(b []byte, i int) byte { return b[i] } 例如 我们观察一下这个案例，是直接通过索引取值，逻辑比较简单。如果不加上 go:noinline 的话，就会出现编译器对其进行内联优化。\n显然，内联有好有坏。该指令就是提供这一特殊处理。\ngo:norace 1 //go:norace 该指令表示禁止进行竞态检测。\n常见的形式就是在启动时执行 go run -race，能够检测应用程序中是否存在双向的数据竞争，非常有用。\n例如 1 2 3 4 //go:norace func forkAndExecInChild(argv0 *byte, argv, envv []*byte, chroot, dir *byte, attr *ProcAttr, sys *SysProcAttr, pipe int) (pid int, err Errno) { ... } go:notinheap 1 //go:notinheap 该指令常用于类型声明，它表示这个类型不允许从 GC 堆上进行申请内存。\n在运行时中常用其来做较低层次的内部结构，避免调度器和内存分配中的写屏障，能够提高性能。\n例如 1 2 3 4 5 6 7 8 9 // notInHeap is off-heap memory allocated by a lower-level allocator // like sysAlloc or persistentAlloc. // // In general, it\u0026#39;s better to use real types marked as go:notinheap, // but this serves as a generic type for situations where that isn\u0026#39;t // possible (like in the allocators). // //go:notinheap type notInHeap struct{} 每日一算 描述 2个逆序的链表，要求从低位开始相加，得出结果也逆序输出，返回值是逆序结果链表的头结点\n解题思路 需要注意的是进位的问题，极端情况如下：\nInput: (9 -\u0026gt; 9 -\u0026gt; 9 -\u0026gt; 9) + (1 -\u0026gt; ) Output 0 -\u0026gt; 0 -\u0026gt; 0 -\u0026gt; 0 -\u0026gt; 1\n为了处理方法统一，可以先建立一个虚拟头结点，这个虚拟头结点的Next指向真正的head，这样head不需要单独处理，直接wihle循环即可。另外判断循环终止的条件不用是 p.Next != nil，这样最后一位还需要额外计算，终止条件应该是 p != nil。\n代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 type ListNode struct { Val int Next *ListNode } func addTwoNumbers(l1 *ListNode, l2 *ListNode) *ListNode { if l1 == nil || l2 == nil{ return nil } // 虚拟头结点 head := \u0026amp;ListNode{ Val: 0, Next: nil, } current := head carry := 0\t// 是否需要进位 // 遍历 for l1 != nil || l2 != nil { var x, y int if l1 == nil { x = 0 }else{ x = l1.Val } if l2 == nil { y = 0 }else { y = l2.Val } current.Next = \u0026amp;ListNode{ Val: (x + y + carry) % 10, Next: nil, } current = current.Next carry = (x+y+carry) / 10 if l1 != nil { l1 = l1.Next } if l2 != nil { l2 = l2.Next } fmt.Println(\u0026#34;carry:\u0026#34;, carry) } if carry \u0026gt; 0 {\t// 最后一位相加又进位，要在尾结点再加一个结点 current.Next = \u0026amp;ListNode{ Val: carry % 10, Next: nil, } } return head.Next } ","date":"2021-12-16T09:35:02+08:00","permalink":"https://x-xkang.com/p/golang-%E6%BA%90%E7%A0%81%E9%87%8C%E7%9A%84-/go-%E6%8C%87%E4%BB%A4%E9%83%BD%E4%BB%A3%E8%A1%A8%E4%BB%80%E4%B9%88%E6%84%8F%E6%80%9D/","title":"Golang 源码里的 //go: 指令，都代表什么意思？"},{"content":"ASCII ascii码使用指定的7位或8位二进制数组合表示128或256种可能的字符，标准ASCII码也叫基础ASCII码，使用7位二进制数（剩下的1位二进制为0）来表示所有的大写和小写字母，数字0到9、标点符号，以及再美式英语中使用的特殊字符，其中：\n0 ~ 31及127（共33个）是控制字符或通信专用字符（其余为可显示字符），如控制字符：LF（换行）, CR（回车）；通信专用字符：SOH（文头）、ACK（确认）等；ASCII值为8、9、10和13分别转换为退格、制表、换行和回车，它们并没有特定的图形显示，但会依不同的应用程序，而对文本显示不同的影响。 32 ~ 126（共95个）是字符（32是空格），其中48~57位0到9十个阿拉伯数字，65 ~ 90为26个大写英文字母，97 ~ 122为26个小写英文字母，其余为一些标点符号、运算符号等。 同时还要注意，在标准ASCII中，其中最高位（b7）用作奇偶校验。所谓奇偶校验，是指在代码传送过程中用来校验是否出现错误的一种方法，一般分为奇校验和偶校验两种。\n奇校验规定：正确的代码一个字节中1的个数必须是1，若非技术，则在最高位b7添1. 偶校验规定：正确的代码一个字节中1的个数必须是偶数，若非偶数，则在最高位（b7）添1. 后128个称为扩展ASCII码。许多基于x86的系统都支持使用扩展（或“高”ASCII）。扩展ASCII码允许将每个字符的第8位用于确定附加的128个特殊符号字符、外来语字母和图形符号\nUnicode Unicode是国际组织制定的可以容纳世界上所有文字和符号的字符编码方案。Unicode用数字0-0x10FFFF来映射这些字符，最多可以容纳1114112个字符，或者说有1114112个码位。码位就是可以分配给字符的数字。UTF-8、UTF-16、UTF-32都是将数字转换到程序数据的编码方案。\nUnicode 源于一个很简单的想法：将全世界所有的字符包含在一个集合里，计算机只要支持这一个字符集，就能显示所有的字符，再也不会有乱码了。\n它从 0 开始，为每个符号指定一个编号，这叫做”码点”（code point）。比如，码点 0 的符号就是 null（表示所有二进制位都是 0）。 这么多符号，Unicode 不是一次性定义的，而是分区定义。每个区可以存放 65536 个（2^16）字符，称为一个平面（plane）。目前，一共有 17 个平面，也就是说，整个 Unicode 字符集的大小现在是 2^21。 最前面的 65536 个字符位，称为基本平面（缩写 BMP），它的码点范围是从 0 一直到 2^16-1，写成 16 进制就是从 U+0000 到 U+FFFF。所有最常见的字符都放在这个平面，这是 Unicode 最先定义和公布的一个平面。 剩下的字符都放在辅助平面（缩写 SMP），码点范围从 U+010000 一直到 U+10FFFF。 Unicode 只规定了每个字符的码点，到底用什么样的字节序表示这个码点，就涉及到编码方法。 Unicode 编码方案 之前提到，Unicode 没有规定字符对应的二进制码如何存储。以汉字“汉”为例，它的 Unicode 码点是 0x6c49，对应的二进制数是 110110001001001，二进制数有 15 位，这也就说明了它至少需要 2 个字节来表示。可以想象，在 Unicode 字典中往后的字符可能就需要 3 个字节或者 4 个字节，甚至更多字节来表示了。 这就导致了一些问题，计算机怎么知道你这个 2 个字节表示的是一个字符，而不是分别表示两个字符呢？这里我们可能会想到，那就取个最大的，假如 Unicode 中最大的字符用 4 字节就可以表示了，那么我们就将所有的字符都用 4 个字节来表示，不够的就往前面补 0。这样确实可以解决编码问题，但是却造成了空间的极大浪费，如果是一个英文文档，那文件大小就大出了 3 倍，这显然是无法接受的。 于是，为了较好的解决 Unicode 的编码问题， UTF-8 和 UTF-16 两种当前比较流行的编码方式诞生了。当然还有一个 UTF-32 的编码方式，也就是上述那种定长编码，字符统一使用 4 个字节，虽然看似方便，但是却不如另外两种编码方式使用广泛。 UTF8 UTF-8 是一个非常惊艳的编码方式，漂亮的实现了对 ASCII 码的向后兼容，以保证 Unicode 可以被大众接受。\nUTF-8 是目前互联网上使用最广泛的一种 Unicode 编码方式，它的最大特点就是可变长。它可以使用 1 - 4 个字节表示一个字符，根据字符的不同变换长度。编码规则如下： 1.对于单个字节的字符，第一位设为 0，后面的 7 位对应这个字符的 Unicode 码点。因此，对于英文中的 0 - 127 号字符，与 ASCII 码完全相同。这意味着 ASCII 码那个年代的文档用 UTF-8 编码打开完全没有问题。 2.对于需要使用 N 个字节来表示的字符（N \u0026gt; 1），第一个字节的前 N 位都设为 1，第 N + 1 位设为 0，剩余的 N - 1 个字节的前两位都设位 10，剩下的二进制位则使用这个字符的 Unicode 码点来填充。 编码规则如下：\nUnicode 十六进制码点范围 UTF-8 二进制 0000 0000 - 0000 007F 0xxxxxxx 0000 0080 - 0000 07FF 110xxxxx 10xxxxxx 0000 0800 - 0000 FFFF 1110xxxx 10xxxxxx 10xxxxxx 0001 0000 - 0010 FFFF 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx 根据上面编码规则对照表，进行 UTF-8 编码和解码就简单多了。下面以汉字“汉”为利，具体说明如何进行 UTF-8 编码和解码。\n“汉”的 Unicode 码点是 0x6c49（110 1100 0100 1001），通过上面的对照表可以发现，0x0000 6c49 位于第三行的范围，那么得出其格式为 1110xxxx 10xxxxxx 10xxxxxx。接着，从“汉”的二进制数最后一位开始，从后向前依次填充对应格式中的 x，多出的 x 用 0 补上。这样，就得到了“汉”的 UTF-8 编码为 11100110 10110001 10001001，转换成十六进制就是 0xE6 0xB7 0x89。\n解码的过程也十分简单：如果一个字节的第一位是 0 ，则说明这个字节对应一个字符；如果一个字节的第一位 1，那么连续有多少个 1，就表示该字符占用多少个字节。\n每日一算 描述 在数组中找到 2 个数之和等于给定值的数字，结果返回 2 个数字在数组中的下标\n解题思路 利用map的特性，遍历数组预算计算出和给定值的差值，也就是目标元素，若值已经在map中，说明已找到，若没有，则把当前元素以 值=\u0026gt;下标 的形式存到map中\n代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func twoSum(arr []int, target int)[]int{ var m = map[int]int{} for i := 0; i \u0026lt; len(arr); i++ { another := target - arr[i] if _, ok := m[another]; ok == true { return []int{m[another], i} }else{ m[arr[i]] = i } } return []int{-1, -1} } ","date":"2021-12-02T09:31:17+08:00","permalink":"https://x-xkang.com/p/asciiunicode%E5%92%8Cutf8/","title":"ASCII、Unicode和UTF8"},{"content":"问题描述 生产环境每隔一段时间会出现mysql数据库的死锁日志:\n同一条update语句，where 条件不同，但是会触发死锁，于是查看阿里云的数据库死锁日志，如下：\nupdate 语句如果使用的是主键索引，会将主键索引锁住，如果是普通索引，会先将普通索引锁住，然后根据普通索引的主键id再锁住主键索引，同一条update语句的索引执行应该是一样的，不应该存在互相等待释放的情况，于是有点陷入僵局，google一下有遇到相似问题的帖子，查看了执行计划，之前也看了执行计划，但是忽略了type字段和extra字段，结果如下：\n索引合并查询，会同时使用idx_status_vmstatus 和 uniq_instance 扫描记录并给普通索引加锁，然后通过普通索引中的主键ID去锁定主键索引，问题就出现在这里，由于 idx_status_vmstatus 索引扫描和 uniq_instance索引扫描是同时的，如果两条update语句同时执行，则 事务2 先锁定 锁定 uniq_instance 成功后锁定对应的主键，然后事务1 锁定idx_status_vmstatus 成功后也去锁定主键,此时主键已被事务2锁定，于是阻塞等待primary释放，接着事务2开始扫描 idx_status_vmstatus 发现普通索引被事务1锁住，于是阻塞等待idx_status_vmstatus，于是出现最终的 事务2等待 事务2释放idx_status_vmstatus，事务1等待事务1释放primary，即出现死锁。\n解决方案也比较简单，先查出主键ID，使用主键ID再更新记录，因为使用主键ID直接加锁的话，锁粒度更小，及时同时更新一条记录，也不会出现同时等待对方将锁释放的场景。问题描述的比较简单，但在排查过程中还是走了不少弯路的。\n","date":"2021-11-22T10:48:54+08:00","permalink":"https://x-xkang.com/p/mysql-deadlock/","title":"Mysql Deadlock"},{"content":"一、GMT 1、什么是GMT GMT (Greenwich Mean Time) 格林威治标准时间\n它规定太阳每天经过位于英国伦敦郊区的皇家格林威治天文台的时间为中午12点。 2、GMT的历史 格林威治皇家天文台为了海上霸权的扩张计划，在十七世纪就开始进行天体观测。为了天文观测，选择了穿过英国伦敦格林威治天文台子午仪中心的一条经线作为零度参考线，这条线，简称格林威治子午线。\n1884年10月在美国华盛顿召开了一个国际子午线会议，该会议将格林威治子午线设定为本初子午线，并将格林威治平时 (GMT, Greenwich Mean Time) 作为世界时间标准（UT, Universal Time）。由此也确定了全球24小时自然时区的划分，所有时区都以和 GMT 之间的偏移量做为参考。\n1972年之前，格林威治时间（GMT）一直是世界时间的标准。1972年之后，GMT 不再是一个时间标准了。\n由于地球在它的椭圆轨道里的运动速度不均匀，这个时间可能和实际的太阳时相差16分钟。\n二、UTC 1、什么是UTC ？ UTC (Coodinated Universal Time)，协调世界时\n又称世界统一时间、世界标准时间、国际协调时间。由于英文（CUT）和法文（TUC）的缩写不同，作为妥协，简称UTC\nUTC是现在全球通用的时间标准，全球各地都同意将各自的时间进行同步协调。UTC时间是经过平均太阳时（以格林威治时间GMT为准）、地轴运动修正后的新时标以及以秒为单位的国际原子时所综合精算而成\n在军事中协调世界时会用”Z“来表示。又由于Z在无线电联络中使用”Zulu“作代称，协调世界时也会被称为”Zulu time“。\n2、UTC的组成 原子时间（TAI，International Atomic Time）：结合了劝阻400个所有的原子钟而得到的时间，它决定了我们每个人的中标中，时间流动的速度。 世界时间（UT，Universal Time）： 也称天文时间，或太阳时，它的依据是地球的自转，我们用它来确定多少原子时，对应于一个地球日的时间长度。 3、UTC的历史 1960年，国际无线电咨询委员会规范统一了UTC的概念，并在次年投入实际使用。1967年以前，UTC被数次调整过，原因是要使用润秒（leap second）来将UTC和地球自转时间进行统一。\n三、GMT与UTC GMT是前世界标准时，UTC是现世界标准时； UTC比GMT更准确，以原子时计时，适应现代社会的精准计时，但在不需要精确到秒的情况下，二者可视为等同； 每年格林尼治天文台会发调时信息，给予UTC。 ","date":"2021-09-29T13:38:27+08:00","permalink":"https://x-xkang.com/p/gmt-%E4%B8%8E-utc-%E6%97%B6%E9%97%B4%E6%A0%BC%E5%BC%8F/","title":"GMT 与 UTC 时间格式"},{"content":"我们平时使用的查询 sql 基本格式如下：\n1 2 3 4 5 6 7 8 9 SELECT DISTINCT \u0026lt;select_list\u0026gt; FROM \u0026lt;left_table\u0026gt; \u0026lt;join_type\u0026gt; JOIN \u0026lt;right_table\u0026gt; ON \u0026lt;join_condition\u0026gt; WHERE \u0026lt;where_condition\u0026gt; GROUP BY \u0026lt;group_by_condition\u0026gt; HAVING \u0026lt;having_condition\u0026gt; ORDER BY \u0026lt;order_by\u0026gt; LIMIT \u0026lt;limit_number\u0026gt;; 实际的执行顺序并不是如上书写顺序一样的：\nFROM: 对 from 左右的表计算笛卡尔积，产生虚拟表VT1； ON: 对笛卡尔积进行筛选，只有符合条件的行才会被记录到虚拟表VT2中； JOIN: 如果是 OUT JOIN，那么将保留表中（如左表或者右表）未匹配的行作为外部行添加到虚拟表VT2中，从而产生了虚拟表VT3； WHERE: 对 JOIN 之后的虚拟表VT3进行进一步的筛选，满足条件的留下生成虚拟表VT4； GROUP BY: 对虚拟表VT4进行分组，生成VT5； HAVING: 对分组后的VT5进行筛选，生成虚拟表VT6； SELECT: 选择 SELECT 指定的列，插入到虚拟表VT7中； DISTINCT: 对虚拟表VT7中的数据进行去重，产生VT8； ORDER BY: 对虚拟表VT8的中的数据进行排序生成VT9； LIMIT: 取出VT9中指定行的数据，产生虚拟表VT10，并返回数据 ","date":"2021-09-25T17:18:01+08:00","permalink":"https://x-xkang.com/p/mysql-%E5%85%B3%E9%94%AE%E5%AD%97%E7%9A%84%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F/","title":"Mysql 关键字的执行顺序"},{"content":"一、Int 1、string 转 int 1 2 3 4 5 6 var str = \u0026#34;1001\u0026#34; n, _ := strconv.Atoi(str) // 官方 \u0026#34;strconv\u0026#34; 包 fmt.Printf(\u0026#34;type:%T, value:%v\\n\u0026#34;, n, n) // type:int, value:1001 2、string 转 int64 1 2 3 4 5 6 var str = \u0026#34;1001\u0026#34; n, _ := strconv.ParseInt(str, 10, 64) fmt.Printf(\u0026#34;type:%T, value:%v\\n\u0026#34;, n, n) // type:int64, value:1001 3、string 转浮点数 1 2 3 4 5 6 var str = \u0026#34;3.1415926\u0026#34; n, _ := strconv.ParseFloat(str, 10) fmt.Printf(\u0026#34;type:%T, value:%v\\n\u0026#34;, n, n) // type:float64, value:3.1415926 二、String 1、int 转 string 1 2 3 4 5 6 var n int = 100 str := strconv.Itoa(n) fmt.Printf(\u0026#34;type:%T, value:%v\\n\u0026#34;, str, str) // type:string, value:100 2、int64 转 string 1 2 3 4 5 6 var n int64 = 100 str := strconv.FormatInt(n, 10) fmt.Printf(\u0026#34;type:%T, value:%v\\n\u0026#34;, str, str) // type:string, value:100 3、uint32 转 string 1 2 3 4 5 6 var n uint32 = 10 str := strconv.ParseUint(uint64(n), 10) fmt.Printf(\u0026#34;type:%T, value:%v\\n\u0026#34;, str, str) // type:string, value:100 三、Struct 1、json 转 struct 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 type Person struct { Name string `json:\u0026#34;name\u0026#34;` Age int `json:\u0026#34;age\u0026#34;` } var jsonStr = []byte(`{\u0026#34;name\u0026#34;: \u0026#34;neil\u0026#34;, \u0026#34;age\u0026#34;: 21}`) var p = Person{} err := json.Unmarshal(jsonStr, \u0026amp;p) // 官方 “json” 包 if err != nil { fmt.Println(\u0026#34;Unmarshal failed:\u0026#34;, err) return } fmt.Printf(\u0026#34;p:%#v\\n\u0026#34;, p) // p:main.Person{Name:\u0026#34;neil\u0026#34;, Age:21} 2、map 转 struct 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 type Person struct { Name string `json:\u0026#34;name\u0026#34;` Age int `json:\u0026#34;age\u0026#34;` } var m = map[string]interface{}{ \u0026#34;name\u0026#34;: \u0026#34;king\u0026#34;, \u0026#34;age\u0026#34;: 19, } jsonStr, err := json.Marshal(m) if err != nil { fmt.Println(\u0026#34;Marshal failed:\u0026#34;, err) return } err = json.Unmarshal(jsonStr, \u0026amp;p) if err != nil { fmt.Println(\u0026#34;Unmarshal failed too:\u0026#34;, err) return } fmt.Printf(\u0026#34;p:%#v\\n\u0026#34;, p) // p:main.Person{Name:\u0026#34;king\u0026#34;, Age:19} Map 1、json 转 map 1 2 3 4 5 6 7 8 9 10 11 12 13 14 var jsonStr = []byte(`{\u0026#34;name\u0026#34;: \u0026#34;neil\u0026#34;, \u0026#34;age\u0026#34;: 21}`) var m = map[string]interface{}{} err := json.Unmarshal(jsonStr, \u0026amp;m) if err != nil { fmt.Println(\u0026#34;Unmarshal failed:\u0026#34;, err) return } fmt.Printf(\u0026#34;m:%#v\\n\u0026#34;, m) // m:map[string]interface {}{\u0026#34;age\u0026#34;:21, \u0026#34;name\u0026#34;:\u0026#34;neil\u0026#34;} 2、 struct 转 map 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 type Person struct { Name string `json:\u0026#34;name\u0026#34;` Age int `json:\u0026#34;age\u0026#34;` } var p = Person{ Name: \u0026#34;king\u0026#34;, Age: 17, } var jsonStr, _ = json.Marshal(p) var m = map[string]interface{}{} _ = json.Unmarshal(jsonStr, \u0026amp;m) fmt.Printf(\u0026#34;m:%#v\\n\u0026#34;, m) // m:map[string]interface {}{\u0026#34;age\u0026#34;:17, \u0026#34;name\u0026#34;:\u0026#34;king\u0026#34;} ","date":"2021-06-24T13:58:33+08:00","permalink":"https://x-xkang.com/p/golang-%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2/","title":"Golang 常用数据类型转换"},{"content":"一、Mysql 事务 MySQL 事务主要用于处理操作量大、复杂度高的数据\nMySQL 数据库中只有 Innodb 存储引擎支持事务操作 事务处理可以用来维护数据库的完整性，保证成批的 SQL 要么全部执行，要么全部不执行 事务用来管理insert，update，delete语句 二、事务特性 一般来说，事务必须满足 4 个条件（ACID），即原子性、一致性、持久性、隔离性，具体如下：\n1、原子性 (Atomicity) 一个事务（Transaction）中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节，事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像事务没有执行过一样。\n2、一致性（Consistency） 在事务开始之前以及事务结束之后，数据库的完整性没有被破坏。这标识写入的数据必须完全符合所有的预设规则，这包含数据的精确度，串联性以及后续数据库可以自发的完成预定的工作。\n3、隔离性（Isolation） 数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致，事务隔离分为以下不同级别：\n读未提交（Read uncommited）: 允许脏读，也就是可能读到其他会话中未提交事务修改的数据。\n读已提交（Read commited）: 只能读取到已提交的数据。\n可重复读（Repeatable read）: 在同一个事务内的查询都是从开始时刻一致的，InnoDB 存储引擎默认的事务隔离级别就是可重复读，在 SQL 标准中，该隔离级别消除了不可重复读，但还是存在幻读。\n串行化（Serializable）: 完全串行化的读，每次读都需要获得表级的共享锁，读写相互都会阻塞。\n4、持久性（Durability） 事务处理结束后，对数据的修改就是永久的，几遍系统故障也不会丢失。\n三、事务的并发处理 准备工作：创建数据表，插入一条数据\n1 2 3 4 5 6 7 8 create table user( id int(10) not null auto_increment comment \u0026#39;主键ID\u0026#39;, name varchar(30) not null default \u0026#39;\u0026#39; comment \u0026#39;用户名\u0026#39;, primary key(id) ) engine=innodb charset=utf8mb4; # 插入数据 insert into `user`(`name`) values(\u0026#39;老王01\u0026#39;); 事务并发可能出现的情况：\n脏读 一个事务读到了另一个未提交事务修改过的数据\n1、会话 B 开启一个事务，把id=1的name改为老王01；\n2、会话 A 也开启一个事务，读取id=1的name，次时的查询结果为老王02；\n3、会话 B 的事务回滚了修改的操作，这样会话 A 读到的数据就是不存在的；\n这个现象就是脏读。（脏读只会在读未提交的隔离级别中才会出现）。\n不可重复读 一个事务只能读到另一个已经提交的事务修改过的数据，并且其他事务每对该数据进行一次修改并提交后，该事务都能查询得到最新值。（不可重复读在读未提交和读已提交隔离级别都可能会出现）\n1、会话 A 开启事务，查询id=1的 name 是老王01；\n2、会话 B 将id=1的 name 更新为老王02（隐式事务，autocommit=1，执行完 sql 会自动 commit）；\n3、会话 A 再查询时id=1的 name 为老王02；\n4、会话 B 又将id=1的 name 更新为老王03；\n5、会话 A 再查询id=1的 name 时，结果为老王03。\n这种现象就是不可重复读。\n幻读 一个事务先根据某些条件查出一些记录，之后另一个事务又想表中插入了符合这些条件的记录，原先的事务再次按照该条件查询时，能够把另一个事务插入的数据也查出来。 （幻读在读未提交、读已提交、可重复读隔离级别中都可能会出现）\n1、会话 A 开始事务，查询id\u0026gt;0的数据，结果只有 name=老王 01 的一条数据\n2、会话 B 像数据表中插入了一条name=老王02的数据（隐式事务，执行 sql 后自动 commit）\n3、会话 A 的事务再次查询 id\u0026gt;0的数据\n不同隔离级别下出现事务并发问题的可能 隔离级别 脏读 不可重复读 幻读 读未提交 可能 可能 可能 读已提交 不可能 可能 可能 可重复读 不可能 不可能 可能 串行化 不可能 不可能 不可能 四、事务的实现原理 首先了解一下 redo log 和 undo log\n1、redo log MYSQL 为了提升性能不会把每次的修改都实时同步到磁盘，而是会优先存储到 Buffer Pool（缓冲池）里面，把这个当做缓存来用，然后使用后台线程去做缓冲池和磁盘之间的同步\n如果还没来得及同步数据就出现宕机或者断电，就会导致丢失部分已提交事务的修改信息，\n所以引入了redo log来记录已成功提交事务的修改信息，并且把 redo log 持久化到磁盘，系统重启之后读取 redo log 恢复最新数据\nredo log 是用来恢复数据的，用于保障已提交事务的持久化特性。\n2、undo log undo log 叫做回滚日志，用于记录数据被修改前的信息，与 redo log 记录的数据相反，redo log 是记录修改后的数据，undo log 记录的是数据的逻辑变化，为了发生错误时回滚之前的操作，需要将之前的操作都记录下来，然后在发生错误时才可以回滚\n每次写入数据或者修改数据之前都会把修改前的信息记录到 undo log\n3、事务特性的具体实现原理 事务的原子性通过 undo log 来实现的 事务的持久性是通过 redo log 实现的 事务的隔离性是通过 读写锁 + MVCC 实现的 事务的一致性是通过 **原子性、持久性、隔离性**来实现的 3.1、原子性的实现 每条数据变更（insert/update/delete）操作都会记录一条undo log，并且undo log必须先于数据持久化到磁盘上。\n所谓的回滚就是根据undo log做逆向操作，比如delete的逆向操作是insert，insert的逆向操作是delete，update的逆向操作是update等。\n为了做到同时成功或者同时失败，当系统发生错误或者执行rollback时需根据undo log进行回滚\n3.2、持久性的实现 Mysql 的数据存储机制是将数据最终持久化到磁盘上，并且频繁的进行磁盘 IO 是非常消耗性能的，为了提升性能，InnoDB 提供了缓冲池（Buffer Pool），缓冲池中包含了磁盘数据也的映射，可以当做缓存来使用\n读数据：会首先从缓冲池中读取，若没有，则从磁盘读取并放入缓冲池中\n写数据：会首先写入缓冲池中，缓冲池中的数据会定期同步到磁盘中\n那么问题来了，如果在缓冲池的数据还没有同步到磁盘上时，出现了机器宕机或者断电，可能会出现数据丢失的问题，因此我们需要记录已提交事务的数据，于是，redo log登场了， redo log 在执行数据变更（insert/update/delete）操作的时候，会变更后的结果记录在缓冲区，待commit事务之后同步到磁盘\n至于redo log也要进行磁盘 IO，为什么还要用\n(1)、redo log是顺序存储，而缓存同步是随机操作\n(2)、缓存同步是以数据页为单位，每次传输的数据大小小于redo log\n3.3、隔离性的实现 读未提交： 读写并行，读的操作不能排斥写的操作，因此会出现脏读,不可重复读,幻读的问题\n读已提交： 使用排他锁X，更新数据需要获取排他锁，已经获取排他锁的数据，不可以再获取共享锁S以及排他锁X，读取数据使用了MVCC（Mutil-Version Concurrency Control）多版本并发控制机制（后续单独展开）以及Read view的概念，每次读取都会产生新的Read view，因此可以解决脏读问题，但解决不了不可重复读和幻读的问题\n可重复读： 同上也是利用MVCC机制实现，但是只在第一次查询的时候创建Read view，后续的查询还是沿用之前的Read view，因此可以解决不可重复读的问题，具体不在这展开，但还是有可能出现幻读\n串行化 ：读操作的时候加共享锁，其他事务可以并发读，但是不能并发写，执行写操作的时候加排他锁，其他事务既不能并发写，也不能并发读，串行化可以解决事务并发中出现的脏读、不可重复读、幻读问题，但是并发性能却因为加锁的开销变得很差\n3.4、一致性的实现 一致性的实现其实是通过原子性、持久性，隔离性共同完成的\n五、结束语 了解 MySQL 的事务机制，以及实现原理，对于使用或者优化都有很大的帮助，要保持知其然和知其所以然的心态和持续学习的劲头，了解更多关于 Mysql 相关的知识！\n","date":"2021-03-24T11:18:58+08:00","permalink":"https://x-xkang.com/p/mysql-%E4%BA%8B%E5%8A%A1%E6%B5%85%E6%9E%90/","title":"Mysql 事务浅析"},{"content":"一、问题描述 1、生产环境主站每隔一段时间就会出现卡顿，接口响应慢，甚至超时的情况、\n2、测试环境重现不了（一抹诡异的光）\n二、问题排查 针对响应慢的接口进行优化，之前的代码风格也存在问题，还是有些滥用sync/await，一些没有依赖关系的操作，全部分开每行await同步执行，分析后把部分DB操作合并一个Promise执行 阿里云查了一下mysql的slow_log，有挺多的慢查询，优化了一部分SQL，业务逻辑太复杂，但是！没有解决问题，主站还是隔一段时间就卡 找到部分接口日志，超时的接口返回的是Knex.js数据库管理工具抛出的异常，KnexTimeoutError: Knex: Timeout acquiring a connection. The pool is probably full.，可能是连接池已满，获取连接失败导致的 看了一下数据库连接池的配置，最大连接数是30，获取连接的超时时间是60s，可能是并发量大加上部分操作未释放连接导致后续的操作无法正常获取数据库连接池的连接，大概又定位了一下可能的问题点\n1，主站的信息列表会隔几秒钟轮询，获取最新的数据，如果1000个用户在线，轮询周期内就会有1000个查询，中间也没有做缓存处理，导致并发到DB的请求会比较多 2，为了维护DB的状态统一，用户的部分操作用了事务，一些事务内包含了太多操作（感觉是长期占用连接未释放的罪魁祸首） 3，测试环境重现不了是因为没有经过压测，只测试了功能，没有测试性能，日常测试也没有大并发 三、问题验证 将数据库连接池的数量改成了1，使用事务的接口中做了延时的transaction.commit()操作，然后另外一个请求再去正常查询，\n结果显示，如果一个用户调用了事务操作的接口，然后再调用查询接口，查询会一直阻塞在获取连接的步骤，直至事务commit之后释放连接，如果在配置的timeout时间之前没有获取到，Knex就会抛出Timeout acquiring a connection. The pool is probably full的异常，\n也侧面印证了为什么测试环境复现不了这种情况，毕竟在没有压测的前提下。两个测试通过手动操作，并发量是达不到配置的数量的，也就不会出现卡顿的情况\n四、解决方案 1，优化长事务的操作，减少不必要的事务，提高处理效率（难度较大，业务逻辑比较复杂）；\n2，合理范围内增加数据库连接池的最大连接数配置，线上的mysql可连接300个，后端3个服务，现在配置是10、30、30，先把主站改50看看，连接数太大也会导致磁盘I/O效率大幅降低又会导致其他问题；\n3，轮询获取列表的操作，可以改成服务端主动去推（使用socket.io），然后加一个中间缓存（redis），毕竟列表数据变化的频率不是很高；\n4，数据库扩展成读写分离，update和insert的操作直接操作主库，大部分select操作转移到从库，即使有部分的事务操作慢，也不会导致主站的基本查询卡住\n五、写在最后 系统的业务逻辑比较复杂，从业务代码层面下手成本还是比较高，接口的耦合都比较高，重构都比改的成本低，开始的设计，可能也没有考虑扩展的问题，并发的问题等， 包括每个服务之间的通信问题，后期再慢慢优化吧，不怕有问题，就怕一直没遇到过问题！加油~。\n","date":"2021-03-12T17:42:20+08:00","permalink":"https://x-xkang.com/p/mysql-%E8%BF%9E%E6%8E%A5%E6%B1%A0%E9%97%AE%E9%A2%98/","title":"Mysql 连接池问题"},{"content":" 资源转自[B站]，如侵删\n","date":"2021-02-04T13:35:56+08:00","permalink":"https://x-xkang.com/p/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%89/","title":"计算机网络学习笔记（三）"},{"content":" 资源转自[B站]，如侵删\n","date":"2021-02-04T09:55:34+08:00","permalink":"https://x-xkang.com/p/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%BA%8C/","title":"计算机网络学习笔记（二）"},{"content":" 资源转自[B站]，如侵删\n","date":"2021-01-22T16:13:30+08:00","permalink":"https://x-xkang.com/p/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%80/","title":"计算机网络学习笔记（一）"},{"content":"一、物理层基本概念 物理层解决如何在连接各种计算机的传输媒体上传输数据比特流，而不是指具体的传输媒体，物理层的主要任务是确定与传输媒体接口有关的一些特性\n1、机械特性 定义物理连接的特性，规定物理连接时所采用的规格，接口形状，引线数目，引脚数量和排列情况\n2、电气特性 规定传输二进制位时，线路上信号的电压范围，阻抗匹配，传输速率和距离限制\n3、功能特性 知名某条线上出现的某一电平标识何种意义，接口不见的信号线的用途\n4、规程特性 (过程特性)定义各条物理线路的工作规程和时序关系\n二、数据通信基础知识 1、典型的数据通信模型 2、数据通信相关术语 通信的目的是传送消息\n数据：传送消息的实体，通常是有意义的符号序列\n信号：数据的电气/电磁表现，是数据在传输过程中的存在形式\n数字信号：代表消息的参数取值是离散的 模拟信号：代表消息的参数是连续的 信源：产生和发送数据的源头\n信宿：接受数据的终点\n信道：信号的传输媒介，一般用来表示向某一个方向传送信息的介质，因此一条通信线路往往包含一条发送信道和一条接受信道\n3、三种通信方式 从通信双方信息的交互方式来看，可以有三种基本方式：\n单工通信：只有一个方向的通信而没有反方向的交互，仅需一条信道\n半双工信道：通信的双方都可以发送或接受信息，但任何一方都不能同时发送和接受，需要两条信道\n全双工信道：通信双方可以同时发送和接受信息，也需要两条信道\n4、两种数据传输方式 串行传输：速度慢、费用低、适合远距离\n并行传输：速度快、费用高、适合近距离\n三、码元、波特、速率、带宽 1、码元 码元： 是指用给一个固定时长的信号波形（信号波形），代表不同离散数值的基本波形，是数字通信中数字信号的计量单位，这个时长内的信号称为k进制码元，而该时长称为码元宽度，当码元的离散状态有M个时（M大于2）此时码元为M进制码元\n1码元可以携带多个比特的信息量， 例如： 在使用二进制编码时，只有两种不同的码元，一种代表0状态，另一种代表1状态\n2、速率、波特、带宽 速率： 也叫 数据率 是指数据的传输速率，表示单位时间内传输的数据量，可以用码元传输速率和信息传输速率表示\n1）码元传输速率： 别名码元速率、波形速率、调制速率、符号速率等，它标识单位时间内数字通讯系统所传输的码元个数（也可称为脉冲个数或信号变化的次数），单位是 波特(Baud)。1波特表示数字通讯系统每秒传输一个码元，这里的码元可以是多进制的，但码元速率与进制无关。\n2）信息传输速率： 别名信息速率、比特率等，表示单位时间内数字通讯系统传输的二进制码元个数（即比特数），单位是 比特/秒（b/s）\n关系： 若一个码元携带 n bit的信息量，则M Baud的码元传输速率所对应的信息传输速率为 M x n bit/s\n带宽： 表示在单位时间内从网络中的某一点到另一点所能通过的 “最高数据率”，常用来表示网络的通信线路所能传输数据的能力，单位是b/s。\n四、奈氏准则和香农定理 1、失真 2、码间串扰 3、奈氏准则 4、香农定理 五、编码与调制 1、基带信号和宽带信号 2、编码与调制 1）非归零编码\n2）归零编码\n3）反向不归零编码\n4）曼彻斯特编码\n5）差分曼彻斯特编码\n6）4B/5B编码\n7）数字数据调制为模拟信号\n8）模拟数据编码为数字信号\n","date":"2020-01-25T17:21:56+08:00","permalink":"https://x-xkang.com/p/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C--%E7%89%A9%E7%90%86%E5%B1%82/","title":"计算机网络--物理层"},{"content":"一、模式切换 i 切换到输入模式，以输入字符。\nx 删除当前光标所在处的字符。\n: 切换到底线命令模式，以在最底一行输入命令。\n二、输入模式 在命令模式下按下i就进入了输入模式。 在输入模式中，可以使用以下按键：\n字符按键以及Shift组合，输入字符\nENTER，回车键，换行\nBACK SPACE，退格键，删除光标前一个字符\nDEL，删除键，删除光标后一个字符\n↑/↓/←/→ 方向键，在文本中移动光标\nHOME/END，移动光标到行首/行尾\nPage Up/Page Down，上/下翻页\nInsert，切换光标为输入/替换模式，光标将变成竖线/下划线\nESC，退出输入模式，切换到命令模式\n三、命令模式 1、移动光标 命令 作用 h 或 向左箭头键(←) 光标向左移动一个字符 j 或 向下箭头键(↓) 光标向下移动一个字符 k 或 向上箭头键(↑) 光标向上移动一个字符 l 或 向右箭头键(→) 光标向右移动一个字符 [Ctrl] + [f] 屏幕『向下』移动一页，相当于 [Page Down]按键 (常用) [Ctrl] + [b] 屏幕『向上』移动一页，相当于 [Page Up] 按键 (常用) [Ctrl] + [d] 屏幕『向下』移动半页 [Ctrl] + [u] 屏幕『向上』移动半页 + 光标移动到非空格符的下一行 - 光标移动到非空格符的上一行 n\u0026lt;space\u0026gt; 那个 n 表示『数字』，例如 20 。按下数字后再按空格键，光标会向右移动这一行的 n 个字符。例如 20\u0026lt;space\u0026gt; 则光标会向后面移动 20 个字符距离。 0 或功能键[Home] 这是数字『 0 』：移动到这一行的最前面字符处 (常用) $ 或功能键[End] 移动到这一行的最后面字符处(常用) H 光标移动到这个屏幕的最上方那一行的第一个字符 M 光标移动到这个屏幕的中央那一行的第一个字符 L 光标移动到这个屏幕的最下方那一行的第一个字符 G 移动到这个档案的最后一行(常用) nG n 为数字。移动到这个档案的第 n 行。例如 20G 则会移动到这个档案的第 20 行(可配合 :set nu) gg 移动到这个档案的第一行，相当于 1G 啊！ (常用) n\u0026lt;Enter\u0026gt; n 为数字。光标向下移动 n 行(常用) 2、搜索替换 命令 作用 /word 向光标之下寻找一个名称为 word 的字符串。例如要在档案内搜寻 vbird 这个字符串，就输入 /vbird 即可！ (常用) ?word 向光标之上寻找一个字符串名称为 word 的字符串。 n 这个 n 是英文按键。代表重复前一个搜寻的动作。举例来说， 如果刚刚我们执行 /vbird 去向下搜寻 vbird 这个字符串，则按下 n 后，会向下继续搜寻下一个名称为 vbird 的字符串。如果是执行 ?vbird 的话，那么按下 n 则会向上继续搜寻名称为 vbird 的字符串！ N 这个 N 是英文按键。与 n 刚好相反，为『反向』进行前一个搜寻动作。 例如 /vbird 后，按下 N 则表示『向上』搜寻 vbird 。 :n1,n2s/word1/word2/g n1 与 n2 为数字。在第 n1 与 n2 行之间寻找 word1 这个字符串，并将该字符串取代为 word2 ！举例来说，在 100 到 200 行之间搜寻 vbird 并取代为 VBIRD 则：『:100,200s/vbird/VBIRD/g』。(常用) :1,$s/word1/word2/g或 :%s/word1/word2/g 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！(常用) :1,$s/word1/word2/gc 或 :%s/word1/word2/gc 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！且在取代前显示提示字符给用户确认 (confirm) 是否需要取代！(常用) 【注】：使用 /word 配合 n 及 N 是非常有帮助的！可以让你重复的找到一些你搜寻的关键词！\n3、删除、复制与粘贴 命令 作用 x, X 在一行字当中，x 为向后删除一个字符 (相当于 [del] 按键)， X 为向前删除一个字符(相当于 [backspace] 亦即是退格键) (常用) nx n 为数字，连续向后删除 n 个字符。举例来说，我要连续删除 10 个字符， 『10x』。 dd 删除游标所在的那一整行(常用) ndd n 为数字。删除光标所在的向下 n 行，例如 20dd 则是删除 20 行 (常用) d1G 删除光标所在到第一行的所有数据 dG 删除光标所在到最后一行的所有数据 d$ 删除游标所在处，到该行的最后一个字符 d0 那个是数字的 0 ，删除游标所在处，到该行的最前面一个字符 yy 复制游标所在的那一行(常用) nyy n 为数字。复制光标所在的向下 n 行，例如 20yy 则是复制 20 行(常用) y1G 复制游标所在行到第一行的所有数据 yG 复制游标所在行到最后一行的所有数据 y0 复制光标所在的那个字符到该行行首的所有数据 y$ 复制光标所在的那个字符到该行行尾的所有数据 p, P p 为将已复制的数据在光标下一行贴上，P 则为贴在游标上一行！ 举例来说，我目前光标在第 20 行，且已经复制了 10 行数据。则按下 p 后， 那 10 行数据会贴在原本的 20 行之后，亦即由 21 行开始贴。但如果是按下 P 呢？ 那么原本的第 20 行会被推到变成 30 行。 (常用) J 将光标所在行与下一行的数据结合成同一行 c 重复删除多个数据，例如向下删除 10 行，[ 10cj ] u 复原前一个动作。(常用) [Ctrl]+r 重做上一个动作。(常用) . 不要怀疑！这就是小数点！意思是重复前一个动作的意思。 如果你想要重复删除、重复贴上等等动作，按下小数点『.』就好了！ (常用) 【注】：这个 u 与 [Ctrl]+r 是很常用的指令！一个是复原，另一个则是重做一次～ 利用这两个功能按键，你的编辑，嘿嘿！很快乐的啦！\n4、进入输入或取代的编辑模式 命令 作用 i, I 进入输入模式(Insert mode)： i 为『从目前光标所在处输入』， I 为『在目前所在行的第一个非空格符处开始输入』。 (常用) a, A 进入输入模式(Insert mode)： a 为『从目前光标所在的下一个字符处开始输入』， A 为『从光标所在行的最后一个字符处开始输入』。(常用) o, O 进入输入模式(Insert mode)： 这是英文字母 o 的大小写。o 为『在目前光标所在的下一行处输入新的一行』； O 为在目前光标所在处的上一行输入新的一行！(常用) r, R 进入取代模式(Replace mode)： r 只会取代光标所在的那一个字符一次； R 会一直取代光标所在的文字，直到按下 ESC 为止；(常用) [Esc] 退出编辑模式，回到一般模式中(常用) 【注】：上面这些按键中，在 vi 画面的左下角处会出现『\u0026ndash;INSERT\u0026ndash;』或『\u0026ndash;REPLACE\u0026ndash;』的字样。 由名称就知道该动作了吧！！特别注意的是，我们上面也提过了，你想要在档案里面输入字符时， 一定要在左下角处看到 INSERT 或 REPLACE 才能输入喔！\n5、指令行的储存、离开等指令 命令 作用 :w 将编辑的数据写入硬盘档案中(常用) :w! 若文件属性为『只读』时，强制写入该档案。不过，到底能不能写入， 还是跟你对该档案的档案权限有关啊！ :q 离开 vi (常用) :q! 若曾修改过档案，又不想储存，使用 ! 为强制离开不储存档案。注意一下啊，那个惊叹号 (!) 在 vi 当中，常常具有『强制』的意思～ :wq 储存后离开，若为 :wq! 则为强制储存后离开 (常用) ZZ 这是大写的 Z 喔！若档案没有更动，则不储存离开，若档案已经被更动过，则储存后离开！ :w [filename] 将编辑的数据储存成另一个档案（类似另存新档） :r [filename] 在编辑的数据中，读入另一个档案的数据。亦即将 『filename』 这个档案内容加到游标所在行后面 :! command 暂时离开 vi 到指令行模式下执行 command 的显示结果！例如『:! ls /home』即可在 vi 当中察看 /home 底下以 ls 输出的档案信息！ 6、vim 环境的变更 命令 作用 :set nu 显示行号，设定之后，会在每一行的前缀显示该行的行号 :set nonu 与 set nu 相反，为取消行号！ ","date":"2020-01-06T09:42:51+08:00","permalink":"https://x-xkang.com/p/vim%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","title":"Vim常用命令"},{"content":" 1、构建镜像 在项目根目录下新建 Dockerfile 文件并编辑保存\n1 2 3 4 5 6 7 8 FROM golang:latest # 依赖的镜像:镜像版本 ADD . /var/www/go-aimaster # 将当前工作目录copy到镜像的/var/www/go-aimaster 目录下 WORKDIR /var/www/go-aimaster # 设置镜像内的工作目录 RUN GOPROXY=\u0026#34;https://goproxy.cn,direct\u0026#34; go build -o main /var/www/go-aimaster/main.go # 运行命令(当前为golang 项目demo) CMD [\u0026#34;/var/www/go-aimaster/main\u0026#34;] # 可执行文件目录，上一步build生成的main可执行文件 EXPOSE 8080 # 暴露端口，最终暴露的端口不一定是当前的8080 端口 ENTRYPOINT [\u0026#34;./main\u0026#34;] # 入口文件 执行命令：docker image build -t 镜像名称[:版本号] . (注意最后有个点 .)\n1 docker image build -t go-aimamster:v0.01 . 上面代码中，-t 参数用来指定 image 文件的名字，后面还可以用冒号指定标签。如果不指定，默认的标签就是latest。最后的那个点表示 Dockerfile 文件所在的路径，上例是当前路径，所以是一个点。 执行结果如下：\n出现上图的Successfully成功标识表示已构建成功，执行 docker images 查看，列表中出现刚刚构建的go-aimaster镜像\n2、下载远端镜像 命令：docker pull 仓库名称\n1 docker pull nginx 3、推送本地镜像至远端仓库 命令： 1 2 3 docker image tag go-docker:v1.0 devxiaokang/go-docker:v1.0 docker push devxiaokang/go-docker:v1.0 4、查看镜像列表 命令：docker image ls | docker images\n1 docker images 5、删除本地镜像 命令：docker rmi 镜像标识|镜像名称:版本号\n【注意】若有容器正在依赖该镜像，则无法删除\n1 docker rmi go-aimamster:v0.01 6、生成容器 命令： docker [container] run 镜像标识 /bin/bash（简单操作）| docker [container] run -d -p 宿主机端口:容器端口 -it --name 容器名称 镜像标识 /bin/bash （常用操作）\n1 docker run nginx 或者：\n1 docker run -d -p 8080:80 -it --name nginx nginx:latest /bin/bash 以上代码中-d 代表后台运行， -p 代表宿主机端口与容器端口的映射关系，-it 代表容器的 shell 映射到当前的 shell，然后再本机窗口输入命令，就会传入容器中，--name nginx 代表定义容器名称nginx 为自定义名称，。\n执行结果如下： 7、查看容器列表 命令：docker ps -a[q]， -a 表示显示所有容器（包括已停止的），-q 列表值显示容器的唯一标识\n1 docker ps -a 1 docker ps -aq 8、进入容器 命令：docker exec -it 容器ID|容器名称 /bin/bash\n1 docker exec -it nginx /bin/bash 9、启动容器 命令：docker start 容器ID|容器名称\n1 docker start nginx 10、重启容器 命令：docker restart 容器ID|容器名称\n1 docker restart nginx 11、停止容器 命令：docker stop 容器ID|容器名称\n1 docker stop nginx 停止全部容器\n1 docker stop $(docker ps -qa) 12、删除容器 命令：docker rm 容器ID|容器名称\n删除指定容器\n1 docker rm nginx 删除全部容器\n1 docker rm $(docker ps -qa) 13、数据卷 数据卷：将宿主机的一个目录映射到容器的一个目录中，可以在宿主机中操作目录中的内容，那么容器内部映射的文件，也会跟着一起改变,创建数据卷之后，默认会存在一个目录下 /var/lib/docker/volumes/数据卷名称/_data\n创建数据卷\n1 docker volume create volume_name 查看数据卷\n1 docker volume inspect volume_name 查看全部数据卷\n1 docker volume ls 删除数据卷：docker volume rm 数据卷名称\n1 docker volume rm volume_name 14、管理多容器 .yml文件以key: value 方式来指定配置信息，多个配置信息以换行+锁紧的方式来区分，在docker-compose文件中，不要使用制表符\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # yml version: ‘3.1’ service: mysql: restart: always # 代表只要docker启动，这个容器就会跟着启动 image: daoclound.io/lib/mysql:5.7.4 # 镜像路径 container_name: mysql # 指定容器名称 ports: - 3306:3306 environment: MYSQL_ROOT_PASSWORD: 123456 TZ: Asia/Shanghai # 时区 volumes: # 数据卷 - /opt/docker_mysql/data:/var/lib/mysql tomcat: restart: always Image: daocloud.io/library/tomcat:8.5.15-jre8 # 镜像 container_name: tomcat ports: - 8080:8080 environment: TZ: Asia/Shanghai volumes: - /opt/docker_mysql_tomcat/tomcat_webapps:/usr/local/tomcat/webapps - /opt/docker_mysql_tomcat/tomcat_logs:/usr/local/tomcat/logs Docker-Compose 配置Dockerfile使用 使用docker-compose.yml文件以及Dockerfile文件在生成自定义镜像的同时启动当前镜像，并且由docker-compose去管理容器\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # yml version: ‘3.1’ services: mysql: restart: always build: context: ../ # 指定dockerfile文件所在路径 dockerfile: Dockerfile #指定dockerfile文件名称 container_name: mysql ports: - 3306:3306 environment: TZ: Asia/Shanghai 可以直接启动基于 docker-compose.yml以及Dockerfile文件构建的自定义镜像 docker-compose up -d 如果自定义镜像不存在，会帮助我们构建出自定义镜像，如果自定义镜像已存在，会直接运行这个自定义镜像，重新构建的话需执行 docker-compose build , 运行前重新构建 docker-compose up -d —build\nDocker-compose 命令：\n后台启动： docker-compose up -d 关闭并删除容器： docker-compose down 开启|关闭|重启已经存在的有docker-compose维护的容器： docker-compose start | stop | restart 查看docker-compose管理的容器： docker-compose ps 查看日志： docker-compose logs -f ","date":"2019-09-18T02:01:58+05:30","permalink":"https://x-xkang.com/p/docker-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","title":"Docker 常用命令"}]