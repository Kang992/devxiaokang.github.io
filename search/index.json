[{"content":"new 初始化 new(T)是为T类型分配一块内存，将内存清零，并返回T的指针。指针指向的内存是未初始化的(T类型的零值)。\n1 2 3 4 func main(){ a := new(int) fmt.Printf(\u0026#34;value of a: %#v\\n\u0026#34;, a) // 输出：value of a: (*int)(0xc000014660) } 参数 new(T) 中的类型T可以是Golang中的任意类型，如 int, slice, string, map, channel, struct等等\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 func main() { // int a := new(int) fmt.Printf(\u0026#34;type of a: %#v\\n\u0026#34;, reflect.TypeOf(*a).Kind().String()) // 输出： type of a: \u0026#34;int\u0026#34; // string b := new(string) fmt.Printf(\u0026#34;type of b: %#v\\n\u0026#34;, reflect.TypeOf(*b).Kind().String()) // 输出： type of b: \u0026#34;string\u0026#34; // map c := new(map[string]int) fmt.Printf(\u0026#34;type of c: %#v\\n\u0026#34;, reflect.TypeOf(*c).Kind().String()) // 输出： type of c: \u0026#34;map\u0026#34; // struct d := new(struct{ name string }) fmt.Printf(\u0026#34;type of d: %#v\\n\u0026#34;, reflect.TypeOf(*d).Kind().String()) // 输出： type of d: \u0026#34;struct\u0026#34; // 指针 e := new(*int) fmt.Printf(\u0026#34;type of e: %#v\\n\u0026#34;, reflect.TypeOf(*e).Kind().String()) // 输出： type of e: \u0026#34;ptr\u0026#34; // slice e := new([]int) fmt.Printf(\u0026#34;value of pointer e: %#v\\n\u0026#34;, *e) // 修改slice // 【注意】，slice 与 map 类似，零值是nil，需要修改时，只能通过*e=[]int{}，而不能通过下标修改 //(*e)[0] = 99 // 输出：panic: runtime error: index out of range [0] with length 0 *e = []int{1, 2, 3} fmt.Printf(\u0026#34;value of pointer e: %#v\\n\u0026#34;, *e) // 输出：value of pointer e: []int{1, 2, 3} } 返回值 new(T) 返回的是指针，指向的内存是未初始化的(T类型的零值)。即new(T)*T。如果要访问T的值，需要解引用。即*T。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 func main(){ // int a := new(int) // 访问 指针a 的值 fmt.Printf(\u0026#34;value of pointer a: %#v\\n\u0026#34;, *a) // 输出：value of pointer a: 0 // 修改 int类型的值 *a = 99 fmt.Printf(\u0026#34;value of pointer a: %#v\\n\u0026#34;, *a) // 输出：value of pointer a: 99 // -------------------- // string b := new(string) fmt.Printf(\u0026#34;value of pointer b: %#v\\n\u0026#34;, *b) // 输出： value of pointer a: 0 // 修改 string类型的值 *b = \u0026#34;hello\u0026#34; fmt.Printf(\u0026#34;value of pointer b: %#v\\n\u0026#34;, *b) // 输出：value of pointer b: \u0026#34;hello\u0026#34; // -------------------- // map c := new(map[string]int) fmt.Printf(\u0026#34;value of pointer c: %#v\\n\u0026#34;, *c) // 修改 map 类型的值， // 【注意】：new(map[T1][T2]) 返回的值`v`是 map 的零值 nil，而不是空map，如果需要修改，只能通过 `v` = map[T1][T2]{}，而不能用v[x] = y // (*c)[\u0026#34;age\u0026#34;] = 20 // 输出：panic: assignment to entry in nil map *c = map[string]int{\u0026#34;age\u0026#34;: 20} fmt.Printf(\u0026#34;value of pointer c: %#v\\n\u0026#34;, *c) // 输出：map[string]int{\u0026#34;age\u0026#34;:20} // -------------------- // struct d := new(struct{ name string }) fmt.Printf(\u0026#34;value of pointer d: %#v\\n\u0026#34;, *d) // 输出：value of pointer d: struct { name string }{name:\u0026#34;\u0026#34;} // 修改struct (*d).name = \u0026#34;zhangsan\u0026#34; fmt.Printf(\u0026#34;value of pointer d: %#v\\n\u0026#34;, *d) // 输出：value of pointer d: struct { name string }{name:\u0026#34;zhangsan\u0026#34;} } make 初始化 make(T[, args]) 为T类型分配一块内存，并初始化内存以及内置的数据结构，返回T的引用而不是指针，并且T的类型只能是slice、map或channel。\n1 2 3 4 5 6 7 8 9 10 11 12 13 func main() { a := make([]int, 0) fmt.Printf(\u0026#34;value of a: %v\\n\u0026#34;, a) // 输出：value of a: [] b := make(map[string]int) fmt.Printf(\u0026#34;value of b: %v\\n\u0026#34;, b) // 输出：value of b: map[] c := make(chan int) fmt.Printf(\u0026#34;value of c: %v\\n\u0026#34;, c) // 输出：value of c: 0xc000090360 } 参数 make(T[, args]) 中的类型T只能是 slice, map 或者 channel中的其中一个。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 func main() { // T = slice 需要指定长度，长度大于0时，切片内的元素为make([]T, n)中T的零值 a := make([]int, 0) fmt.Printf(\u0026#34;value of a: %v\\n\u0026#34;, a) // 输出：value of a: [] // T = slice aMap := make([]map[int]int, 1) fmt.Printf(\u0026#34;value of aMap: %#v\\n\u0026#34;, aMap) // 输出：value of aMap: []map[int]int{map[int]int(nil)} // T = map b := make(map[string]int) fmt.Printf(\u0026#34;value of b: %#v\\n\u0026#34;, b) // 输出：value of b: map[] // T = chan c := make(chan int) fmt.Printf(\u0026#34;value of c: %#v\\n\u0026#34;, c) // 输出：value of c: 0xc000090360 } 返回值 make(T [, args]) T 的返回值为 T 的引用，在函数内部可以直接修改原始值\nSlice，返回不再是零值，而是一个空切片，可以通过下标修改元素值 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func main() { // Slice，初始化内存和数据结构，返回长度=2的[]int类型切片的引用， //【注意】：这里的s可以通过下标直接修改元素值，如s[1]=999，但下标不可以超过`make(t [,args])`中的`args`参数，会导致溢出。 s := make([]int, 2) s[1]=999 fmt.Printf(\u0026#34;value of s is %#v\\n\u0026#34;, s) // 输出：value of s is []int{0, 999} // 修改s的值，在下一行的打印中会生效，说明 `s` 是引用类型 updateSlice(s) // 上面updateSlice内修改的值，在这里依然生效 fmt.Printf(\u0026#34;value of s is updated %#v\\n\u0026#34;, s) // 输出：value of s is updated []int{99, 999} } func updateSlice(s []int) { s[0] = 99 } Map，返回的也不是map的零值nil，而是一个空map 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 func main() { // Map m := make(map[string]string) fmt.Printf(\u0026#34;value of m-1: %#v\\n\u0026#34;, m) // 输出：value of m-1: map[string]string{} // 修改 map，这里m不再是map的零值，而是一个空map，可以直接修改 m[\u0026#34;name\u0026#34;] = \u0026#34;zhangsan\u0026#34; fmt.Printf(\u0026#34;value of m-2: %#v\\n\u0026#34;, m) // 输出：value of m-2: map[string]string{\u0026#34;name\u0026#34;:\u0026#34;zhangsan\u0026#34;} updateMap(m) fmt.Printf(\u0026#34;value of m-3: %#v\\n\u0026#34;, m) // 输出：value of m-3: map[string]string{\u0026#34;age\u0026#34;:\u0026#34;18\u0026#34;, \u0026#34;name\u0026#34;:\u0026#34;lisi\u0026#34;} } func updateMap(m map[string]string) { m[\u0026#34;name\u0026#34;] = \u0026#34;lisi\u0026#34; m[\u0026#34;age\u0026#34;] = \u0026#34;18\u0026#34; } Channel, 返回的并不是chan的零值nil，而是一个空channel，可以像channel中写入数据，但如果是通过new初始化的channel，则初始化之后的channel是零值nil,写入数据会阻塞，并不会报错 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func main() { ch := make(chan int, 0) fmt.Printf(\u0026#34;value of ch: %#v\\n\u0026#34;, ch) ch2 := new(chan int) fmt.Printf(\u0026#34;value of ch2: %#v\\n\u0026#34;, *ch2) ch3 := *ch2 go func() { for { if v, ok := \u0026lt;-ch3; ok { fmt.Printf(\u0026#34;value of ch3: %v\\n\u0026#34;, v) break } fmt.Println(\u0026#34;loop...\u0026#34;) } }() fmt.Println(\u0026#34;starting insert number to channel...\u0026#34;) // 【重点】ch3=ch时，这里正常写入数据，因为make返回的是一个空channel；但是如果ch3=*ch2时，这里会阻塞，因为new 返回的*ch2是零值nil，向值为nil的channel写入数据时是会永久阻塞的 ch3 \u0026lt;- 2 } 总结 new(T) 是为T分配内存空间并将空间初始化为T的零值，返回的是T的指针并且T可以使任意数据类型；而make(T [,args]) 分配内存空间之后会初始化底层的数据结构，返回的是T类型数据结构初始化之后的引用，而不是T的零值，并且T只能是slice、map、channel中的其中一个。\n","date":"2024-10-12T09:25:01+08:00","permalink":"https://x-xkang.com/p/golang-new-%E5%92%8C-make%E5%85%B3%E9%94%AE%E5%AD%97%E7%9A%84%E5%8C%BA%E5%88%AB/","title":"Golang new 和 make关键字的区别"},{"content":"Go语言的并发模型是CSP（Communicating Sequential Processes），提倡通过通信共享内存而不是通过共享内存而实现通信。\n虽然可以使用共享内存进行数据交换，但是共享内存在不同的goroutine中容易发生竞态问题。为了保证数据交换的正确性，必须使用互斥量对内存进行加锁，这种做法势必造成性能问题。\n如果说goroutine是Go程序并发的执行体，channel就是它们之间的连接。channel是可以让一个goroutine发送特定值到另一个goroutine的通信机制。\nGo语言中的通道（channel）是一种特殊的类型，通道像一个传送带或队列，总是遵循先进先出（First In First Out）的规则，保证收发数据的顺序，每一个通道就是一个具体类型的导管，也就是声明 channel 的时候需要为其指定元素类型。\n一、 channel 的声明方式 var ch chan T 此时的 ch 值为 nil，channel的一个特性是向值为nil的channel发送数据会阻塞\nch := new(chan T) 同上使用 var 关键字，ch的值是 nil\nch := make(chan T) ch 是一个非空的指针，此时可以向 ch 发送或者接收数据\n因此，第一种和第二种虽然可以声明，但返回值无法使用，一定会要通过 make 关键字声明的值才可用。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 func main() { // 第一种： 通过 var ch chan T 声明类型为T的channel var ch chan int fmt.Printf(\u0026#34;value of ch: %#v\\n\u0026#34;, ch) // 输出：value of ch: (chan int)(nil) // 第二种：通过new(chan T) 声明类型为T的channel ch2 := new(chan int) fmt.Printf(\u0026#34;value of ch2: %#v\\n\u0026#34;, *ch2) // 输出：value of ch2: (chan int)(nil) // 第三种：通过make(chan T) 声明类型为T的channel ch3 := make(chan int) fmt.Printf(\u0026#34;value of ch3: %#v\\n\u0026#34;, ch3) // 输出：value of ch3: (chan int)(0xc000090360) } 二、channel的类型 根据缓冲区区分 无缓冲区channel：无缓冲区channel的容量为0，即无缓冲区，发送数据时，如果接收端没有准备好，则阻塞，直到接收端准备好接收数据。 有缓冲区channel：有缓冲区channel的容量大于0，即有缓冲区，发送数据时，如果缓冲区已满，则阻塞，直到接收端接收数据后缓冲区有空间。 根据读写类型 chan 可读可写 \u0026lt;-chan 只读 chan\u0026lt;- 只写 channel 的基本操作和注意事项 channel的3种状态 nil, 未初始化的状态，只进行了声明，或手动赋值为nil active, 正常的channel，可读或者可写 closed, 已关闭的channel，【注意】已关闭的channel值不是nil，在select 中可以判断是否关闭，但无法判断是否为nil，而且已关闭的channel依然可以读，只是读出的数据是零值。只有将关闭的channel赋值为nil，才可以停止接收。 channel的三种操作 1. 读操作（接收数据） 从nil值的chan读取数据会阻塞 1 2 3 4 5 func main() { var ch chan int // 只声明，未初始化，值为nil \u0026lt;-ch // 从 nil 值的chan 读取数据会阻塞 } 从正常的channel读取数据，如果缓冲区为空，则阻塞，直到缓冲区有数据 1 2 3 4 5 6 7 8 9 10 11 12 func main() { var ch chan int // 只声明，未初始化，值为nil ch = make(chan int) // 初始化 go func() { time.Sleep(time.Second) // 休眠1秒 ch \u0026lt;- 1 // 向 ch 中发送数据 }() v := \u0026lt;-ch // 阻塞，直到上面的 goroutine 内休眠1秒后向 ch 发送数据，这里正常接收 fmt.Println(\u0026#34;received:\u0026#34;, v) // 输出：received: 1 } 读取已关闭的 channel，读取到的是零值 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func main() { var ch chan int // 只声明，未初始化，值为nil ch = make(chan int) // 初始化 go func() { //time.Sleep(time.Second) // 休眠1秒 ch \u0026lt;- 1 // 向 ch 中发送数据 close(ch) // 发送数据后立刻关闭 ch }() time.Sleep(time.Second) // 休眠1秒后再接收数据 v := \u0026lt;-ch // channel 已关闭，但还可以正常读取已发送的值：1 fmt.Println(\u0026#34;received:\u0026#34;, v) // 输出：received: 1 if v2, ok := \u0026lt;-ch; ok { // chan 已关闭，ok 返回 false，v2返回的是channel 类型的零值0 fmt.Println(\u0026#34;received:\u0026#34;, v2) } else { fmt.Println(\u0026#34;channel closed, v:\u0026#34;, v2) // 输出：channel closed, v: 0 } } 写操作（发送数据） 向nil值的chan发送数据会阻塞 1 2 3 4 5 6 7 8 9 10 func main() { var ch chan int // 只声明，未初始化，值为nil go func() { ch \u0026lt;- 1 // 向 nil 值的channel 中写入数据 fmt.Println(\u0026#34;发送数据\u0026#34;) // 阻塞，不打印数据 }() \u0026lt;-ch } 向正常的channel发送数据，如果缓冲区已满，则阻塞，直到缓冲区有空间 1 2 3 4 5 6 7 8 9 10 11 12 func main() { var ch chan int // 只声明，未初始化，值为nil ch = make(chan int, 1) go func() { ch \u0026lt;- 1 // 向正常的channel 中写入数据 fmt.Println(\u0026#34;发送数据成功\u0026#34;) // 输出：发送数据成功 }() time.Sleep(time.Second) \u0026lt;-ch } 向已关闭的channel发送数据，会panic 1 2 3 4 5 6 7 8 9 10 11 12 func main() { var ch chan int // 只声明，未初始化，值为nil ch = make(chan int, 1) go func() { close(ch) // 关闭 channel ch \u0026lt;- 1 // panic: send on closed channel fmt.Println(\u0026#34;发送数据成功\u0026#34;) }() time.Sleep(time.Second) } 关闭 关闭 nil 的channel，会导致panic 1 2 3 4 5 6 7 func main() { var ch chan int // 只声明，未初始化，值为nil close(ch) // panic: close of nil channel time.Sleep(time.Second) } 关闭正常channel，成功 1 2 3 4 5 6 7 8 9 func main() { var ch chan int // 只声明，未初始化，值为nil ch = make(chan int) close(ch) // 关闭 fmt.Println(\u0026#34;closed!!!\u0026#34;) // 输出：closed!!! time.Sleep(time.Second) } 关闭已关闭的channel，会导致panic 1 2 3 4 5 6 7 8 9 10 11 12 func main() { var ch chan int // 只声明，未初始化，值为nil ch = make(chan int) close(ch) // 关闭 fmt.Println(\u0026#34;closed!!!\u0026#34;) // 输出：closed!!! // 再次关闭， close(ch) // panic: close of closed channel time.Sleep(time.Second) } 针对 channel的三种状态和三种操作，可以组合成九种情况如下：\n操作 nil的channel 正常的channel 已关闭的channel 读 阻塞 成功或阻塞 正常(返回零值) 写 阻塞 成功或阻塞 panic 关闭 panic 成功 panic channel 如何实现线程安全？ Go中的channel是一种用于不同goroutine之间通信的原语，它可以在多个goroutine之间安全地传递数据，而不需要显式地使用锁机制（如mutex）来同步访问。Go语言的设计确保了channel在并发场景下是安全的，这使得它非常适合在多goroutine环境中用于数据传递和同步。\n锁机制 在channel的底层实现中，所有对channel的操作（包括发送、接收、关闭等）都会被加锁，以防止多个goroutine同时操作channel时出现数据竞争。Go runtime为每个channel分配了一个mutex锁来保护channel的状态，从而保证了在多goroutine并发操作时的线程安全性。\nGoroutine调度与阻塞 当一个goroutine因为channel满了（发送方）或channel空了（接收方）而被阻塞时，Go的调度器会将该goroutine挂起，放入对应的队列（sendq或recvq）。一旦条件满足（比如有接收者准备好接收数据），被阻塞的goroutine会被唤醒继续执行\n关闭channel的安全性 关闭一个channel时，所有在等待接收该channel的goroutine都会被立即唤醒，并且它们会收到零值，从而安全退出。此外，尝试向已关闭的channel发送数据会引发panic，这是Go语言的一种安全机制，避免意外的并发问题。\nchannel的底层实现总结 channel使用 锁（mutext） 来确保线程安全，防止数据竞争。 channel通过 goroutine调度和队列 实现阻塞和唤醒机制，使得多个goroutine可以安全的发送和接收数据。 无缓冲的channel是同步的，而有缓冲的channel是异步的，二者在实现机制上有所不同。 Go的调度器负责管理阻塞的goroutine，使得程序不会因为阻塞而卡死。 channel 的发送和接收数据过程实现 无缓冲区的channel 发送数据流程 加锁，发送方调用ch \u0026lt;- value，Go runtime 会对channel加锁，防止其他 goroutine 同时操作channel。 检查接收队列，Go runtime 会首先检查 revq(接收队列)是否有等待的接收方。 如果有，则从队列中取出一个接收方，并唤醒它。 如果没有，则将发送方加入到发送队列sendq中。 解锁，发送操作结束后，Go runtime 会解锁，允许其他 goroutine 操作channel。 接收数据流程 加锁，接收方调用value := \u0026lt;-ch，Go runtime 会对channel加锁，防止其他 goroutine 同时操作channel。 检查发送队列，Go runtime 会首先检查 sendq(发送队列)是否有等待的发送方。 如果有，则从队列中取出一个发送方，并唤醒它。 如果没有，则将接收方加入到接收队列recvq中。 解锁，接收操作结束后，Go runtime 会解锁，允许其他 goroutine 操作channel。 无缓冲channel操作的总结 如果发送方先到，且没有接收方，发送方阻塞并进入 sendq。 如果接收方先到，且没有发送方，接收方阻塞并进入 recvq。 如果发送方和接收方匹配成功后，Go runtime 会进行数据交换，并唤醒被阻塞的 goroutine。 有缓冲区的channel 有缓冲的 channel 不需要发送和接收操作严格同步，发送方可以在缓冲区未满时发送数据，而不阻塞。接收方可以在缓冲区中有数据时接收数据，而不等待。\n发送数据流程 加锁，发送方调用 ch \u0026lt;- value，Go runtime 加锁，防止其他 goroutine 并发操作 channel。 检查缓冲区： 如果缓冲未满，则将数据写入缓冲区，sendx(发送索引)递增。 如果缓冲已满，则阻塞发送方，并将发送方加入到发送队列sendq中。 解锁，发送操作结束后，Go runtime 解锁，允许其他 goroutine 操作 channel。 接收数据流程 加锁，接收方调用 value := \u0026lt;-ch，Go runtime 加锁，防止其他 goroutine 并发操作 channel。 检查缓冲区： 如果缓冲中有数据，则从缓冲区中取出数据，recvx(接收索引)递增。 如果缓冲为空，则阻塞接收方，并将接收方加入到接收队列recvq中。 解锁，接收操作结束后，Go runtime 解锁，允许其他 goroutine 操作 channel。 有缓冲channel操作的总结 如果缓冲未满，则发送方直接写入缓冲区。 如果缓冲区已满，发送方阻塞并进入sendq。 如果缓冲中有数据，则接收方直接从缓冲区中读取数据而不阻塞。 如果缓冲为空，接收方阻塞并进入recvq。 ","date":"2024-10-12T08:48:16+08:00","permalink":"https://x-xkang.com/p/golang%E9%9D%A2%E8%AF%95%E9%A2%98%E4%B9%8B-channel/","title":"Golang面试题之 Channel"},{"content":"概述 题目：Leetcode 703 : 取出数据流中第K大的元素 描述： 设计一个找到数据流中第 k 大元素的类（class）。注意是排序后的第 k 大元素，不是第 k 个不同的元素。请实现 KthLargest 类： KthLargest(int k, int[] nums) 使用整数 k 和整数流 nums 初始化对象。 int add(int val) 将 val 插入数据流 nums 后，返回当前数据流中第 k 大的元素。 思路 使用数组 nums []int 存储数据流，模拟堆结构，堆中只存储 k 个元素，且堆顶元素nums[0]为最小值。\n每次插入数据，如果数据流中的元素个数小于 k，直接插入堆中，如果数据流中的元素个数大于等于 k，判断插入的元素是否大于堆顶元素，如果大于堆顶元素，则将堆顶元素替换为插入的元素，然后重新调整堆，保持堆的有序性。\n插入元素时，执行上浮算法将该元素与父级元素比较和交换，直到父级元素小于等于该元素。 替换堆首元素时，执行下沉算法，将堆首元素与子级元素比较和交换，直到子级元素大于该元素。 获取第 k 大元素，直接返回 nums[0] 即可。 实现 初始化数据结构 定义结构体类型 minHeap 并创建一个堆结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // 堆结构 type minHeap struct { nums []int // 存储数据 k int // 堆中元素个数，非实际堆中元素个数 } // createMinHeap 创建一个堆 func createMinHeap(nums []int, k int) *minHeap { // 初始化 heap := \u0026amp;minHeap{nums: []int{}, k: k} for i := 0; i \u0026lt; len(nums); i++ { heap.add(nums[i]) } return heap } 在minHeap上定义add方法，用于插入元素，并保持堆的有序性。 1 2 3 4 5 6 7 8 9 10 11 12 13 func (mh *minHeap) add(num int) int { if mh.k \u0026gt; len(mh.nums) { // 堆中还有位置 mh.nums = append(mh.nums, num) // 添加在堆尾，然后开始上浮 mh.swim(len(mh.nums) - 1) // 上浮 } else { // 堆中没有位置 if num \u0026gt; mh.nums[0] { // num 大于堆顶的数字，即大于堆中的最小值，将最小值替换为num，然后下沉 mh.nums[0] = num mh.sink(0) // 下沉 } } return mh.nums[0] } 定义下沉算法sink 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // sink 下沉 func (mh *minHeap) sink(idx int) { // idx 开始下沉的位置 for 2*idx+1 \u0026lt; len(mh.nums) { // 左子节点不溢出 // 当前节点与左右两个节点中较小的一个交换 child := 2*idx + 1 if child+1 \u0026lt; len(mh.nums) \u0026amp;\u0026amp; mh.nums[child] \u0026gt; mh.nums[child+1] { child++ // 取右子节点 } if mh.nums[child] \u0026gt;= mh.nums[idx] { // 子节点大于等于当前值，下沉结束 break } // 交换 mh.nums[idx], mh.nums[child] = mh.nums[child], mh.nums[idx] // 将下标更新为当前交换的子节点 idx = child } } 定义上浮算法swim 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // swim 上浮 func (mh *minHeap) swim(idx int) { // idx 开始上浮的位置 for idx \u0026gt; 0 { // idx 上浮至0(根节点)时停止 parent := (idx - 1) \u0026gt;\u0026gt; 1 // 父节点 if mh.nums[parent] \u0026lt;= mh.nums[idx] { // 父节点小于等于当前值，上浮结束 break } // 交换 mh.nums[parent], mh.nums[idx] = mh.nums[idx], mh.nums[parent] idx = parent } } \u0026ndash; 完整代码如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 package main // 小顶堆 type minHeap struct { nums []int k int } func createMinHeap(nums []int, k int) *minHeap { // 初始化 heap := \u0026amp;minHeap{nums: []int{}, k: k} for i := 0; i \u0026lt; len(nums); i++ { heap.add(nums[i]) } return heap } // add 添加元素 func (mh *minHeap) add(num int) int { if mh.k \u0026gt; len(mh.nums) { // 堆中还有位置 mh.nums = append(mh.nums, num) // 添加在堆尾，然后开始上浮 mh.swim(len(mh.nums) - 1) // 上浮 } else { // 堆中没有位置 if num \u0026gt; mh.nums[0] { // num 大于堆顶的数字，即大于堆中的最小值，将最小值替换为num，然后下沉 mh.nums[0] = num mh.sink(0) // 下沉 } } return mh.nums[0] } // sink 下沉 func (mh *minHeap) sink(idx int) { // idx 开始下沉的位置 for 2*idx+1 \u0026lt; len(mh.nums) { // 左子节点不溢出 // 当前节点与左右两个节点中较小的一个交换 child := 2*idx + 1 if child+1 \u0026lt; len(mh.nums) \u0026amp;\u0026amp; mh.nums[child] \u0026gt; mh.nums[child+1] { child++ // 取右子节点 } if mh.nums[child] \u0026gt;= mh.nums[idx] { // 子节点大于等于当前值，下沉结束 break } // 交换 mh.nums[idx], mh.nums[child] = mh.nums[child], mh.nums[idx] // 将下标更新为当前交换的子节点 idx = child } } // swim 上浮 func (mh *minHeap) swim(idx int) { // idx 开始上浮的位置 for idx \u0026gt; 0 { // idx 上浮至0时停止 parent := (idx - 1) \u0026gt;\u0026gt; 1 // 父节点 if mh.nums[parent] \u0026lt;= mh.nums[idx] { // 父节点小于等于当前值，上浮结束 break } // 交换 mh.nums[parent], mh.nums[idx] = mh.nums[idx], mh.nums[parent] idx = parent } } type KthLargest struct { heap *minHeap } // Constructor 初始化 func Constructor(k int, nums []int) KthLargest { return KthLargest{heap: createMinHeap(nums, k)} } func (this *KthLargest) Add(val int) int { return this.heap.add(val) } func main() { k := Constructor(3, []int{4, 5, 8, 2}) } ","date":"2024-10-10T14:55:15+08:00","permalink":"https://x-xkang.com/p/topk-%E7%AE%97%E6%B3%95-%E5%B0%8F%E9%A1%B6%E5%A0%86/","title":"TopK 算法 - 小顶堆"},{"content":"HTTP Header 中常见的字段有哪些？\n字段名 描述 示例 Accept 客户端可以接受的内容类型，多个用逗号隔开 text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,/;q=0.8 Accept-Charset 能够接受的字符集 Accept-Charset: utf-8 Accept-Datetime 能够接受的按照时间表示的版本 Accept-Datetime: Thu, 31 May 2007 20:35:00 GMT Accept-Encoding 能够接受的编码方式列表。参考HTTP压缩 Accept-Encoding: gzip, deflate Accept-Language 客户端能够接受的语言列表 Accept-Language: en-US,en;q=0.8 Authorization 用于HTTP基本认证的认证信息 Authorization: Basic QWxhZGRpbjpvcGVuIHNlc2FtZQ== Cache-Control 请求缓存控制 Cache-Control: no-cache Connection 请求的连接类型，close表示关闭连接，keep-alive表示保持连接 Connection: keep-alive Content-Length 请求体长度，单位为字节 Content-Length: 348 Content-MD5 请求体的MD5值 Content-MD5: Q2hlY2sgSW50ZWdyaXR5IQ== Content-Type 请求体的类型，application/json表示JSON格式，text/html表示HTML格式 Content-Type: application/json Cookie 客户端携带的Cookie信息 Cookie: $Version=1; Skin=new Date 请求发送的时间，格式为RFC 1123 Date: Tue, 15 Nov 1994 08:12:31 GMT Expect 表明客户端要求服务器做出特定的行为 Expect: 100-continue From 发起此请求的用户的邮件地址 From: user@example.com Host 请求的服务器地址和端口号，默认为80端口 Host: www.example.com If-Match 客户端期望服务器返回的实体的ETag，如果ETag不匹配，则返回412错误 If-Match: \u0026ldquo;737060cd8c284d8af7ad3082f209582d\u0026rdquo; If-Modified-Since 客户端期望服务器返回的实体的最后修改时间，如果时间不匹配，则返回304错误 If-Modified-Since: Sat, 29 Oct 1994 19:43:31 GMT If-None-Match 允许服务器在请求的资源的 ETag 未发生变化的情况下返回 304 Not Modified 状态码 If-None-Match: \u0026ldquo;737060cd8c284d8af7ad3082f209582d\u0026rdquo; If-Range 如果该实体未被修改过，则向我发送我所缺少的那一个或多个部分；否则，发送整个新的实体 If-Range: \u0026ldquo;737060cd8c284d8af7ad3082f209582d\u0026rdquo; If-Unmodified-Since 仅当该实体自某个特定时间以来未被修改的情况下，才发送回应。 If-Unmodified-Since: Sat, 29 Oct 1994 19:43:31 GMT Max-Forwards 限制该消息可被代理及网关转发的次数。 Max-Forwards: 10 Origin 表明请求来自哪个源，浏览器会根据这个字段来决定是否允许请求。 Origin: http://www.example.com Pragma 浏览器的私有指令，表示是否缓存请求 Pragma: no-cache Proxy-Authorization 用于HTTP代理服务器的认证信息 Proxy-Authorization: Basic QWxhZGRpbjpvcGVuIHNlc2FtZQ== Range 仅请求某个实体的一部分。字节偏移以 0 开始。参见字节服务。 Range: bytes=500-999 Referer 表示浏览器所访问的前一个页面，正是那个页面上的某个链接将浏览器带到了当前所请求的这个页面。 Referer: http://www.example.com/index.html TE 浏览器预期接受的传输编码方式：可使用回应协议头 Transfer-Encoding 字段中的值 TE: trailers, deflate Upgrade 客户端期望服务器升级到指定的协议，服务器可以返回406错误 Upgrade: HTTP/2.0, SHTTP/1.3, IRC/6.9, RTA/x11 User-Agent 客户端浏览器信息，包括浏览器名称、版本、操作系统、CPU 类型等 User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36 Via 用于跟踪请求来源，服务器会通过 Via 字段将请求转发到下一个服务器，同时将 Via 字段添加到请求头中。 Via: 1.0 fred, 1.1 example.com (Apache/1.1) Warning 表明服务器的警告信息，格式为：数字 版本 代码 消息 Warning: 199 Miscellaneous warning ","date":"2024-09-18T13:38:31+08:00","permalink":"https://x-xkang.com/p/http-header-%E4%B8%AD%E5%B8%B8%E8%A7%81%E7%9A%84%E5%AD%97%E6%AE%B5%E6%9C%89%E5%93%AA%E4%BA%9B/","title":"Http Header 中常见的字段有哪些"},{"content":"具体步骤如下：\n在浏览器中输入指定网页的URL 浏览器通过DNS（UDP）协议，获取域名对应的IP地址。 浏览器根据IP地址和端口号，向目标服务器发起一个TCP连接请求。 浏览器在TCP连接上，向服务器发送一个HTTP请求报文，请求获取网页的内容。 服务器收到HTTP请求报文后，处理请求，并返回HTTP响应报文给浏览器。 浏览器收到HTTP响应报文后，解析响应体重的HTML代码，渲染网页的结构和样式，同时根据HTML中其他资源的URL（如图片、CSS、JavaScript等）请求这些资源。直到网页完全加载显示。 浏览器在不需要和服务器通信时，可以主动关闭TCP连接，或者等待服务器的关闭请求。 ","date":"2024-09-18T13:13:06+08:00","permalink":"https://x-xkang.com/p/%E5%9C%A8%E6%B5%8F%E8%A7%88%E5%99%A8%E4%B8%AD%E8%BE%93%E5%85%A5url%E5%88%B0%E5%B1%95%E7%A4%BA%E9%A1%B5%E9%9D%A2%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/","title":"在浏览器中输入URL到展示页面发生了什么"},{"content":"Socket 是什么？","date":"2024-08-19T15:01:16+08:00","permalink":"https://x-xkang.com/p/socket%E7%BC%96%E7%A8%8B%E6%A6%82%E8%BF%B0/","title":"Socket编程概述"},{"content":"","date":"2024-08-15T10:46:35+08:00","permalink":"https://x-xkang.com/p/%E5%8D%95%E6%9C%BA%E9%83%A8%E7%BD%B2kafka/","title":"单机部署Kafka"},{"content":"引言 IP 是 TCP/IP 协议族中最为核心的协议，所有的TCP、UDP、ICMP及IGMP数据都以IP数据报格式传输，并且IP向上层提供不可靠、无连接的数据传送服务，不可靠(unreliable)的意思是它不能保证IP数据报能成功的到达目的地。IP仅提供最好的传输服务。如果发生某种错误时，如某个路由器暂时用完了缓冲区，IP有一个简单的错误处理算法：丢弃该数据报，然后发送ICMP消息报给信源端。任何要求的可靠性必须由上层来提供（如TCP）。无连接（connectionless）这个术语的意思是IP并不维护任何关于后续数据报的状态信息。每个数据报的处理的相互独立的。这也说明，IP数据报可以不按发送顺序接收。如果一信源向相同的信宿发送两个连续的数据报（先是A，然后是B），每个数据报都是独立地进行路由选择，可能选择不同的路线，因此B可能在A到达之前先到达。\nIP首部 IP数据报的格式如下图所示。普通的IP首部长为 20 字节，除非含有选项字段。\n版本 占4位，表示IP协议的版本。通信双方使用的IP协议版本必须一致。目前广泛使用的IP协议版本号是 4，即 IPv4.\n首部长度 占4位，可表示的最大十进制数值是 15。这个字段所表示数的单位是 32 位字长（1个32位字长时4字节）。因此，当IP的首部长度为 1111 时（即10进制的15），首部长度就达到60字节。当IP分组的首部长度不是4字节的整数倍时，必须利用最后的填充字段加以填充。数据部分永远在4字节的整数倍开始，这样在实现IP协议时较为方便。首部长度限制为60字节的缺点是，长度有时可能不够用，之所以限制长度为60字节，是希望用户尽量减少开销。最常用的首部长度就是20字节（即首部长度为 0101）。\n区分服务（tos） 也被称为服务类型，占8位，包括一个3bit的优先权子字段（现已被忽略），4bit的TO S子字段和1bit未用位但必须置0.4bit的TOS分别代表：最小时延、最大吞吐量、最高可靠性和最小费用。4bit中只能置其中1bit。如果所有的4bit均为0，那么就意味着是一般服务。\n总长度 (totlen) 首部和数据之和，单位为字节，总长度字段为16位，因此数据报的最大长度为 2^16 - 1 = 65535 字节。 尽管可以传送一个65535字节的IP数据报，但是大多数的链路层都会对它进行切片。而且，主机也要求不能接收超过576字节的数据报。由于TCP把用户数据分成若干片，因此一般来说这个限制不会影响TCP。\n标识（identification） 用来标识数据报，占16位。IP协议在存储器中维持一个计数器。每产生一个数据报，计数器就加 1，并将此值赋值给标识字段。当数据报的长度超过网络的 MTU，而必须分片时，这个标识字段的值就被复制到所有的数据报的标识字段中。具有相同的标识字段值的分片报文会被重组成原来的数据报。\n标志（flag） 占3位，第一位未使用，其值为0，第二位称为DF（不分片），表示是否允许分片。取值为0时，表示允许分片；取值为1时，表示不允许分片。第三位称为MF（更多分片），表示是否还有分片正在传输，设置为0时，表示没有更多分片需要发送，或数据报没有分片。\n片偏移（offsetfrag） 占13位，当报文被分片后，该字段标记该分片在原报文中的相对位置。片偏移以8个字节为偏移单位。所以，除了最后一个分片，其他分片的偏移量都是8字节（64位）的整数倍。\n生存时间（TTL） 表示数据报在网络中的寿命，占8位。该字段由发出数据报的源主机设置。其目的是防止无法交付的数据报无限的在网络中传输，从而消耗网络资源。 路由器在转发数据报之前，先把TTL值减1，若TTL值减少到0，则丢弃这个数据报，不再转发。因此，TTL指明数据报在网络中最多可经过多少路由器。TTL的最大数值为255.若把TTL的初始值设置为1，则表示这个数据报只能在本局域网中传送。\n协议 表示该数据报文所携带的数据使用的协议类型，占8位。该字段可以方便目的主机的IP层知道按照什么协议来处理数据部分。不同的协议有专门不同的协议号。\n例如：TCP的协议号是6，UDP的协议号是17，ICMP的协议号是1。\n首部检验和（checksum） 用于校验数据报的首部，占16位。数据报每经过一个路由器，首部的字段都可能发生变化（如TTL），所以需要重新校验。而数据部分不发生变化，所以不用重新生成校验值。\n源地址 表示数据报的源IP地址，占32位\n目的地址 表示数据报的目的IP地址，占32位。该字段用于校验发送是否正确。\n可选字段 该字段用于一些可选的报头设置，主要用于测试、调试和安全的目的。这些选项包括严格源路由（数据报必须经过指定的路由）、网际时间戳（经过每个路由器时的时间戳记录）和安全限制\n填充 由于可选字段中的长度不是固定的，使用若干个0填充该字段，可以保证整个报头的长度是32位的整数倍。\n数据部分 表示传输层的数据，如保存TCP，UDP，ICMP或IGMP的数据，数据部分的长度不固定。\nIP路由选择 从概念上说，IP路由选择是简单的，特别对于主机来说。如果目的主机与源主机直接相连（如点对点链路）或都在一个共享网络上（以太网或令牌环网），那么IP数据报就直接送到目的主机上。否则，主机把数据报发往一默认的路由器上，由路由器来转发该数据报。大多数的主机都是采用这种简单机制。\nIP层既可以配置成路由器的功能，也可以配置成主机的功能。当今的大多数多用户系统，包括几乎所有的Unix系统，都可以配置成一个路由器。我们可以为它指定主机和路由器都可以使用的简单路由算法。本质上的区别在于主机从不把数据报从一个接口转发到另一个接口，而路由器则要转发数据报。内含路由器功能的主机应该从不转发数据报，除非它被设置成那样。\n在一般的体制中，IP可以从TCP、UDPICMP和IGMP接收数据报（即在本地生成的数据报）并进行发送，或者从一个网络接口接收数据报（待转发的数据报）并进行发送。IP层在内存中有一个路由表。当收到一份数据报并进行发送时，它都要对该表搜索一次。当数据报来自某个网络接口时，IP首先检查目的IP地址是否为本机的IP地址之一或者IP广播地址。如果确实是这样，数据报就被送到由IP首部协议字段所指定的协议模块进行处理。如果数据包的目的地不是这些地址，那么：\n如果IP层被设置为路由器的功能，那么就对数据报进行转发（也就是说，像下面对待发出的的数据报一样处理）； 数据报被丢弃。 路由表中的每一项都包含下面这些信息：\n目的IP地址，它既可以是一个完整的主机地址，也可以是一个网络地址，由该表目中的标志字段来指定。主机地址有一个非0的主机号，以指定某一特定的主机，而网络地址中的主机号为0，以指定网络中的所有主机（如以太网，令牌环网）。 下一跳路由器（next-hop router）的IP地址，或者有直接连接的网络IP地址。下一跳路由器是指一个在直接相连网络上的路由器，通过它可以转发数据报。下一跳路由器不是最终的目的地，但是它可以把传送给它的数据报转发到最终目的 。 标志，其中一个标志指明目的IP地址是网络地址还是主机地址，另一个标志指明下一跳路由器是否真正的下一跳路由器，还是一个直接相连的接口。 为数据的传输制定一个网络接口。 IP路由选择是逐跳地（hop-by-hop）进行的。从这个路由表信息可以看出，IP并不知道任何目的的完整路径（当然，除了那些与主机直接相连的目的）。所有的IP路由选择只为数据包传输提供下一站路由器的IP地址。它假定下一跳路由器比发送数据包的主机更接近目的，而且下一跳路由器与该主机是直接相连的。\nIP路由选择主要完成一下这些功能：\n搜索路由表，寻找能与目的IP地址完全匹配的表目（网络号和主机号都要匹配）。如果找到，则把报文发送给该表目指定的下一跳路由器或者直接连接的网络接口（取决于标志字段的值）。 搜索路由表，寻找能与目的网络号相匹配的表目，如果找到，则把报文发送给该表目指定的下一站路由器或直接连接的网络接口（取决于标志字段的值）。目的网络上的所有主机都可以通过这个表目来处置。例如，一个以太网上的所有主机都是通过这种表目进行寻径的。这种搜索网络的匹配方法必须考虑可能的子网掩码。关于这一点我们在下一节中进行讨论。 搜索路由表，寻找标为“默认（default）”的表目。如果找到，则把报文发送给该表目指定的下一站路由器。如果上面这些步骤都没有成功，那么该数据报就不能被传送。 如果不能传送的数据报来自本机，那么一般会向生成数据报的应用程序返回一个“主机不可达”或“网络不可达”的错误。\n","date":"2024-07-22T09:48:47+08:00","permalink":"https://x-xkang.com/p/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C--ip-%E5%8D%8F%E8%AE%AE/","title":"计算机网络 -- IP 协议"},{"content":"","date":"2024-07-19T15:42:30+08:00","permalink":"https://x-xkang.com/p/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B--%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%92%8C%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B/","title":"计算机网络之 -- 三次握手和四次挥手"},{"content":"分布式事务是在分布式系统中保证多个独立的资源（如数据库、消息队列等）上的操作作为一个整体成功或失败的机制。为了确保这些操作的原子性、一致性、隔离性和持久性（即ACID特性），分布式事务管理器使用各种协议和技术。以下是分布式事务的主要原理和关键概念的详解。\n分布式事务的关键概念 全局事务和局部事务：\n全局事务（Global Transaction）：跨越多个独立资源的事务，由分布式事务管理器（Transaction Coordinator, TC）管理。 局部事务（Local Transaction）：在单个资源管理器（Resource Manager, RM）上执行的事务。 事务管理器（Transaction Coordinator, TC）：负责管理全局事务的协调、提交和回滚。\n资源管理器（Resource Manager, RM）：管理单个资源的事务操作，如数据库的事务管理器。\n参与者（Participant）：参与全局事务的各个资源管理器。\n分布式事务的主要协议 两阶段提交协议（2PC, Two-Phase Commit） 两阶段提交是最常用的分布式事务协议，由准备阶段和提交阶段组成。\n准备阶段（Prepare Phase）：\n事务管理器向所有参与者发送准备请求（prepare request）。 参与者在接收到准备请求后，执行本地事务预处理，并将预处理结果记录到日志中，然后返回准备好的确认或失败信息给事务管理器。 提交阶段（Commit Phase）：\n如果所有参与者都返回准备好，事务管理器向所有参与者发送提交请求（commit request）。 参与者在接收到提交请求后，正式提交事务，并将提交结果记录到日志中，然后返回提交确认。 如果有任何一个参与者返回失败，事务管理器向所有参与者发送回滚请求（rollback request），所有参与者回滚事务，并将回滚结果记录到日志中。 三阶段提交协议（3PC, Three-Phase Commit） 三阶段提交是对两阶段提交的改进，通过增加一个预提交阶段来减少阻塞的可能性和增强容错能力。\n准备阶段（Prepare Phase）：\n事务管理器向所有参与者发送准备请求（prepare request）。 参与者在接收到准备请求后，执行本地事务预处理，并将预处理结果记录到日志中，然后返回准备好的确认或失败信息给事务管理器。 预提交阶段（Pre-commit Phase）：\n如果所有参与者都返回准备好，事务管理器向所有参与者发送预提交请求（pre-commit request）。 参与者在接收到预提交请求后，记录预提交状态，并锁定事务，然后返回预提交确认。 提交阶段（Commit Phase）：\n如果所有参与者都返回预提交确认，事务管理器向所有参与者发送提交请求（commit request）。 参与者在接收到提交请求后，正式提交事务，并将提交结果记录到日志中，然后返回提交确认。 如果在任何阶段有一个参与者返回失败，事务管理器会发送回滚请求，所有参与者回滚事务并记录在日志中。\n分布式事务的实现原理 日志记录：\n在每个阶段，参与者都需要将当前状态记录在日志中，以便在系统故障或重启时能够恢复正确的状态。 超时机制：\n参与者和事务管理器都需要设置超时机制，以防止在等待对方响应时陷入无限等待的状态。 错误处理：\n如果任何阶段出现错误，事务管理器会发出回滚请求，确保所有参与者都回滚已执行的操作。 其他分布式事务协议 TCC（Try-Confirm-Cancel）：\nTry：尝试执行业务操作，预留资源。 Confirm：确认业务操作，真正执行业务。 Cancel：取消业务操作，释放预留资源。 Saga：\n将全局事务拆分为一系列有序的子事务，每个子事务都有对应的补偿事务。 如果某个子事务失败，则按顺序执行已成功子事务的补偿操作。 分布式事务的挑战 网络分区：网络故障可能导致部分节点无法通信，需要设计容错机制。 性能开销：分布式事务需要多次通信和日志记录，增加了系统的延迟和性能开销。 一致性与可用性：在分布式系统中，通常需要在一致性和可用性之间进行权衡（CAP定理）。 通过使用上述协议和技术，分布式事务可以在保证系统一致性的前提下，实现跨多个独立资源的事务操作。然而，实际应用中需要根据具体业务场景和性能需求，选择适合的分布式事务管理方案。\n","date":"2024-07-17T09:20:58+08:00","permalink":"https://x-xkang.com/p/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/","title":"分布式事务"},{"content":"","date":"2024-05-09T14:26:30+08:00","permalink":"https://x-xkang.com/p/rust-%E5%AD%A6%E4%B9%A0%E5%85%AD--%E5%BC%95%E7%94%A8%E4%B8%8E%E5%80%9F%E7%94%A8/","title":"Rust 学习（六）-- 引用与借用"},{"content":"什么是所有权 Rust 的核心功能（之一）是 所有权（ownership）。虽然该功能很容易解释，但它对语言的其他部分有着深刻的影响。\n所有程序都必须管理其运行时使用计算机内存的方式。一些语言中具有垃圾回收机制，在程序运行时有规律地寻找不再使用的内存；在另一些语言中，程序员必须亲自分配和释放内存。Rust 则选择了第三种方式：通过所有权系统管理内存，编译器在编译时会根据一系列的规则进行检查。如果违反了任何这些规则，程序都不能编译。在运行时，所有权系统的任何功能都不会减慢程序。\n因为所有权对很多程序员来说都是一个新概念，需要一些时间来适应。好消息是随着你对 Rust 和所有权系统的规则越来越有经验，你就越能自然地编写出安全和高效的代码。持之以恒！\n当你理解了所有权，你将有一个坚实的基础来理解那些使 Rust 独特的功能。在本章中，你将通过完成一些示例来学习所有权，这些示例基于一个常用的数据结构：字符串。\n栈（Stack）与堆（Heap）\n在很多语言中，你并不需要经常考虑到栈与堆。不过在像 Rust 这样的系统编程语言中，值是位于栈上还是堆上在更大程度上影响了语言的行为以及为何必须做出这样的抉择。我们会在本章的稍后部分描述所有权与栈和堆相关的内容，所以这里只是一个用来预热的简要解释。\n栈和堆都是代码在运行时可供使用的内存，但是它们的结构不同。栈以放入值的顺序存储值并以相反顺序取出值。这也被称作 后进先出（last in, first out）。想象一下一叠盘子：当增加更多盘子时，把它们放在盘子堆的顶部，当需要盘子时，也从顶部拿走。不能从中间也不能从底部增加或拿走盘子！增加数据叫做 进栈（pushing onto the stack），而移出数据叫做 出栈（popping off the stack）。栈中的所有数据都必须占用已知且固定的大小。在编译时大小未知或大小可能变化的数据，要改为存储在堆上。 堆是缺乏组织的：当向堆放入数据时，你要请求一定大小的空间。内存分配器（memory allocator）在堆的某处找到一块足够大的空位，把它标记为已使用，并返回一个表示该位置地址的 指针（pointer）。这个过程称作 在堆上分配内存（allocating on the heap），有时简称为 “分配”（allocating）。（将数据推入栈中并不被认为是分配）。因为指向放入堆中数据的指针是已知的并且大小是固定的，你可以将该指针存储在栈上，不过当需要实际数据时，必须访问指针。想象一下去餐馆就座吃饭。当进入时，你说明有几个人，餐馆员工会找到一个够大的空桌子并领你们过去。如果有人来迟了，他们也可以通过询问来找到你们坐在哪。\n入栈比在堆上分配内存要快，因为（入栈时）分配器无需为存储新数据去搜索内存空间；其位置总是在栈顶。相比之下，在堆上分配内存则需要更多的工作，这是因为分配器必须首先找到一块足够存放数据的内存空间，并接着做一些记录为下一次分配做准备。\n访问堆上的数据比访问栈上的数据慢，因为必须通过指针来访问。现代处理器在内存中跳转越少就越快（缓存）。继续类比，假设有一个服务员在餐厅里处理多个桌子的点菜。在一个桌子报完所有菜后再移动到下一个桌子是最有效率的。从桌子 A 听一个菜，接着桌子 B 听一个菜，然后再桌子 A，然后再桌子 B 这样的流程会更加缓慢。出于同样原因，处理器在处理的数据彼此较近的时候（比如在栈上）比较远的时候（比如可能在堆上）能更好的工作。\n当你的代码调用一个函数时，传递给函数的值（包括可能指向堆上数据的指针）和函数的局部变量被压入栈中。当函数结束时，这些值被移出栈。\n跟踪哪部分代码正在使用堆上的哪些数据，最大限度的减少堆上的重复数据的数量，以及清理堆上不再使用的数据确保不会耗尽空间，这些问题正是所有权系统要处理的。一旦理解了所有权，你就不需要经常考虑栈和堆了，不过明白了所有权的主要目的就是管理堆数据，能够帮助解释为什么所有权要以这种方式工作。\n所有权规则 首先，让我们看一下所有权的规则。当我们通过举例说明时，请谨记这些规则：\nRust 中的每一个值都有一个 所有者（owner）。 值在任一时刻有且只有一个所有者。 当所有者（变量）离开作用域，这个值将被丢弃。 变量作用域 既然我们已经掌握了基本语法，将不会在之后的例子中包含 fn main() { 代码，所以如果你是一路跟过来的，必须手动将之后例子的代码放入一个 main 函数中。这样，例子将显得更加简明，使我们可以关注实际细节而不是样板代码。 在所有权的第一个例子中，我们看看一些变量的 作用域（scope）。作用域是一个项（item）在程序中有效的范围。假设有这样一个变量：\n1 2 3 4 #![allow(unused)] fn main() { let s = \u0026#34;hello\u0026#34;; } 变量 s 绑定到了一个字符串字面值，这个字符串值是硬编码进程序代码中的。这个变量从声明的点开始直到当前 作用域 结束时都是有效的。示例 4-1 中的注释标明了变量 s 在何处是有效的。\n1 2 3 4 5 6 7 fn main() { { // s 在这里无效，它尚未声明 let s = \u0026#34;hello\u0026#34;; // 从此处起，s 是有效的 // 使用 s } // 此作用域已结束，s 不再有效 } 换句话说，这里有两个重要的时间点：\n当 s 进入作用域时，它就是有效的。 这一直及持续到它离开作用域为止。 目前为止，变量是否有效与作用域的关系跟其他编程语言是类似的。现在我们在此基础上介绍 String 类型。\nString 类型 为了掩饰所有权的规则，我们需要一个比“数据类型”章节中讲到的都要复杂的数据类型。前面介绍的类型都是已知大小的，可以存储在栈中，并且当离开作用域时被移出栈，如果代码的另一部分需要在不同的作用域中使用相同的值，可以快速简单地复制它们来创建一个新的独立实例。不过我们需要寻找一个存储在堆上的数据来探索 Rust 是如何知道该在合适清理数据的。\n我们会专注于 String 与所有权相关的部分。这些方面也同样适用于标准库提供的或你自己创建的其他复杂数据类型。\n我们已经见过字符串字面值，即被硬编码进程序里的字符串值。字符串字面值是很方便的，不过它们并不适合使用文本的每一种场景。原因之一就是它们是不可变的。另一个原因是并非所有的字符串的值都能在编写代码时就知道，例如：要是想获取用户输入并存储该怎么办呢？为此，Rust有另一种字符串类型：String。这个类型管理被分配到堆上的数据，所以能够存储在编译时未知大小的文本。可以使用from函数基于字符串字面值来创建 String\n1 2 3 fn main(){ let s = String::from(\u0026#34;hello\u0026#34;); } 这两个冒号 :: 是运算符，允许将特定的 from 函数置于 String 类型的命名空间（namespace）下，而不需要使用类似 string_from 这样的名字。\n可以修改此类字符串\n1 2 3 4 5 fn main(){ let mut s = String::from(\u0026#34;hello\u0026#34;); s.push_str(\u0026#34;, world\u0026#34;); // push_str() 在字符串后面追加字面值 println!(\u0026#34;{}\u0026#34;, s); // 将打印 `hello world` } 那么这里有什么区别呢？为什么 String 可变而字面值却不行呢？区别在于两个类型对内存的处理上。\n内存与分配 就字符串字面值来说，我们在编译时就知道其内容，所以文本被直接硬编码进最终的可执行文件中。这使得字符串字面值快速且高效。不过这些特性都只得益于字符串字面值的不可变性。不幸的是，我们不能为了每一个在编译时大小未知的文本而将一块内存放入二进制文件中，并且它的大小还可能随着程序运行而改变。\n对于 String 类型，为了支持一个可变，可增长的文本片段，需要在堆上分配一块在编译时未知大小的内存来存放内容。这意味着：\n必须在运行时向内存分配器（memory allocator）请求内存。 需要一个当我们处理完 String 时将内存返回给分配器的方法。 第一部分由我们完成：当调用String::from时，它的实现（implementation）请求其所需的内存，这在编程语言中是非常通用的。\n然而，第二部分实现起来就各有区别了。在有垃圾回收（garbage collector, GC）的语言中，GC记录并清除不再使用的内存，而我们并不需要关心它。在大部分没有GC的语言中，识别出不再使用的内存并调用代码显示的释放就是我们的责任了，跟请求内存的时候一样，从历史的角度上说正确处理内存回收曾经是一个困难的编程问题。如果忘记回收了，会浪费内存。如果过早回收了，将会出现无效变量。如果重复回收，这也是个bug。我们需要精确的为一个allocate配对一个free。\nRust 采取了一个不同的策略：内存在拥有它的变量离开作用域后就被自动释放。下面是作用域例子的一个使用 String 而不是字符串字面值的版本：\n1 2 3 4 5 6 7 8 fn main() { { let s = String::from(\u0026#34;hello\u0026#34;); // 从此处起，s 是有效的 // 使用 s } // 此作用域已结束， // s 不再有效 } 这是一个将 String 需要的内存返回给分配器的很自然的位置：当 s 离开作用域的时候。当变量离开作用域，Rust 为我们调用一个特殊的函数。这个函数叫做 drop，在这里 String 的作者可以放置释放内存的代码。Rust 在结尾的 } 处自动调用 drop。\n注意：在 C++ 中，这种 item 在生命周期结束时释放资源的模式有时被称作 资源获取即初始化（Resource Acquisition Is Initialization (RAII)）。如果你使用过 RAII 模式的话应该对 Rust 的 drop 函数并不陌生。\n这个模式对编写 Rust 代码的方式有着深远的影响。现在它看起来很简单，不过在更复杂的场景下代码的行为可能是不可预测的，比如当有多个变量使用在堆上分配的内存时。现在让我们探索一些这样的场景。\n变量与数据交互的方式（一）：移动 在 Rust 中，多个变量可以采取不同的方式与同一数据进行交互。让我们看看示例中一个使用整型的例子。\n1 2 3 4 fn main() { let x = 5; let y = x; } 我们大致可以猜到这在干什么：“将 5 绑定到 x；接着生成一个值 x 的拷贝并绑定到 y”。现在有了两个变量，x 和 y，都等于 5。这也正是事实上发生了的，因为整数是有已知固定大小的简单值，所以这两个 5 被放入了栈中。\n现在看看这个 String 版本：\n1 2 3 4 fn main() { let s1 = String::from(\u0026#34;hello\u0026#34;); let s2 = s1; } 这看起来与上面的代码非常类似，所以我们可能会假设它们的运行方式也是类似的：也就是说，第二行可能会生成一个 s1 的拷贝并绑定到 s2 上。不过，事实上并不完全是这样。\nString 由三部分组成，如图左侧所示：一个指向存放字符串内容内存的指针，一个长度，和一个容量。这一组数据存储在栈上。右侧则是堆上存放内容的内存部分。\n长度表示 String 的内容当前使用了多少字节的内存。容量是 String 从分配器总共获取了多少字节的内存。长度与容量的区别是很重要的，不过在当前上下文中并不重要，所以现在可以忽略容量.\n当我们将 s1 赋值给 s2，String 的数据被复制了，这意味着我们从栈上拷贝了它的指针、长度和容量。我们并没有复制指针指向的堆上数据。换句话说，内存中数据的表现如下图所示。\n如果 Rust 也拷贝了堆上的数据，那么内存看起来就是这样的。如果 Rust 这么做了，那么操作 s2 = s1 在堆上数据比较大的时候会对运行时性能造成非常大的影响。\n之前我们提到过当变量离开作用域后，Rust 自动调用 drop 函数并清理变量的堆内存。不过图 4-2 展示了两个数据指针指向了同一位置。这就有了一个问题：当 s2 和 s1 离开作用域，它们都会尝试释放相同的内存。这是一个叫做 二次释放（double free）的错误，也是之前提到过的内存安全性 bug 之一。两次释放（相同）内存会导致内存污染，它可能会导致潜在的安全漏洞。\n为了确保内存安全，在 let s2 = s1; 之后，Rust 认为 s1 不再有效，因此 Rust 不需要在 s1 离开作用域后清理任何东西。看看在 s2 被创建之后尝试使用 s1 会发生什么；这段代码不能运行：\n1 2 3 4 5 6 fn main() { let s1 = String::from(\u0026#34;hello\u0026#34;); let s2 = s1; println!(\u0026#34;{}, world!\u0026#34;, s1); } 如果你在其他语言中听说过术语 浅拷贝（shallow copy）和 深拷贝（deep copy），那么拷贝指针、长度和容量而不拷贝数据可能听起来像浅拷贝。不过因为 Rust 同时使第一个变量无效了，这个操作被称为 移动（move），而不是叫做浅拷贝。上面的例子可以解读为 s1 被 移动 到了 s2 中。那么具体发生了什么。\n这样就解决了我们的问题！因为只有 s2 是有效的，当其离开作用域，它就释放自己的内存，完毕。\n另外，这里还隐含了一个设计选择：Rust 永远也不会自动创建数据的 “深拷贝”。因此，任何 自动 的复制都可以被认为是对运行时性能影响较小的。\n变量与数据交互的方式（二）：克隆 如果我们 确实 需要深度复制 String 中堆上的数据，而不仅仅是栈上的数据，可以使用一个叫做 clone 的通用函数。第五章会讨论方法语法，不过因为方法在很多语言中是一个常见功能，所以之前你可能已经见过了。\n这是一个实际使用 clone 方法的例子：\n1 2 3 4 5 6 fn main() { let s1 = String::from(\u0026#34;hello\u0026#34;); let s2 = s1.clone(); println!(\u0026#34;s1 = {}, s2 = {}\u0026#34;, s1, s2); } 当出现 clone 调用时，你知道一些特定的代码被执行而且这些代码可能相当消耗资源。你很容易察觉到一些不寻常的事情正在发生。\n只在栈上的数据：拷贝 这里还有一个没有提到的小窍门。这些代码使用了整型并且是有效的\n1 2 3 4 5 6 fn main() { let x = 5; let y = x; println!(\u0026#34;x = {}, y = {}\u0026#34;, x, y); } 但这段代码似乎与我们刚刚学到的内容相矛盾：没有调用 clone，不过 x 依然有效且没有被移动到 y 中。\n原因是像整型这样的在编译时已知大小的类型被整个存储在栈上，所以拷贝其实际的值是快速的。这意味着没有理由在创建变量 y 后使 x 无效。换句话说，这里没有深浅拷贝的区别，所以这里调用 clone 并不会与通常的浅拷贝有什么不同，我们可以不用管它。\nRust 有一个叫做 Copy trait 的特殊注解，可以用在类似整型这样的存储在栈上的类型上。如果一个类型实现了 Copy trait，那么一个旧的变量在将其赋值给其他变量后仍然可用。\nRust 不允许自身或其任何部分实现了 Drop trait 的类型使用 Copy trait。如果我们对其值离开作用域时需要特殊处理的类型使用 Copy 注解，将会出现一个编译时错误。要学习如何为你的类型添加 Copy 注解以实现该 trait，请阅读附录 C 中的 “可派生的 trait”。\n那么哪些类型实现了 Copy trait 呢？你可以查看给定类型的文档来确认，不过作为一个通用的规则，任何一组简单标量值的组合都可以实现 Copy，任何不需要分配内存或某种形式资源的类型都可以实现 Copy 。如下是一些 Copy 的类型：\n所有整数类型，比如 u32。 布尔类型，bool,它的值是true和false。 所有浮点类型，比如 f64。 字符类型，char，比如 'a'。 元组， 当且仅当其包含的类型也都实现了 Copy的时候。例如，(i32, i32) 实现了 Copy，但是 (i32, String) 没有实现。 所有权与函数 将值传递给函数与给变量赋值的原理相似。向函数传递值可能会移动或者复制，就像赋值语句一样\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 fn main() { let s = String::from(\u0026#34;hello\u0026#34;); // s 进入作用域 takes_ownership(s); // s 的值移动到函数里 ... // ... 所以到这里不再有效 let x = 5; // x 进入作用域 makes_copy(x); // x 应该移动函数里， // 但 i32 是 Copy 的， // 所以在后面可继续使用 x } // 这里，x 先移出了作用域，然后是 s。但因为 s 的值已被移走， // 没有特殊之处 fn takes_ownership(some_string: String) { // some_string 进入作用域 println!(\u0026#34;{}\u0026#34;, some_string); } // 这里，some_string 移出作用域并调用 `drop` 方法。 // 占用的内存被释放 fn makes_copy(some_integer: i32) { // some_integer 进入作用域 println!(\u0026#34;{}\u0026#34;, some_integer); } // 这里，some_integer 移出作用域。没有特殊之处 当尝试在调用 takes_ownership 后使用 s 时，Rust 会抛出一个编译时错误。这些静态检查使我们免于犯错。试试在 main 函数中添加使用 s 和 x 的代码来看看哪里能使用它们，以及所有权规则会在哪里阻止我们这么做。\n返回值和作用域 返回值也可以转移所有权。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 fn main() { let s1 = gives_ownership(); // gives_ownership 将返回值 // 转移给 s1 let s2 = String::from(\u0026#34;hello\u0026#34;); // s2 进入作用域 let s3 = takes_and_gives_back(s2); // s2 被移动到 // takes_and_gives_back 中， // 它也将返回值移给 s3 } // 这里，s3 移出作用域并被丢弃。s2 也移出作用域，但已被移走， // 所以什么也不会发生。s1 离开作用域并被丢弃 fn gives_ownership() -\u0026gt; String { // gives_ownership 会将 // 返回值移动给 // 调用它的函数 let some_string = String::from(\u0026#34;yours\u0026#34;); // some_string 进入作用域。 some_string // 返回 some_string // 并移出给调用的函数 // } // takes_and_gives_back 将传入字符串并返回该值 fn takes_and_gives_back(a_string: String) -\u0026gt; String { // a_string 进入作用域 // a_string // 返回 a_string 并移出给调用的函数 } 变量的所有权总是遵循相同的模式：将值赋给另一个变量时移动它。当持有堆中数据值的变量离开作用域时，其值将通过 drop 被清理掉，除非数据被移动为另一个变量所有。\n虽然这样是可以的，但是在每一个函数中都获取所有权并接着返回所有权有些啰嗦。如果我们想要函数使用一个值但不获取所有权该怎么办呢？如果我们还要接着使用它的话，每次都传进去再返回来就有点烦人了，除此之外，我们也可能想返回函数体中产生的一些数据。\n我们可以使用元组来返回多个值:\n1 2 3 4 5 6 7 8 9 10 11 12 13 fn main() { let s1 = String::from(\u0026#34;hello\u0026#34;); let (s2, len) = calculate_length(s1); println!(\u0026#34;The length of \u0026#39;{}\u0026#39; is {}.\u0026#34;, s2, len); } fn calculate_length(s: String) -\u0026gt; (String, usize) { let length = s.len(); // len() 返回字符串的长度 (s, length) } 但是这未免有些形式主义，而且这种场景应该很常见。幸运的是，Rust 对此提供了一个不用获取所有权就可以使用值的功能，叫做 引用（references）。\n每日一算 题目： 比特位计数\n来源： leetcode 338\n难度：简单\n描述： 给你一个整数 n ，对于 0 \u0026lt;= i \u0026lt;= n 中的每个 i ，计算其二进制表示中 1 的个数 ，返回一个长度为 n + 1 的数组 ans 作为答案。\n解题：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 impl Solution { pub fn count_bits( n: i32) -\u0026gt; Vec\u0026lt;i32\u0026gt; { let mut vec = Vec::new(); for i in 0..n + 1 { vec.push(Solution::ones_count(i)); } vec } pub fn ones_count( mut n: i32) -\u0026gt; i32 { let mut ones = 0; while n \u0026gt; 0 { ones += 1; n \u0026amp;= (n - 1); } ones } } ","date":"2024-04-23T17:00:23+08:00","permalink":"https://x-xkang.com/p/rust%E5%AD%A6%E4%B9%A0%E4%BA%94--%E8%AE%A4%E8%AF%86%E6%89%80%E6%9C%89%E6%9D%83/","title":"Rust学习（五）-- 认识所有权"},{"content":"Map 的数据结构 Golang中的 map是一个 hmap类型的指针\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 // 哈希表 type hmap struct { count int // 哈希表中的 k-v数量 flags uint8 // 迭代 map 或者对 map进行写操作的时候，会记录标志位，用于一些并发场景的检测校验 B uint8 // 创建的 buckets 桶的数量为 2^B 个 noverflow uint16 // 溢出的buckets数量 hash0 uint32 // hash seed，为hash函数的结果引入随机性 buckets unsafe.Pointer // bucket数组指针，指向一个 []bmap 类型的数组，数组大小为 2^B， 我们将一个bmap 叫做一个桶， bucket 字段我们称之为正常桶，正常桶存满8个元素后，正常桶指向的下一个桶，我们将其称为溢出桶（拉链法） oldbuckets unsafe.Pointer // 当map扩容的时候，指向旧的 buckets 数组。 nevacuate uintptr // 计数器，表示扩容时的迁移进度 extra *mapextra // 用于GC，指向所有的溢出桶，正常桶里面的某个 bmap 存满了，会使用者里面的 } // 溢出桶结构 type mapextra struct { // 如果 key 和 value 都不包含指针，并且可以被 inline (\u0026lt;= 128 字节) // 就是用 hmap 的 extra 字段来存储 overflow buckets，这样可以避免 GC 扫描整个 map。 // 然而 bmap.overflow 也是个指针，这时候我们只能把这些 overflow 的指针 // 都放在 hmap.extra.overflow 和 hmap.extra.oldoverflow 字段中。 // overflow 包含的是 hmap.buckets 的 overflow 的 buckers // oldoverflow 包含扩容时的 hmap.oldbuckets 的 overflow的 buckets // overflow *[]*bmap // 指针数组，指向所有的溢出桶 oldoverflow *[]*bmap // 指针数组，发生扩容时，指向所有旧的溢出桶 // nextOverflow holds a pointer to a free overflow bucket. nextOverflow *bmap // 指向所有溢出桶中，下一个可以使用的溢出桶 } // 正常 桶结构 type bmap struct { tophash [bucketCnt]uint8\t// 存储key hash 值的高8位，用于决定 kv 键值对存放在桶内的哪个位置 // 以下属性，编译时动态生成 keys [bucketCnt]keytype\t// 存放 key 的数组 values [bucketCnt]valuetype // 存放 value 的数组 pad uintptr // 用于对齐内存 overflow uintptr // 指向下一个桶，即溢出桶 } map的内存模型中，最重要的三种结构： hmap, bmap, mapextra hmap 表示整个map，bmap 表示hmap中的一个桶，map底层其实是由很多歌桶组成的 当一个桶存满之后，指向的下一个桶，就叫做溢出桶，溢出桶是拉链法的具体体现 mapextra 表示所有的溢出桶，之所以还需要重新的指向，目的是为了用于GC，避免GC时扫描整个map，仅扫描所有溢出桶就够了 桶结构的很多字段得在编译时才会动态生成，比如 key 和 values等 桶结构中，之所以所有的 key 放一起，所有的 value 放一起，而不是 key/value 一对对的一起存放，目的便是在某些情况下可以省去 pad 字段，节省内存空间 golang 中的 map 使用的内存是不会收缩的，只会越用越多 map 中比较重要的3个数据结构： hmap, mapextra, bmap\nhmap 为 map 的主要结构，hmap.buckets 指向了一个桶数组，数组中的每一个元素就是一个 bmap 结构 mapextra 表示所有的溢出桶，之所以还要重新的指向，目的是为了用于GC，避免GC时扫描整个map，仅扫描所有溢出桶就够了 bmap 为bucket 桶结构，存储 key/value 值，v1.17之前是 k/v/k/v\u0026hellip;交替存储 Map的设计原理 hash值的使用 通过hash函数，key 可以得到一个唯一值，map将这个唯一值，分成高8位和低8位，分别有不同的用途\n低8位：用于寻找当前key 属于哪个 bucket 高8位： 用于寻找当前 key 在bucket中的位置，bucket 有 tophash 字段，便是存储的高8位的值，用来声明当前bucket中有哪些key，这样搜索查找时就不用遍历bucket中的每个key，只要先看看 tophash 数组值即可，提高搜索查找效率 map 其使用的hash算法会根据硬件选择，比如如果cpu是否支持 aes，那么采用 aes hash，并且将hash值映射到bucket时，会采用位运算来规避mod的开销\n桶的细节设计 bmap结构，即桶，是map中最重要的底层实现之一，其设计要点如下：\n桶是map中最小的挂载粒度： map中不是每个key都申请一个结构通过链表串联，而是每8个kv键值对存放在一个bucket中，然后桶再以链表的形式串联起来，这样做的原因就是减少对象的数量，减轻GC的负担。 桶串联实现拉链法：当某个桶数量满了，会申请一个新桶，挂在这个桶后面形成链表，新桶优先使用预分配的桶。 哈希高8位优化桶查找key：将key 哈希值的高8位存储在桶的 tophash数组中，这样查找时不用比较完整的 key 就能过滤掉不符合要求的 key， tophash中的值相等，再去比较 key 值 桶中key/value分开存放：同种所有的key存一起，所有的value村一起，目的是为了方便内存对齐 根据k/v大小存储不同值：当 k 或 v 大于 128字节时，其存储的字段为指针，指向k或v的实际内容，小于等于128字节，其存储的字段为原值 桶的搬迁状态：可以根据 tophash 字段的值，是否小于minTopHash，来表示桶是否处于搬迁状态 map 的扩容和搬迁策略 map的底层扩容策略如下：\nmap 的扩容策略是新分配一个更大的数组，然后在插入和删除key的时候，将对应桶中的数据迁移到新分配的桶中去 map的搬迁策略如下：\n由于map扩容需要将原有的kv键值对搬迁到新的内存地址，直接一下子全部搬完会非常的影响性能 采用渐进式的搬迁策略，将搬迁的O(N)开销均摊到O(1)的赋值和删除操作上 以下两种情况时，会进行扩容：\n当装载因子超过6.5时，扩容一倍，属于增量扩容 当使用的溢出桶过多时间，重新分配一样大的内存空间，属于等量扩容，实际上没有扩容，主要是为了回收空闲的溢出桶 装载因子等于 map中元素的个数 / map的容量，即len(map) / 2^B\n载因子用来表示空闲位置的情况，装载因子越大，表明空闲位置越少，冲突也越多 随着装载因子的增大，哈希表线性探测的平均用时就会增加，这会影响哈希表的性能，当装载因子大于70%，哈希表的性能就会急剧下降，当装载因子达到100%，整个哈希表就会完全失效，这个时候，查找和插入任意元素的复杂度都是O(N),因为需要遍历所有元素 为什么会出现以上两种情况?\n情况1：确实是数据量越来越多，撑不住了 情况2：比较特殊，归根结底还是map删除的特性导致的，当我们不断向哈希表中插入数据，并且将他们又全部删除时，其内存占用并不会减少，因为删除只是将桶对应位置的tohash置nil而已，这种情况下，就会不断的积累溢出桶造成内存泄露。为了解决这种情况，采用了等量扩容的机制，一旦哈希表中出现了过多的溢出桶，她会创建新桶保存数据，gc会清理掉老的溢出桶，从而避免内存泄露。 如何定义溢出桶是否太多需要等量扩容呢？两种情况：\n当B小于15时，溢出桶的数量超过2^B，属于溢出桶数量太多，需要等量扩容 当B大于等于15时，溢出桶数量超过2^B，属于溢出桶数量太多，需要等量扩容 Map 的源码实现 创建 map 创建 map，主要是创建 hmap 这个结构，以及对 hmap 的初始化\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 func makemap(t *maptype, hint int, h *hmap) *hmap { mem, overflow := math.MulUintptr(uintptr(hint), t.Bucket.Size_) if overflow || mem \u0026gt; maxAlloc { hint = 0 } // 初始化 Hmap if h == nil { h = new(hmap) } // 获取一个随机种子 h.hash0 = fastrand() // 确定B的大小 B := uint8(0) for overLoadFactor(hint, B) { B++ } h.B = B if h.B != 0 { var nextOverflow *bmap h.buckets, nextOverflow = makeBucketArray(t, h.B, nil) if nextOverflow != nil { h.extra = new(mapextra) h.extra.nextOverflow = nextOverflow } } return h } 默认会创建 2^B个bucket，如果b大于等于4，会预先创建一些溢出桶，b小于4的情况可能用不到溢出桶\nmap中赋值元素 确认hash值 根据hash值确认 key 所属的 bucket 遍历所属桶和该桶串联的溢出桶，寻找key 当前所属桶以及串联的溢出桶都已满，则创建一个新的溢出桶，串联的末尾 根据 key 是否存在，选择插入还是更新 key/value 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 func mapassign(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer { if h == nil { panic(plainError(\u0026#34;assignment to entry in nil map\u0026#34;)) } if raceenabled { callerpc := getcallerpc() pc := abi.FuncPCABIInternal(mapassign) racewritepc(unsafe.Pointer(h), callerpc, pc) raceReadObjectPC(t.Key, key, callerpc, pc) } if msanenabled { msanread(key, t.Key.Size_) } if asanenabled { asanread(key, t.Key.Size_) } if h.flags\u0026amp;hashWriting != 0 { fatal(\u0026#34;concurrent map writes\u0026#34;) } // 第一步，确认 hash值 hash := t.Hasher(key, uintptr(h.hash0)) // Set hashWriting after calling t.hasher, since t.hasher may panic, // in which case we have not actually done a write. h.flags ^= hashWriting if h.buckets == nil { h.buckets = newobject(t.Bucket) // newarray(t.Bucket, 1) } again: // 第二步，根据hash值确认所属的 bucket bucket := hash \u0026amp; bucketMask(h.B) if h.growing() { growWork(t, h, bucket) } b := (*bmap)(add(h.buckets, bucket*uintptr(t.BucketSize))) top := tophash(hash) var inserti *uint8 var insertk unsafe.Pointer var elem unsafe.Pointer bucketloop: // 第三步，遍历所属桶和此通串联的溢出桶，寻找key（通过桶的tophash字段和 key 值） for { for i := uintptr(0); i \u0026lt; bucketCnt; i++ { if b.tophash[i] != top { if isEmpty(b.tophash[i]) \u0026amp;\u0026amp; inserti == nil { inserti = \u0026amp;b.tophash[i] insertk = add(unsafe.Pointer(b), dataOffset+i*uintptr(t.KeySize)) elem = add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.KeySize)+i*uintptr(t.ValueSize)) } if b.tophash[i] == emptyRest { break bucketloop } continue } k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.KeySize)) if t.IndirectKey() { k = *((*unsafe.Pointer)(k)) } if !t.Key.Equal(key, k) { continue } // already have a mapping for key. Update it. if t.NeedKeyUpdate() { typedmemmove(t.Key, k, key) } elem = add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.KeySize)+i*uintptr(t.ValueSize)) goto done } ovf := b.overflow(t) if ovf == nil { break } b = ovf } // Did not find mapping for key. Allocate new cell \u0026amp; add entry. // If we hit the max load factor or we have too many overflow buckets, // and we\u0026#39;re not already in the middle of growing, start growing. if !h.growing() \u0026amp;\u0026amp; (overLoadFactor(h.count+1, h.B) || tooManyOverflowBuckets(h.noverflow, h.B)) { hashGrow(t, h) goto again // Growing the table invalidates everything, so try again } // 第四步，当前链上所有桶都满了，创建一个新的溢出桶，串联的末尾，然后更新相关字段 if inserti == nil { // The current bucket and all the overflow buckets connected to it are full, allocate a new one. newb := h.newoverflow(t, b) inserti = \u0026amp;newb.tophash[0] insertk = add(unsafe.Pointer(newb), dataOffset) elem = add(insertk, bucketCnt*uintptr(t.KeySize)) } // 第五步，在插入位置存储新的 key/elem if t.IndirectKey() { kmem := newobject(t.Key) *(*unsafe.Pointer)(insertk) = kmem insertk = kmem } if t.IndirectElem() { vmem := newobject(t.Elem) *(*unsafe.Pointer)(elem) = vmem } typedmemmove(t.Key, insertk, key) *inserti = top h.count++ done: if h.flags\u0026amp;hashWriting == 0 { fatal(\u0026#34;concurrent map writes\u0026#34;) } h.flags \u0026amp;^= hashWriting if t.IndirectElem() { elem = *((*unsafe.Pointer)(elem)) } return elem } 删除元素 写保护 获取hash值 根据hash值确定bucket位置，判断是否需要扩容 遍历桶和桶串联的溢出桶 找到key，将tophash中key的标志位置空 删除 key 仅仅只是将其对应的 tophash 值置空，如果 kv 存储的是指针，那么会清理指针指向的内存，否则不会真正回收内存，内存占用并不会减少 如果正在扩容，并且操作的 bucket 没有搬迁完，那么会搬迁bucket 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 func mapdelete(t *maptype, h *hmap, key unsafe.Pointer) { if raceenabled \u0026amp;\u0026amp; h != nil { callerpc := getcallerpc() pc := abi.FuncPCABIInternal(mapdelete) racewritepc(unsafe.Pointer(h), callerpc, pc) raceReadObjectPC(t.Key, key, callerpc, pc) } if msanenabled \u0026amp;\u0026amp; h != nil { msanread(key, t.Key.Size_) } if asanenabled \u0026amp;\u0026amp; h != nil { asanread(key, t.Key.Size_) } if h == nil || h.count == 0 { if err := mapKeyError(t, key); err != nil { panic(err) // see issue 23734 } return } if h.flags\u0026amp;hashWriting != 0 { fatal(\u0026#34;concurrent map writes\u0026#34;) } hash := t.Hasher(key, uintptr(h.hash0)) // Set hashWriting after calling t.hasher, since t.hasher may panic, // in which case we have not actually done a write (delete). h.flags ^= hashWriting bucket := hash \u0026amp; bucketMask(h.B) if h.growing() { growWork(t, h, bucket) } b := (*bmap)(add(h.buckets, bucket*uintptr(t.BucketSize))) bOrig := b top := tophash(hash) search: for ; b != nil; b = b.overflow(t) { for i := uintptr(0); i \u0026lt; bucketCnt; i++ { if b.tophash[i] != top { if b.tophash[i] == emptyRest { break search } continue } k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.KeySize)) k2 := k if t.IndirectKey() { k2 = *((*unsafe.Pointer)(k2)) } if !t.Key.Equal(key, k2) { continue } // Only clear key if there are pointers in it. if t.IndirectKey() { *(*unsafe.Pointer)(k) = nil } else if t.Key.PtrBytes != 0 { memclrHasPointers(k, t.Key.Size_) } e := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.KeySize)+i*uintptr(t.ValueSize)) if t.IndirectElem() { *(*unsafe.Pointer)(e) = nil } else if t.Elem.PtrBytes != 0 { memclrHasPointers(e, t.Elem.Size_) } else { memclrNoHeapPointers(e, t.Elem.Size_) } b.tophash[i] = emptyOne // If the bucket now ends in a bunch of emptyOne states, // change those to emptyRest states. // It would be nice to make this a separate function, but // for loops are not currently inlineable. if i == bucketCnt-1 { if b.overflow(t) != nil \u0026amp;\u0026amp; b.overflow(t).tophash[0] != emptyRest { goto notLast } } else { if b.tophash[i+1] != emptyRest { goto notLast } } for { b.tophash[i] = emptyRest if i == 0 { if b == bOrig { break // beginning of initial bucket, we\u0026#39;re done. } // Find previous bucket, continue at its last entry. c := b for b = bOrig; b.overflow(t) != c; b = b.overflow(t) { } i = bucketCnt - 1 } else { i-- } if b.tophash[i] != emptyOne { break } } notLast: h.count-- // Reset the hash seed to make it more difficult for attackers to // repeatedly trigger hash collisions. See issue 25237. if h.count == 0 { h.hash0 = fastrand() } break search } } if h.flags\u0026amp;hashWriting == 0 { fatal(\u0026#34;concurrent map writes\u0026#34;) } h.flags \u0026amp;^= hashWriting } 查询元素 计算hash值，并根据hash值找到桶 遍历桶以及该桶串联的溢出桶，查找key 如果根据hash值定位到桶正在搬迁，并且这个桶还没有搬迁到新的hash表中，那么就从老的hash表中查找 在bucket中进行顺序查找，使用高8位进行快速过滤，高8位相等，再比较key是否相等，找到就返回 value，如果当前bucket未找到，就继续找溢出桶中的数据，都没有的话则返回零值 扩容和搬迁 扩容触发条件： 当 map 中的键值对数量达到负载因子（load factor）乘以桶的数量时，就会触发扩容。负载因子是键值对数量与桶的数量的比值，它可以看作是哈希表的填充程度。扩容条件通常是负载因子达到阈值（通常为0.75），此时 map 会自动扩容。 创建新的哈希表：在扩容时，Go 会创建一个新的哈希表，通常将当前哈希表的大小翻倍。新的哈希表的桶数量会是当前桶数量的两倍 重新计算哈希值：对于当前哈希表中的每个键值对，会重新计算其哈希值。由于新哈希表的桶数量发生变化，哈希函数可能也会不同。 搬迁键值对：将重新计算哈希值后的键值对搬迁到新的哈希表中。这个过程可能涉及到哈希冲突的处理，因为在新的哈希表中，同一个桶上可能会有多个键值对。链地址法被用来处理这种情况，即将相同桶上的键值对存储在链表中。 替换旧的哈希表：一旦所有的键值对都搬迁完成，新的哈希表会取代旧的哈希表，成为 map 的底层数据结构。 释放旧哈希表：最后，旧的哈希表会被释放，回收其占用的内存。 这个扩容和搬迁的过程保证了 map 在不断插入新的键值对时，能够动态地调整大小以保持较低的负载因子，从而提高性能。需要注意的是，由于搬迁的过程，map 的迭代顺序在扩容后可能会发生变化。这是因为新的哈希表的桶数量和哈希函数可能不同，导致键值对的分布发生改变。因此，如果迭代顺序对你的程序逻辑有影响，需要谨慎处理。\n并发安全 map 并不是并发安全的，如果多个 goroutine 同时对同一个 map 进行读写操作，可能会导致数据竞争，进而导致程序行为不一致，甚至崩溃，这是因为 map 操作不是原子的，例如读取、写入等操作都可能被打断，从而导致并发访问问题\n以下是一个简单的例子，演示了 map 的并发安全问题：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 func main() { myMap := make(map[string]int) for i := 0; i \u0026lt; 10; i++ { go func (i int) { myMap[fmt.Sprintf(\u0026#34;key%d\u0026#34;, i)] = i\t}(i) } go func () { for k, v := range myMap { fmt.Println(k, v) } }() time.Sleep(time.Second * 2) } 在上述例子中，多个 goroutine 同时向 myMap 写入数据，同时一个 goroutine 读取 myMap 的数据。由于 map 操作不是并发安全的，可能会发生数据竞争，导致输出结果不确定。\n为了解决这个问题，可以采用以下两种方式之一：\n加锁保护：使用 sync.Mutex 或者其他互斥锁来保护对 map 的并发访问。在读写 map 之前，使用锁进行加锁，操作完成后解锁。这样可以确保在任何时刻只有一个 goroutine 能够访问 map。 使用并发安全的数据结构：Go 提供了 sync.Map 类型，它是一种并发安全的映射。sync.Map 的操作是原子的，不需要额外的锁。使用 sync.Map 可以避免手动管理锁，但也需要注意它的一些特性，例如不能通过 range 直接迭代，需要使用 Load 和 Range 方法 map 的无序性 Go 中的 map 是无序的，这意味着在遍历 map 时，元素的顺序是不确定的。这是由于 map 的实现原理和设计选择所导致的。\n哈希表实现： map 的底层实现是哈希表，它使用哈希函数将键映射到桶（buckets）中。在理想情况下，哈希函数将键均匀地分散到桶中，但由于哈希表的大小是有限的，而键的域可能是无限的，因此会出现哈希冲突。 冲突解决方法：Go使用链地址发来解决哈希冲突，如果两个键映射到同一个bucket，他们将被链接成一个链表，当桶的链表较长时，可能会影响查找性能，但由于哈希表的大小是固定的，冲突是不可避免的 动态扩容：为了保持较低的负载因子，map在键值对数量达到一定阈值（负载因子达到0.75）时会自动进行扩容，这就导致了哈希表的重新排列，而这个重新排列会使得元素的顺序发生变化 ","date":"2023-11-09T16:09:27+08:00","permalink":"https://x-xkang.com/p/golang%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E4%B9%8Bmap%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0/","title":"Golang数据类型之Map源码实现"},{"content":" 变量与可变性 数据类型 标量类型 复合类型 函数 注释 控制流 变量与可变性 声明变量使用let关键字 默认情况下，变量是不可变的（Immutable） 声明变量时，在变量前面加mut，就可以使变量可变。 变量和常量 常量（constant），常量在绑定值以后也是不可变的，但是它与不可变的变量有很多区别：\n不可以使用mut，常量永远都是不可变的 声明常量使用 const 关键字，它的类型必须被标注 常量可以在任何作用域内进行声明，包括全局作用域 常量只可以绑定到常量表达式，无法绑定到函数的调用结果或只能在运行时才能计算出的值 在程序运行期间，常量在其声明的作用域内一直有效\n命名规范：Rust锂常量使用全大写字母，每个单词之间使用下划线分类例如：MAX_POINTS\nShadowing 可以使用相同的名字声明新的变量，新的变量就会shadow之前声明的同名变量 在后续的代码中这个变量名代表的就是新的变量 shadow 和把变量标记为 mut 是不一样的： 如果不使用 let 关键字，那么重新给非mut的变量赋值会导致编译时错误 而使用let关键字声明的同名新变量，也是不可变的 使用let声明的同名新变量，它的类型可以与之前不同 1 2 3 4 5 6 7 8 9 10 fn main(){ let x = 5; let x = x+1; { let x = x * 2; println!(\u0026#34;The value of x in the inner scope is: {}\u0026#34;, x); } println(\u0026#34;The value of x is {}\u0026#34;, x); } 这个程序首先将 x 绑定到 5 上，接着通过 let x = 创建了一个新变量 x，获取初始值并加1，这样x的值就变成了6了,然后，在花括号创建的内部作用域内，第三个 let 语句也隐藏了 x,并创建了一个新的变量，将之前的值乘以 2,x得到的值是12，当该作用域结束时，内部 shadowing的作用域也结束了，x又回到了 6，运行这段程序，它会有如下输出：\n1 2 3 4 5 6 ❯ cargo run Compiling project-1 v0.1.0 (D:\\www\\demos\\rust\\project-1) Finished dev [unoptimized + debuginfo] target(s) in 0.46s Running `target\\debug\\project-1.exe` The value of x in the inner scope is 12 The value of x is 6 隐藏与将变量标记为 mut 是有区别的，当不小心尝试对变量重新赋值时，如果没有使用let关键字，就会导致编译时错误，通过使用let,我们可以用这个值进行一些计算，不过计算完之后变量仍然是不可变的\nmut 与隐藏的另一个区别是，当再次使用 let 时，实际上创建了一个新变量，我们可以改变值的类型，并且复用这个名字，例如，假设程序请求用户输入空格字符串来说明希望在文本之间显示多个空格，接下来我们 将输入存储成数字（多少个空格）:\n1 2 let spaces = \u0026#34; \u0026#34;; let spaces = spaces.len(); 第一个 spaces 变量是字符串类型，第二个 spaces 变量是数字类型。隐藏使我们不必使用不同的名字，如 spaces_str 和 spaces_num；相反，我们可以复用 spaces 这个更简单的名字。然而，如果尝试使用 mut，将会得到一个编译时错误\n数据类型 在 Rust 中，每一个值都属于某一个 数据类型（data type），这告诉 Rust 它被指定为何种数据，以便明确数据处理方式。我们将看到两类数据类型子集：标量（scalar）和复合（compound）。\n记住，Rust 是 静态类型（statically typed）语言，也就是说在编译时就必须知道所有变量的类型。根据值及其使用方式，编译器通常可以推断出我们想要用的类型。当多种类型均有可能时， “比较猜测的数字和秘密数字” 使用 parse 将 String 转换为数字时，必须增加类型注解，像这样：\n1 let guess: u32 = \u0026#34;42\u0026#34;.parse().expect(\u0026#34;Not a number\u0026#34;); 如果不像上面的代码这样添加类型注解 : u32，Rust 会显示如下错误，这说明编译器需要我们提供更多信息，来了解我们想要的类型\n标量类型 标量（scalar）类型代表一个单独的值。Rust有四种基本的标量类型：整形、浮点型、布尔类型和字符串类型，你可能在其他语言中建国它们。让我们深入了解它们在Rust中是如何工作的。\n整型 整数是一个没有小数部分的数字，该类型声明表明，它关联的值应该是一个占据32比特位的无符号整数（有符号整数类型以 i 开头而不是 u）。\nRust中的整形\n长度 有符号 无符号 8-bit i8 u8 16-bit i16 u16 32-bit i32 u32 64-bit i64 u64 128-bit i128 u128 arch isize usize 每一个有符号的变体可以储存包含从 -(2^(n - 1)) 到 2^(n - 1) - 1 在内的数字，这里 n 是变体使用的位数。所以 i8 可以储存从 -(2^7) 到 2^7 - 1 在内的数字，也就是从 -128 到 127。无符号的变体可以储存从 0 到 2^n - 1 的数字，所以 u8 可以储存从 0 到 2^8 - 1 的数字，也就是从 0 到 255。\n另外，isize 和 usize 类型依赖运行程序的计算机架构：64 位架构上它们是 64 位的，32 位架构上它们是 32 位的。\nRust 中的整型字面量\n数字字面量 例子 Decimal(十进制) 98_222 Hex(十六进制) 0xff Octal(八进制) 0o77 Binary(二进制) 0b1111_0000 Byte(字节) b\u0026rsquo;A' 【注意】：整型溢出 比如说有一个 u8 类型的变量 num,它可以存储从0-255的值，那么当你将其修改为256时会发生什么呢？这被称为整型溢出（integer overflow），这会导致以下两种行为之一的发生，当在debug模式时，Rust检查这类问题并使程序panic，这个术语被Rust用来表明程序因错误而退出 使用 \u0026ndash;release flag 在 release 模式中构建时，Rust不会检测会导致panic的整型溢出。相反发生整型溢出时，Rust会进行一种被称为二进制补码wrapping(two\u0026rsquo;s complement wrapping)的操作。简而言之，比此类型能容纳的最大值还大的值会绕到最小值，值256会变成0，值257会变成1，以此类推。程序不会 panic 为了显式的处理溢出的可能性，可以使用者几类标准库提供的原始数字类型方法：\n所有模式下都可以使用 wrapping_*方法进行 wrapping，如 wrapping_add 如果 checked_* 方法出现溢出，则返回 None 用 overflowing_* 方法返回值和一个布尔值，布尔值表示是否溢出 用 saturating_* 方法在值的最小值或最大值处进行饱和处理 浮点型 Rust 也有两个原生的 浮点数（floating-point numbers）类型，它们是带小数点的数字。Rust 的浮点数类型是 f32 和 f64，分别占 32 位和 64 位。默认类型是 f64，因为在现代 CPU 中，它与 f32 速度几乎一样，不过精度更高。所有的浮点型都是有符号的。\n1 2 3 4 5 fn main() { let x = 2.0; // f64 let y: f32 = 3.0; // f32 } 浮点数采用 IEEE-754 标准表示。f32 是单精度浮点数，f64 是双精度浮点数。\n数值运算 Rust 中所有数字类型都支持数字运算：加法、减法、乘法、除法和取余。整数除法会向零舍入到最接近的的整数。下面的代码展示了如何在 let 语句中使用它们：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 fn main() { // addition let sum = 5 + 10; // subtraction let diffrence = 95.5 - 4.3; // multiplication let product = 4 * 30; // division let quotient = 56.7 / 32.2; let truncated = -5 / 3; // 结果为 -1 // remainder let remainder = 43 % 5; println!( \u0026#34;sum: {}, diffrence: {}, product: {}, quotient: {}, truncated: {}, remainder: {}\u0026#34;, sum, diffrence, product, quotient, truncated, remainder ) // 输出结果： // sum: 15, diffrence: 91.2, product: 120, quotient: 1.7608695652173911, truncated: -1, remainder: 3 } 布尔型 正如其他大部分编程语言一样，Rust中的布尔类型有两个可能得值：true 和 false。Rust中的布尔类型使用 bool 表示。例如：\n1 2 3 4 fn main(){ let t = true; let f:bool } 字符类型 Rust的 char类型是语言中最原生的字母类型，下面是一些声明 char 值的例子：\n1 2 3 4 5 fn main(){ let c = \u0026#39;z\u0026#39;; let z:char = \u0026#39;Z\u0026#39;; let heart_eyed_cat = \u0026#39;😻\u0026#39;; } 注意，我们用单引号声明 char 字面量，而与之相反的是，使用双引号声明字符串字面量。Rust的char类型的大小为四个字节（four bytes）,并代表了一个 Unicode标量值（Unicode scalar value）,这意味着它可以比ASCII表示更多的内容。在Rust中，带变音符号的字母（Accented letters），中文、日文、韩文等字符，emoji（绘文字）以及零长度的空白字符都是有效的char值。Unicode标量值包含 U+0000 到 U+D7FF 和 U+E000 到 U+10FFFF 之间的所有值。不过，“字符”并不是一个Unicode中的概念，所以人直觉上的”字符“可能与Rust中的char并不符合。\n复合类型 复合类型（compound types）可以将多个值组合成一个类型。Rust有两个原生的符合类型：元组（tuple）和数组（array）。\n元组\n元组是一个将多个其他类型的值组合进一个符合类型的主要方式。元组长度固定：一旦声明，其长度不会增大或缩小。 我们使用包含在圆括号中的逗号分割的值列表来创建一个元组。元组中的每一个位置都有一个类型，而且这些不同值的类型也不必是相同的，：\n1 2 3 fn main(){ let tup:(f32, f64, u8) = (500, 6.4, 1); } tup变量绑定到整个元组上，因为元组是一个单独的复合元素。为了从元组中获取单个值，可以使用模式匹配（pattern matching）来解构（distructure）元组值 like this:\n1 2 3 4 5 fn main(){ let tup: (u32, \u0026amp;str, f64) = (3, \u0026#34;hello\u0026#34;, 3.14); let (x, y, z) = tup; println!(\u0026#34;The x is {}, y is {}, z is {}\u0026#34;, x, y, z); } 程序首先创建了一个元组并绑定到tup变量上。接着使用了let 和一个模式将tup分成了三个不同的变量，x、y和z。这叫做解构，因为它将一个元组拆成了三个部分。 我们也可以使用符号（.）后跟值的索引来直接访问它们。 例如：\n1 2 3 4 5 6 7 8 9 fn main(){ let tup:(i32, f64, u8) = (500, 6.4, 1); let five_hundred = x.0; let six_point_four = x.1; let one = x.2; } 不带任何值的元组有个特殊的名称，叫做**单元（uint）**元组。这种值以及对应的类型都写作()，表示空值或空的返回类型。如果表达式不返回任何其他值，则会隐式返回单元值。\n数组类型\n另一个包含多个值的方式是数组（array）。与元组不同，数组中的每个元素的类型必须相同。Rust 中的数组与一些其他语言中的数组不同，Rust 中的数组长度是固定的。 我们将数组的值写成在方括号内，用逗号分隔：\n1 2 3 fn main(){ let arr = [1,2,3,4,5]; } 当你想要在栈（stack）而不是在堆（heap）上为数据分配空间，或者是想要确保总是有固定数量的元素时，数组非常有用。但是数组并不如 vector 类型灵活。vector 类型是标准库提供的一个 允许 增长和缩小长度的类似数组的集合类型。当不确定是应该使用数组还是 vector 的时候，那么很可能应该使用 vector。\n然而，当你确定元素个数不会改变时，数组会更有用。例如，当你在一个程序中使用月份名字时，你更应趋向于使用数组而不是 vector，因为你确定只会有 12 个元素。\n1 2 3 4 fn main(){ let months = [\u0026#34;January\u0026#34;, \u0026#34;February\u0026#34;, \u0026#34;March\u0026#34;, \u0026#34;April\u0026#34;, \u0026#34;May\u0026#34;, \u0026#34;June\u0026#34;, \u0026#34;July\u0026#34;, \u0026#34;August\u0026#34;, \u0026#34;September\u0026#34;, \u0026#34;October\u0026#34;, \u0026#34;November\u0026#34;, \u0026#34;December\u0026#34;]; } 可以像这样编写数组的类型：在方括号中包含每个元素的类型，后跟分号，再后跟数组元素的数量。\n1 2 3 fn main() { let a:[i32; 5] = [1,2,3,4,5]; } 这里，i32是每个元素的类型。分号之后，数字5表名改数组包含五个元素。 还可以通过在方括号内指定初始值加分号再加元素个数来创建一个每个元素都为相同值的数组：\n1 2 3 fn main(){ let a = [3; 5]; } 变量名为 a 的数组将包含 5 个元素，这些元素的值最初都将被设置为 3。这种写法与 let a = [3, 3, 3, 3, 3]; 效果相同，但更简洁。\n访问数组元素 数组是可以在栈（stack）上分配的一直固定大小的单个内存块。可以使用索引来访问数组的元素，像这样： 1 2 3 4 5 6 fn main(){ let a = [1,2,3,4,5]; let first = a[0]; let second = a[1]; } 在这个例子中，叫做first的变量的值是 1，因为它是数组索引 [0]的值。变量second 将会是数组索引 [1] 的值 2。\n无效的数组元素访问 让我们看看如果我们访问数组结尾之后的元素会发生什么呢？比如你执行以下代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 use std::io; fn main() { let a = [1, 2, 3, 4, 5]; println!(\u0026#34;Please enter an array index.\u0026#34;); let mut index = String::new(); io::stdin() .read_line(\u0026amp;mut index) .expect(\u0026#34;Failed to read line\u0026#34;); let index: usize = index .trim() .parse() .expect(\u0026#34;Index entered was not a number\u0026#34;); let element = a[index]; println!(\u0026#34;The value of the element at index {index} is: {element}\u0026#34;); } 此代码编译成功。如果您使用 cargo run 运行此代码并输入 0、1、2、3 或 4，程序将在数组中的索引处打印出相应的值。如果你输入一个超过数组末端的数字，如 10，你会看到这样的输出：\n1 2 thread \u0026#39;main\u0026#39; panicked at \u0026#39;index out of bounds: the len is 5 but the index is 10\u0026#39;, src/main.rs:19:19 note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace 程序在索引操作中使用一个无效的值时导致 运行时 错误。程序带着错误信息退出，并且没有执行最后的 println! 语句。当尝试用索引访问一个元素时，Rust 会检查指定的索引是否小于数组的长度。如果索引超出了数组长度，Rust 会 panic，这是 Rust 术语，它用于程序因为错误而退出的情况。这种检查必须在运行时进行，特别是在这种情况下，因为编译器不可能知道用户在以后运行代码时将输入什么值。\n这是第一个在实战中遇到的 Rust 安全原则的例子。在很多底层语言中，并没有进行这类检查，这样当提供了一个不正确的索引时，就会访问无效的内存。通过立即退出而不是允许内存访问并继续执行，Rust 让你避开此类错误。\n函数 函数在 Rust 代码中非常普遍。你已经见过语言中最重要的函数之一：main 函数，它是很多程序的入口点。你也见过 fn 关键字，它用来声明新函数。\nRust 代码中的函数和变量名使用 snake case 规范风格。在 snake case 中，所有字母都是小写并使用下划线分隔单词。这是一个包含函数定义示例的程序：\n1 2 3 4 5 6 7 8 9 fn main(){ println!(\u0026#34;Hello world\u0026#34;); another_function(); } fn another_function(){ println!(\u0026#34;Another function.\u0026#34;); } 我们在 Rust 中通过输入 fn 后面跟着函数名和一对圆括号来定义函数。大括号告诉编译器哪里是函数体的开始和结尾。\n可以使用函数名后跟圆括号来调用我们定义过的任意函数。因为程序中已定义 another_function 函数，所以可以在 main 函数中调用它。注意，源码中 another_function 定义在 main 函数 之后；也可以定义在之前。Rust 不关心函数定义所在的位置，只要函数被调用时出现在调用之处可见的作用域内就行。\n参数 我们可以定义为拥有 参数（parameters）的函数，参数是特殊变量，是函数签名的一部分。当函数拥有参数（形参）时，可以为这些参数提供具体的值（实参）。技术上讲，这些具体值被称为参数（arguments），但是在日常交流中，人们倾向于不区分使用 parameter 和 argument 来表示函数定义中的变量或调用函数时传入的具体值。\n在这版 another_function 中，我们增加了一个参数：\n1 2 3 4 5 6 7 fn main() { another_function(5); } fn another_function(x: i32) { println!(\u0026#34;The value of x is: {x}\u0026#34;); } 尝试运行程序，将会输出如下内容：\n1 2 3 4 5 $ cargo run Compiling functions v0.1.0 (file:///projects/functions) Finished dev [unoptimized + debuginfo] target(s) in 1.21s Running `target/debug/functions` The value of x is: 5 在函数签名中，必须 声明每个参数的类型。这是 Rust 设计中一个经过慎重考虑的决定：要求在函数定义中提供类型注解，意味着编译器再也不需要你在代码的其他地方注明类型来指出你的意图。而且，在知道函数需要什么类型后，编译器就能够给出更有用的错误消息。\n当定义多个参数时，使用逗号分隔，像这样：\n1 2 3 4 5 6 7 fn main() { print_labeled_measurement(5, \u0026#39;h\u0026#39;); } fn print_labeled_measurement(value: i32, unit_label: char) { println!(\u0026#34;The measurement is: {value}{unit_label}\u0026#34;); } 这个例子创建了一个名为 print_labeled_measurement 的函数，它有两个参数。第一个参数名为 value，类型是 i32。第二个参数是 unit_label ，类型是 char。然后，该函数打印包含 value 和 unit_label 的文本。\n语句和表达式 函数体由一系列的语句和一个可选的结尾表达式构成。目前为止，我们提到的函数还不包含结尾表达式，不过你已经见过作为语句一部分的表达式。因为 Rust 是一门基于表达式（expression-based）的语言，这是一个需要理解的（不同于其他语言）重要区别。其他语言并没有这样的区别，所以让我们看看语句与表达式有什么区别以及这些区别是如何影响函数体的。\n语句(Statement)是执行一些操作但不返回值的指令。表达式（expressions）计算并产生一个值。让我们看一些例子： 实际上，我们已经使用过语句和表达式了，使用let关键字创建变量并绑定一个值是一个语句。\n1 2 3 fn main(){ let y = 6; } 函数定义也是语句，上面整个例子本身就是一个语句。 语句不返回值。因此，不能把 let 语句赋值给另一个变量，因为 let 语句本身没有返回值。比如下面的例子尝试做的，会产生一个错误：\n1 let x = (let y = 6); 当运行这个程序时，会得到如下错误：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 $ cargo run Compiling functions v0.1.0 (file:///projects/functions) error: expected expression, found `let` statement --\u0026gt; src/main.rs:2:14 | 2 | let x = (let y = 6); | ^^^ error: expected expression, found statement (`let`) --\u0026gt; src/main.rs:2:14 | 2 | let x = (let y = 6); | ^^^^^^^^^ | = note: variable declaration using `let` is a statement error[E0658]: `let` expressions in this position are unstable --\u0026gt; src/main.rs:2:14 | 2 | let x = (let y = 6); | ^^^^^^^^^ | = note: see issue #53667 \u0026lt;https://github.com/rust-lang/rust/issues/53667\u0026gt; for more information warning: unnecessary parentheses around assigned value --\u0026gt; src/main.rs:2:13 | 2 | let x = (let y = 6); | ^ ^ | = note: `#[warn(unused_parens)]` on by default help: remove these parentheses | 2 - let x = (let y = 6); 2 + let x = let y = 6; | For more information about this error, try `rustc --explain E0658`. warning: `functions` (bin \u0026#34;functions\u0026#34;) generated 1 warning error: could not compile `functions` due to 3 previous errors; 1 warning emitted let y = 6 语句并不返回值，所以没有可以绑定到x上的值。这与其他语言不同，例如 C 和 Ruby，他们的赋值语句会返回所赋的值。在这些语言中，可以这么写 x = y = 6,这样 x 和 y 的值都是 6; Rust 中不能这么写。\n表达式会计算出一个值，并且你将编写的大部分 Rust 代码是由表达式组成的。考虑一个数学运算，比如 5 + 6，这是一个表达式并计算出值 11。表达式可以是语句的一部分：在示例中，语句 let y = 6; 中的 6 是一个表达式，它计算出的值是 6。函数调用是一个表达式。宏调用是一个表达式。用大括号创建的一个新的块作用域也是一个表达式，例如：\n1 2 3 4 5 6 7 8 fn main(){ let y = { let x = 3; x + 1 // 【注意】没有 \u0026#34;;\u0026#34; } println!(\u0026#34;The value of y is: {y}\u0026#34;); } 这个表达式：\n1 2 3 4 fn main(){ let x = 3; x + 1 } 是一个代码块，它的值是 4。这个值作为 let 语句的一部分被绑定到 y 上。注意 x + 1 这一行在结尾没有分号，与你见过的大部分代码行不同。表达式的结尾没有分号。如果在表达式的结尾加上分号，它就变成了语句，而语句不会返回值。在接下来探索具有返回值的函数和表达式时要谨记这一点。\n具有返回值的函数\n函数可以向调用它的代码返回值。我们并不对返回值命名，但要在箭头（-\u0026gt;）后声明它的类型。在 Rust 中，函数的返回值等同于函数体最后一个表达式的值。使用 return关键字和指定值，可从函数中提前返回；但大部分函数隐式的返回最后的表达式。这是一个有返回值的函数的例子：\n1 2 3 4 5 6 7 8 9 fn five() -\u0026gt; i32 { 5 } fn main() { let x = five(); println!(\u0026#34;The value of x is: {x}\u0026#34;); } 在 five 函数中没有函数调用、宏、甚至没有 let 语句 —— 只有数字 5。这在 Rust 中是一个完全有效的函数。注意，也指定了函数返回值的类型，就是 -\u0026gt; i32。尝试运行代码；输出应该看起来像这样:\n1 2 3 4 5 $ cargo run Compiling functions v0.1.0 (file:///projects/functions) Finished dev [unoptimized + debuginfo] target(s) in 0.30s Running `target/debug/functions` The value of x is: 5 five 函数的返回值是 5，所以返回值类型是 i32。让我们仔细检查一下这段代码。有两个重要的部分：首先，let x = five(); 这一行表明我们使用函数的返回值初始化一个变量。因为 five 函数返回 5，这一行与如下代码相同：\n1 2 3 fn main(){ let x = 5; } 其次，five 函数没有参数并定义了返回值类型，不过函数体只有单单一个 5 也没有分号，因为这是一个表达式，我们想要返回它的值。\n让我们看看另外一个例子：\n1 2 3 4 5 6 7 8 9 fn main() { let x = plus_one(5); println!(\u0026#34;The value of x is: {x}\u0026#34;); } fn plus_one(x: i32) -\u0026gt; i32 { x + 1 } 运行代码会打印出 The value of x is: 6。但如果在包含 x + 1 的行尾加上一个分号，把它从表达式变成语句，我们将看到一个错误。\n主要的错误信息，“mismatched types”（类型不匹配），揭示了代码的核心问题。函数 plus_one 的定义说明它要返回一个 i32 类型的值，不过语句并不会返回值，使用单位类型 () 表示不返回值。因为不返回值与函数定义相矛盾，从而出现一个错误。在输出中，Rust 提供了一条信息，可能有助于纠正这个错误：它建议删除分号，这会修复这个错误。\n控制流 if表达式 if 表达式允许根据条件执行不同的代码分支。你提供一个条件并表示“如果条件满足，运行这段代码；如果条件不满足，不运行这段代码。”\n1 2 3 4 5 6 7 8 9 fn main(){ let number = 3; if number \u0026lt; 5 { println!(\u0026#34;condition was true\u0026#34;); }else { println!(\u0026#34;condition was false\u0026#34;); } } 另外值得注意的是代码中的条件 必须 是 bool 值。如果条件不是 bool 值，我们将得到一个错误。例如，尝试运行以下代码：\n1 2 3 4 5 6 7 fn main(){ let number = 3; // 这里if 条件的值是3 if number { println!(\u0026#34;number was three\u0026#34;); } } Rust抛出了一个错误：\n1 2 3 4 5 6 7 8 9 10 $ cargo run Compiling branches v0.1.0 (file:///projects/branches) error[E0308]: mismatched types --\u0026gt; src/main.rs:4:8 | 4 | if number { | ^^^^^^ expected `bool`, found integer For more information about this error, try `rustc --explain E0308`. error: could not compile `branches` due to previous error 这个错误表明 Rust 期望一个 bool 却得到了一个整数。不像 Ruby 或 JavaScript 这样的语言，Rust 并不会尝试自动地将非布尔值转换为布尔值。必须总是显式地使用布尔值作为 if 的条件。\n使用if else处理多重条件\n可以将 else if 表达式与 if 和 else 组合来实现多重条件。例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 fn main() { let number = 6; if number % 4 == 0 { println!(\u0026#34;number is divisible by 4\u0026#34;); } else if number % 3 == 0 { println!(\u0026#34;number is divisible by 3\u0026#34;); } else if number % 2 == 0 { println!(\u0026#34;number is divisible by 2\u0026#34;); } else { println!(\u0026#34;number is not divisible by 4, 3, or 2\u0026#34;); } } 这个程序有四个可能的执行路径。运行后应该能看到如下输出：\n1 2 3 4 5 $ cargo run Compiling branches v0.1.0 (file:///projects/branches) Finished dev [unoptimized + debuginfo] target(s) in 0.31s Running `target/debug/branches` number is divisible by 3 当执行这个程序时，它按顺序检查每个 if 表达式并执行第一个条件为 true 的代码块。注意即使 6 可以被 2 整除，也不会输出 number is divisible by 2，更不会输出 else 块中的 number is not divisible by 4, 3, or 2。原因是 Rust 只会执行第一个条件为 true 的代码块，并且一旦它找到一个以后，甚至都不会检查剩下的条件了。\n在let中使用if\n因为 if 是一个表达式，我们可以在 let 语句的右侧使用它:\n1 2 3 4 5 6 7 fn main(){ let condition = true; let number = if condition {5} else {6}; println!(\u0026#34;The value of number is: {number}\u0026#34;); } number变量将会绑定到表示if表达式结果的值上。运行这段代码看看会出现什么：\n1 2 3 4 5 $ cargo run Compiling branches v0.1.0 (file:///projects/branches) Finished dev [unoptimized + debuginfo] target(s) in 0.30s Running `target/debug/branches` The value of number is: 5 记住，代码块的值是最后一个表达式的值，而数字本身就是一个表达式。在这个例子中，整个if表达式的值取决于哪个代码块被执行。这意味着 if 的每个分支的可能的返回值都必须是相同类型；在上例中，if分支和else分支的结果都是i32整型。如果它们的类型不匹配，如下面这个例子，则会出现一个错误：\n1 2 3 4 5 6 7 fn main() { let condition = true; let number = if condition { 5 } else { \u0026#34;six\u0026#34; }; println!(\u0026#34;The value of number is: {number}\u0026#34;); } 当编译这段代码时，会得到一个错误。if 和 else 分支的值类型是不相容的，同时 Rust 也准确地指出在程序中的何处发现的这个问题：\n1 2 3 4 5 6 7 8 9 10 11 12 $ cargo run Compiling branches v0.1.0 (file:///projects/branches) error[E0308]: `if` and `else` have incompatible types --\u0026gt; src/main.rs:4:44 | 4 | let number = if condition { 5 } else { \u0026#34;six\u0026#34; }; | - ^^^^^ expected integer, found `\u0026amp;str` | | | expected because of this For more information about this error, try `rustc --explain E0308`. error: could not compile `branches` due to previous error if 代码块中的表达式返回一个整数，而 else 代码块中的表达式返回一个字符串。这不可行，因为变量必须只有一个类型。Rust 需要在编译时就确切的知道 number 变量的类型，这样它就可以在编译时验证在每处使用的 number 变量的类型是有效的。如果 number 的类型仅在运行时确定，则 Rust 无法做到这一点；且编译器必须跟踪每一个变量的多种假设类型，那么它就会变得更加复杂，对代码的保证也会减少。\n使用循环重复执行 Rust有三种循环：loop、while 和 for。我们每一个都试试。\n使用 loop 重复执行代码 loop关键字告诉 Rust 一遍又一遍地执行一段代码直到你明确要求停止。\n1 2 3 4 5 6 7 8 9 10 11 fn main(){ let mut num = 0; loop { println!(\u0026#34;The number is {}\u0026#34;, num); num += 1; if num == 10 { break; } } } 从循环返回值\nloop 的一个用例是重试可能会失败的操作，比如检查线程是否完成了任务。然而你可能会需要将操作的结果传递给其它的代码。如果将返回值加入你用来停止循环的 break 表达式，它会被停止的循环返回：\n1 2 3 4 5 6 7 8 9 10 11 12 13 fn main() { let mut counter = 0; let result = loop { counter += 1; if counter == 10 { break counter * 2; } }; println!(\u0026#34;The result is {result}\u0026#34;); } 在循环之前，我们声明了一个名为 counter 的变量并初始化为 0。接着声明了一个名为 result 来存放循环的返回值。在循环的每一次迭代中，我们将 counter 变量加 1，接着检查计数是否等于 10。当相等时，使用 break 关键字返回值 counter * 2。循环之后，我们通过分号结束赋值给 result 的语句。最后打印出 result 的值，也就是 20。\n循环标签：在多个循环之间消除歧义\n如果存在嵌套循环，break 和 continue 应用于此时最内层的循环。你可以选择在一个循环上指定一个 循环标签（loop label），然后将标签与 break 或 continue 一起使用，使这些关键字应用于已标记的循环而不是最内层的循环。下面是一个包含两个嵌套循环的示例\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 fn main() { let mut count = 0; \u0026#39;counting_up: loop { println!(\u0026#34;count = {count}\u0026#34;); let mut remaining = 10; loop { println!(\u0026#34;remaining = {remaining}\u0026#34;); if remaining == 9 { break; } if count == 2 { break \u0026#39;counting_up; } remaining -= 1; } count += 1; } println!(\u0026#34;End count = {count}\u0026#34;); } 外层循环有一个标签 counting_up，它将从 0 数到 2。没有标签的内部循环从 10 向下数到 9。第一个没有指定标签的 break 将只退出内层循环。break 'counting_up; 语句将退出外层循环。这个代码打印：\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ cargo run Compiling loops v0.1.0 (file:///projects/loops) Finished dev [unoptimized + debuginfo] target(s) in 0.58s Running `target/debug/loops` count = 0 remaining = 10 remaining = 9 count = 1 remaining = 10 remaining = 9 count = 2 remaining = 10 End count = 2 while 条件循环 在程序中计算循环的条件也很常见。当条件为 true，执行循环。当条件不再为 true，调用 break 停止循环。这个循环类型可以通过组合 loop、if、else 和 break 来实现；如果你喜欢的话，现在就可以在程序中试试。\n然而，这个模式太常用了，Rust 为此内置了一个语言结构，它被称为 while 循环。下例中使用了 while：程序循环三次，每次数字都减一。接着，在循环结束后，打印出另一个信息并退出。\n1 2 3 4 5 6 7 8 9 10 11 fn main() { let mut number = 3; while number != 0 { println!(\u0026#34;{number}!\u0026#34;); number -= 1; } println!(\u0026#34;LIFTOFF!!!\u0026#34;); } 这种结构消除了很多使用 loop、if、else 和 break 时所必须的嵌套，这样更加清晰。当条件为 true 就执行，否则退出循环。\n使用 for 遍历集合 可以使用 while 结构来遍历集合中的元素，比如数组。例如:\n1 2 3 4 5 6 7 8 9 10 fn main() { let a = [10, 20, 30, 40, 50]; let mut index = 0; while index \u0026lt; 5 { println!(\u0026#34;the value is: {}\u0026#34;, a[index]); index += 1; } } 这里，代码对数组中的元素进行计数。它从索引 0 开始，并接着循环直到遇到数组的最后一个索引（这时，index \u0026lt; 5 不再为真）。运行这段代码会打印出数组中的每一个元素：\n1 2 3 4 5 6 7 8 9 $ cargo run Compiling loops v0.1.0 (file:///projects/loops) Finished dev [unoptimized + debuginfo] target(s) in 0.32s Running `target/debug/loops` the value is: 10 the value is: 20 the value is: 30 the value is: 40 the value is: 50 数组中的所有五个元素都如期被打印出来。尽管 index 在某一时刻会到达值 5，不过循环在其尝试从数组获取第六个值（会越界）之前就停止了。\n但这个过程很容易出错；如果索引长度或测试条件不正确会导致程序 panic。例如，如果将 a 数组的定义改为包含 4 个元素而忘记了更新条件 while index \u0026lt; 4，则代码会 panic。这也使程序更慢，因为编译器增加了运行时代码来对每次循环进行条件检查，以确定在循环的每次迭代中索引是否在数组的边界内。\n作为更简洁的替代方案，可以使用 for 循环来对一个集合的每个元素执行一些代码。for 循环看起来如下示例所示：\n1 2 3 4 5 6 7 fn main() { let a = [10, 20, 30, 40, 50]; for element in a { println!(\u0026#34;the value is: {element}\u0026#34;); } } for 循环的安全性和简洁性使得它成为 Rust 中使用最多的循环结构。即使是在想要循环执行代码特定次数时，大部分 Rustacean 也会使用 for 循环。这么做的方式是使用 Range，它是标准库提供的类型，用来生成从一个数字开始到另一个数字之前结束的所有数字的序列。下面是一个使用 for 循环来倒计时的例子，它还使用了一个我们还未讲到的方法，rev，用来反转 range。\n注意：以下代码不会踏足到数字 4，仅从一个数字开始到另一个数字之前。\n1 2 3 4 5 6 fn main() { for number in (1..4).rev() { println!(\u0026#34;{number}!\u0026#34;); } println!(\u0026#34;LIFTOFF!!!\u0026#34;); } 每日一算 题目： 3的幂\n来源：leetcode 326\n难度：简单\n描述： 给定一个整数，写一个函数来判断它是否是3的幂次方。如果是，返回 true，否则，返回 false。 整数 n 是3的幂次方需满足：存在整数 x 使得 n == 3^x\n算法：\n1 2 3 4 5 6 7 8 9 10 11 12 fn main(){ let n = 9; println!(\u0026#34;{}\u0026#34;, is_three_power(n)); } fn is_three_power(mut n: i32) -\u0026gt; bool { if n \u0026gt; 0 \u0026amp;\u0026amp; n % 3 == 0 { n /= 3; } n == 1 } ","date":"2023-10-11T17:02:40+08:00","permalink":"https://x-xkang.com/p/rust%E5%AD%A6%E4%B9%A0%E5%9B%9B--%E9%80%9A%E7%94%A8%E7%BC%96%E7%A8%8B%E6%A6%82%E5%BF%B5/","title":"Rust学习（四）-- 通用编程概念"},{"content":" 知识点： let、match 等方法 相关的函数 外部的crate 猜数游戏\u0026ndash;目标 生成一个 1 到 100 间的随机数 提示玩家输入一个猜测数字 猜完之后，程序会提示猜测是太大了还是太小了 如果猜测正确，那么打印出一个庆祝信息，程序退出 如果猜测错误，则会继续提示错误信息，直到猜测正确 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 use std::io; // prelude use rand::Rng; // trait 接口 use std::cmp::Ordering; // 枚举类型 fn main() { println!(\u0026#34;猜数游戏\u0026#34;); // 宏 let secret_number = rand::thread_rng().gen_range(1..101); // 包括1，不包括101 loop { println!(\u0026#34;猜测一个数字:\u0026#34;); let mut guess = String::new(); // mut 可变变量 io::stdin().read_line(\u0026amp;mut guess).expect(\u0026#34;failed to read line\u0026#34;); // shadow， 使用相同变量名进行覆盖 let guess: u32 = guess.trim().parse().expect(\u0026#34;please type a number!\u0026#34;); // 字符串转整数类型 println!(\u0026#34;猜测的数字是： {}\u0026#34;, guess); match guess.cmp(\u0026amp;secret_number) { Ordering::Less =\u0026gt; println!(\u0026#34;Too small\u0026#34;), Ordering::Greater =\u0026gt; println!(\u0026#34;Too big\u0026#34;), Ordering::Equal =\u0026gt; { println!(\u0026#34;You win!\u0026#34;); break; } } } } ","date":"2023-09-28T16:31:03+08:00","permalink":"https://x-xkang.com/p/rust%E5%AD%A6%E4%B9%A0%E4%B8%89--%E7%8C%9C%E6%95%B0%E6%B8%B8%E6%88%8F/","title":"Rust学习（三）-- 猜数游戏"},{"content":"Cargo Cargo是Rust的构建系统和包管理工具\n构建代码、下载依赖的库、构建这些库\u0026hellip; 安装Rust的时候会安装Cargo\n执行cargo --version 查看是否安装 使用Cargo 创建项目 执行 cargo new hello_cargo 1 cargo new hello_cargo 项目名称也是 hello_cargo 创建新目录 hello_cargo Cargo.toml src 目录 main.rs 初始化了一个新的Git仓库, .gitnore 可以使用其他的VCS或不使用VCS： cargo new 的时候使用 --vcs 这个 flag Cargo.toml toml (Tom\u0026rsquo;s Obvious,Minimal Language)格式，是Cargo的配置格式 name: 项目名 version: 项目版本 edition: 使用的Rust版本 [dependencies]：另一个区域的开始\n它会列出项目的依赖项 在Rust里面，代码的包称作crate\nscr/main.rs cargo 生成的 main.rs 在 src 目录下 而Cargo.toml在项目根目录下 源代码都应该在src目录下 项目根目录下可以放置： README、许可信息、配置文件和其他与程序源码无关的文件 如果创建项目时没有使用cargo，也可以把项目转化为使用 cargo: 把源代码文件移动到src下 创建 Cargo.toml 并填写相应的配置 构建 Cargo 项目 cargo build\n创建可执行文件： target/debug/hello_cargo 或target\\debug\\hello_cargo.exe (Windows) 运行可执行文件： ./target/debug/hello_cargo 或 .\\target\\debug\\hello_cargo.exe (Windows) 第一次运行 cargo build 会在顶层目录生成 cargo.lock 文件\n该文件负责追踪项目依赖的精确版本 不需要手动修改该文件 构建和运行 cargo 项目 cargo run, 编译代码 + 执行结果 如果之前编译成功过，并且源码没有修改，那么就会直接运行二进制文件 cargo check cargo check，检查代码，确保能通过编译，但是不产生任何可执行文件 cargo check 要比 cargo build 快的多 编写代码的时候可以连续反复的使用cargo check 检查代码，提高效率 为发布构建 cargo build \u0026ndash;release 编译时会进行优化 代码会运行的更快，但是编译时间更长 会在target/release 而不是 target/debug 生成可执行文件 尽量使用 Cargo","date":"2023-09-28T15:03:17+08:00","permalink":"https://x-xkang.com/p/rust%E5%AD%A6%E4%B9%A0%E4%BA%8C--%E5%88%9D%E8%AF%86cargo/","title":"Rust学习（二）-- 初识Cargo"},{"content":"为什么要用Rust Rust 是一种令人兴奋的新编程语言，他可以让每个人编写可靠且搞笑的软件。 它可以用来替换 C/C++,Rust 和它们具有同样的性能，但是很多常见的bug在编译时就可以被消灭 Rust是一种通用的编程语言，但是它们更善于以下场景： 需要运行时的速度 需要内存安全 更好的利用多核处理器 与其他语言比较 C/C++性能非常好，但类型系统和内存都不太安全。 Java/C#，拥有GC，能保证内存安全，也有很多优秀特性，但是性能不行。 Rust: 安全 无需GC 易于维护、调试、代码安全高效 Rust擅长的领域 高性能 web-service WebAssembly 命令行工具 网络编程 嵌入式设备 系统编程 Rust 和Firefox Rust 最初是Mozilla公司的一个研究性项目。FIrefox是Rust产品应用的一个重要的例子。 Mozilla一直以来都在用Rust创建一个名为Servo的实验性浏览器引擎，其中的所有内容都是并行执行的。 目前Servo的部分功能已经被集成到Firefox里面了 Firefox原来的量子版就包含了Serve的CSS渲染引擎 Rust使得Firefox在这方面得到了巨大的性能改进 Rust 的用户和案例 Google：新操作系统Fuschia，其中Rust代码量大约占30% Amazon: 基于Linux开发的直接可以在裸机、虚拟机上运行容器的操作系统 System76: 纯Rust开发了下一代安全操作系统Redox 蚂蚁金服：库操作系统Occlum 斯坦福和密歇根大学：嵌入式实时操作系统，应用于Google的加密产品 微软： 正在使用Rust重写Windows系统中的一些低级组件 微软：WinRT/Rust项目 Dropbox、Yelp、Coursera、LINE、Cloudflare、Atlassian、npm、Ceph、百度、华为、Sentry、Deno\u0026hellip; Rust 的优点 高性能 安全性 无所畏惧的并发 Rust 的缺点 学习曲线陡峭 【注意】\nRust有很多独有的概念。它们和现在大多主流语言不同，所以学习Rust必须从基础概念一步一步学，否则会懵。 安装Rust 安装：https://www.rust-lang.org/\nLinux or Mac:\ncurl https://sh.rustup.rs -sSf | sh Windows : 按官网指示操作\nWindows Subsystem for Linux(WSL):\ncurl \u0026ndash;proto \u0026lsquo;=https\u0026rsquo; \u0026ndash;tlsv1.2 -sSf https://sh.rustup.rs | sh ","date":"2023-09-28T11:02:54+08:00","permalink":"https://x-xkang.com/p/rust%E5%AD%A6%E4%B9%A0%E4%B8%80--%E5%9F%BA%E6%9C%AC%E4%BA%86%E8%A7%A3/","title":"Rust学习（一）-- 基本了解"},{"content":"二叉树遍历方式有三种\n前序遍历 先遍历根节点，再遍历左子树，再遍历右子树\n中序遍历 先遍历左子树，再遍历根节点，最后遍历右子树\n后序遍历 先遍历左子树，再遍历右子树，再遍历根节点\n其实不难发现，遍历方式是根据根节点遍历的顺序来定义的\n演示代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 package main type TreeNode struct { Left *TreeNode Right *TreeNode Val int } func main(){ root := \u0026amp;TreeNode{ Left: \u0026amp;TreeNode{ Left: \u0026amp;TreeNode{Val: 4}, Right: \u0026amp;TreeNode{Val: 5}, Val: 2, }, Right: \u0026amp;TreeNode{ Val: 3, Left: \u0026amp;TreeNode{Val: 6}, Right: \u0026amp;TreeNode{Val: 7, }, }, Val: 1, } preorder(root) } // 前序遍历 func preorder(root *TreeNode){ if root == nil { return } fmt.Println(\u0026#34;vav:\u0026#34;, root.Val) preorder(root.Left) preorder(root.Right) } // 中序遍历 func inorder (root *TreeNode) { if root == nil { return } inorder(root.Left) fmt.Println(\u0026#34;inorder:\u0026#34;, root.Val) inorder((root.Right)) } // 后序遍历 func postorder(root *TreeNode) { if root == nil { return } postorder(root.Left) postorder(root.Right) fmt.Println(\u0026#34;postorder:\u0026#34;, root.Val) } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 前序遍历输出： val: 1 val: 2 val: 4 val: 5 val: 3 val: 6 val: 7 中序遍历输出： inorder: 4 inorder: 2 inorder: 5 inorder: 1 inorder: 6 inorder: 3 inorder: 7 后序遍历输出： postorder: 4 postorder: 5 postorder: 2 postorder: 6 postorder: 7 postorder: 3 postorder: 1 ","date":"2023-09-22T10:33:53+08:00","permalink":"https://x-xkang.com/p/%E4%BA%8C%E5%8F%89%E6%A0%91%E9%81%8D%E5%8E%86/","title":"二叉树遍历"},{"content":"设计原理 Go 语言中最常见的、也是经常被人提及的设计模式就是：不要通过共享内存的方式进行通信，而是应该通过通信的方式共享内存。在很多主流的编程语言中，多个线程传递数据的方式一般都是共享内存，为了解决线程竞争，我们需要限制同一时间能够读写这些变量的线程数量，然而这与 Go 语言鼓励的设计并不相同。\n多线程使用共享内存传递数据\n虽然我们在 Go 语言中也能使用共享内存加互斥锁进行通信，但是 Go 语言提供了一种不同的并发模型，即通信顺序进程（Communicating sequential processes，CSP）1。Goroutine 和 Channel 分别对应 CSP 中的实体和传递信息的媒介，Goroutine 之间会通过 Channel 传递数据\nGoroutine 使用 Channel 传递数据\n上图中的两个 Goroutine，一个会向 Channel 中发送数据，另一个会从 Channel 中接收数据，它们两者能够独立运行并不存在直接关联，但是能通过 Channel 间接完成通信。\n先入先出 目前的 Channel 收发操作均遵循了先进先出的设计，具体规则如下：\n先从 Channel 读取数据的 Goroutine 会先接收到数据； 先向 Channel 发送数据的 Goroutine 会得到先发送数据的权利； 无锁管道 锁是一种常见的并发控制技术，我们一般会将锁分成乐观锁和悲观锁，即乐观并发控制和悲观并发控制，无锁（lock-free）队列更准确的描述是使用乐观并发控制的队列。乐观并发控制也叫乐观锁，很多人都会误以为乐观锁是与悲观锁差不多，然而它并不是真正的锁，只是一种并发控制的思想。\n悲观并发控制与乐观并发控制\n乐观并发控制本质上是基于验证的协议，我们使用原子指令 CAS（compare-and-swap 或者 compare-and-set）在多线程中同步数据，无锁队列的实现也依赖这一原子指令。\nChannel 在运行时的内部表示是 runtime.hchan，该结构体中包含了用于保护成员变量的互斥锁，从某种程度上说，Channel 是一个用于同步和通信的有锁队列，使用互斥锁解决程序中可能存在的线程竞争问题是很常见的，我们能很容易地实现有锁队列。\n然而锁导致的休眠和唤醒会带来额外的上下文切换，如果临界区过大，加锁解锁导致的额外开销就会成为性能瓶颈。1994 年的论文 Implementing lock-free queues 就研究了如何使用无锁的数据结构实现先进先出队列，而 Go 语言社区也在 2014 年提出了无锁 Channel 的实现方案，该方案将 Channel 分成了以下三种类型：\n同步Channel - 不需要缓冲区，发送方会直接将数据交给（Handoff）接收方； 异步Channel - 基于环形缓存的传统生产者消费者模型； chan struct{} 类型的异步Channel - struct{}类型不占用内存空间，不需要实现缓冲区和直接发送(Handoff)的语义。 这个提案的目的也不是实现完全无锁的队列，只是在一些关键路径上通过无锁提升 Channel 的性能。社区中已经有无锁 Channel 的实现，但是在实际的基准测试中，无锁队列在多核测试中的表现还需要进一步的改进。\n数据结构 源码位置：src/runtime/chan.go#L33\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 type hchan struct { qcount uint // total data in the queue， 队列中的元素数量 dataqsiz uint // size of the circular queue， 底层循环数组的长度 buf unsafe.Pointer // points to an array of dataqsiz elements， 指向底层循环数组的指针，只针对有缓冲区的 channel elemsize uint16 // channel 中的元素大小 closed uint32 // channel是否被关闭的标识 elemtype *_type // element type ，channel中元素类型 sendx uint // send index，已发送元素在循环数组中的索引 recvx uint // receive index，已接收元素在数组中的索引 recvq waitq // list of recv waiters，等待接收的 `goroutine` 队列 sendq waitq // list of send waiters，等待发送的 `goroutine` 队列 // lock protects all fields in hchan, as well as several // fields in sudogs blocked on this channel. // // Do not change another G\u0026#39;s status while holding this lock // (in particular, do not ready a G), as this can deadlock // with stack shrinking. lock mutex // 保护 channel 中的所有字段 } 字段解释 qcount: 队列中的元素数量\ndataqsiz: 底层循环数组的长度\nbuf: 指向底层循环数组的指针，只针对有缓冲区的 channel\nelemsize: channel 中的元素数据类型大小\nclosed: channel是否被关闭的标识\nelemtype: channel中的元素类型\nsendx: 已发送元素在循环数组中的索引\nrecvx: 已接收元素在数组中的索引\nrecvq: 等待接收的goroutine 队列\nsendq: 等待发送的goroutine 队列\nlock: 保护channel 中所有字段，保证每个读或者写channel都是原子的。\nsendq 和 recvq 存储了当前Channel由于缓冲区空间不足二阻塞的 Goroutine 列表，这些等待队列使用双向链表runtime.waitq表示，结构如下：\n1 2 3 4 type waitq struct { first *sudog last *sudog } runtime.sudog 表示一个在等待列表中的Goroutine，该结构中存储了两个分别指向前后runtime.sudog的指针以构成链表。\n创建管道 语法如下：\n1 2 3 4 5 // 无缓冲通道 ch1 := make(chan int) // 有缓冲通道 ch2 := make(chan int, 2); // 创建一个缓冲区长度为2，元素类型为 int 的`channel`，若未指定缓冲区长度，则默认为0 Go 语言中所有Channel的床架都会使用 make关键字。编译器会将make(chan int, 10)表达式转换成 OMAKE类型的节点，并在类型检查阶段将OMAKE类型的节点转换成OMAKECHAN类型:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 func typecheck1(n *Node, top int) (res *Node) { switch n.Op { case OMAKE: ... switch t.Etype { case TCHAN: l = nil if i \u0026lt; len(args) { // 带缓冲区的异步 Channel ... n.Left = l } else { // 不带缓冲区的同步 Channel n.Left = nodintconst(0) } n.Op = OMAKECHAN } } } 这一阶段会对传入的make关键字的缓冲区大小进行检查，如果我们不向make传递表示缓冲区大小参数，那么就会设置一个默认值0，也就是当前的Channel不存在缓冲区。\nOMAKECHAN类型的节点最终都会在SSA中间代码生成阶段之前被转换成调用runtime.makechan或者runtime.makechan64函数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func walkexpr(n *Node, init *Nodes) *Node { switch n.Op { case OMAKECHAN: size := n.Left fnname := \u0026#34;makechan64\u0026#34; argtype := types.Types[TINT64] if size.Type.IsKind(TIDEAL) || maxintval[size.Type.Etype].Cmp(maxintval[TUINT]) \u0026lt;= 0 { fnname = \u0026#34;makechan\u0026#34; argtype = types.Types[TINT] } n = mkcall1(chanfn(fnname, 1, n.Type), n.Type, init, typename(n.Type), conv(size, argtype)) } } runtime.makechan 和 runtime.makechan64 会根据传入的参数类型和缓冲区大小创建一个新的 Channel 结构，其中后者用于处理缓冲区大小大于 2 的 32 次方的情况，因为这在 Channel 中并不常见，所以我们重点关注 runtime.makechan：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 func makechan(t *chantype, size int) *hchan { elem := t.elem mem, _ := math.MulUintptr(elem.size, uintptr(size)) var c *hchan switch { case mem == 0: c = (*hchan)(mallocgc(hchanSize, nil, true)) c.buf = c.raceaddr() case elem.kind\u0026amp;kindNoPointers != 0: c = (*hchan)(mallocgc(hchanSize+mem, nil, true)) c.buf = add(unsafe.Pointer(c), hchanSize) default: c = new(hchan) c.buf = mallocgc(mem, elem, true) } c.elemsize = uint16(elem.size) c.elemtype = elem c.dataqsiz = uint(size) return c } 上述代码根据 Channel 中收发元素的类型和缓冲区的大小初始化 runtime.hchan 和缓冲区：\n如果当前Channel中不存在缓冲区，那么就只会为runtime.hchan分配一段内存空间； 如果当前Channel中存储的类型不是指针类型，会为当前的Channel和底层的数组分配一块连续的内存空间； 在默认情况下会单独为runtime.hchan和缓冲区分配内存。 在函数的最后会统一更新runtime.hchan的elemsize、elemtype和datasize几个字段。\n源码位置：src/runtime/chan.go#L72\n发送数据 我们想要向Channel发送数据时，就需要使用 ch \u0026lt;- i语句，编译器会将它解析成OSEND节点并在cmd/compile/internal/gc.walkexpr中转换成runtime.chansend1:\n1 2 3 4 5 6 7 8 9 10 func walkexpr(n *Node, init *Nodes) *Node { switch n.Op { case OSEND: n1 := n.Right n1 = assignconv(n1, n.Left.Type.Elem(), \u0026#34;chan send\u0026#34;) n1 = walkexpr(n1, init) n1 = nod(OADDR, n1, nil) n = mkcall1(chanfn(\u0026#34;chansend1\u0026#34;, 2, n.Left.Type), nil, init, n.Left, n1) } } chansend1只是调用了runtime.chansend并传入Channel和需要发送的数据。chansend是向Channel 中发送数据时一定会调用的函数，该函数包含了发送数据的全部逻辑，如果我们在调用时将block参数设置成true,那么表示当前发送操作是阻塞的。 源码位置：src/runtime/chan.go#L160\n1 2 3 4 5 6 7 func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool { lock(\u0026amp;c.lock) if c.closed != 0 { unlock(\u0026amp;c.lock) panic(plainError(\u0026#34;send on closed channel\u0026#34;)) } 在发送数据的逻辑执行之前会先为当前 Channel 加锁，防止多个线程并发修改数据。如果 Channel 已经关闭，那么向该 Channel 发送数据时会报 “send on closed channel” 错误并中止程序。\n因为runtime.chansend函数的实现比较复杂，所以我们将该函数的执行过程分为以下三部分：\n当存在等待的接收者，通过runtime.send直接将数据发送给阻塞的接收者； 当缓冲区存在空余空间时，将发送的数据写入Channel的缓冲区； 当不存在缓冲区或者缓冲区已满，等待其他 Goroutine从Channel接收数据。 直接发送\n如果目标Channel没有被关闭并且已经有处于读等待的Goroutine,那么runtime.chansend会从接收队列recvq中取出最先陷入等待的Goroutine并直接向它们发送数据：\n1 2 3 4 if sg := c.recvq.dequeue(); sg != nil { send(c, sg, ep, func() { unlock(\u0026amp;c.lock) }, 3) return true } 发送数据时会调用runtime.send，该函数的执行可以分为两个部分：\n调用runtime.sendDirect将发送的数据直接拷贝到x = \u0026lt;-c表达式中变量x所在的内存地址上； 调用runtime.goready将等待接收数据的Goroutine标记成可运行状态Grunnable并把该Goroutine放到发送方所在的处理器runnext上等待执行，该处理器在下一次调度时会立刻唤醒数据的接收方； 1 2 3 4 5 6 7 8 9 10 func send(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int) { if sg.elem != nil { sendDirect(c.elemtype, sg, ep) sg.elem = nil } gp := sg.g unlockf() gp.param = unsafe.Pointer(sg) goready(gp, skip+1) } 需要注意的是，发送数据的过程只是将接收方的Goroutine放到了处理器的runnext中，程序没有立刻执行该Goroutine。\n缓冲区\n如果创建的Channel包含缓冲区并且Channel中的数据没有装满，会执行下面这段代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool { ... if c.qcount \u0026lt; c.dataqsiz { qp := chanbuf(c, c.sendx) typedmemmove(c.elemtype, qp, ep) c.sendx++ if c.sendx == c.dataqsiz { c.sendx = 0 } c.qcount++ unlock(\u0026amp;c.lock) return true } ... } 在这里我们首先会使用 runtime.chanbuf 计算出下一个可以存储数据的位置，然后通过 runtime.typedmemmove 将发送的数据拷贝到缓冲区中并增加 sendx 索引和 qcount 计数器。\n如果当前Channel的缓冲区未满，向Channel发送的数据会存储在Channel的sendx索引所在的位置，并将sendx索引加1，，因为这里的buf是一个循环数组，所以当sendx等于dataqsiz时会重新回到数组开始的位置。\n阻塞发送\n当Channel没有接收者能够处理数据时，向Channel发送数据会被下游阻塞，当然使用select关键字可以向Channel非阻塞的发送消息。向Channel阻塞地发送数据会执行下面的代码。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool { ... if !block { unlock(\u0026amp;c.lock) return false } gp := getg() mysg := acquireSudog() mysg.elem = ep mysg.g = gp mysg.c = c gp.waiting = mysg c.sendq.enqueue(mysg) goparkunlock(\u0026amp;c.lock, waitReasonChanSend, traceEvGoBlockSend, 3) gp.waiting = nil gp.param = nil mysg.c = nil releaseSudog(mysg) return true } 调用 runtime.getg 获取发送数据使用的 Goroutine； 执行 runtime.acquireSudog 获取 runtime.sudog 结构并设置这一次阻塞发送的相关信息，例如发送的 Channel、是否在 select 中和待发送数据的内存地址等； 将刚刚创建并初始化的 runtime.sudog 加入发送等待队列，并设置到当前 Goroutine 的 waiting 上，表示 Goroutine 正在等待该 sudog 准备就绪； 调用 runtime.goparkunlock 将当前的 Goroutine 陷入沉睡等待唤醒； 被调度器唤醒后会执行一些收尾工作，将一些属性置零并且释放 runtime.sudog 结构体； 函数在最后会返回 true 表示这次我们已经成功向 Channel 发送了数据。\n小结\n我们在这里可以简单梳理和总结一下使用 ch \u0026lt;- i 表达式向 Channel 发送数据时遇到的几种情况：\n如果当前 Channel 的 recvq 上存在已经被阻塞的 Goroutine，那么会直接将数据发送给当前 Goroutine 并将其设置成下一个运行的 Goroutine； 如果 Channel 存在缓冲区并且其中还有空闲的容量，我们会直接将数据存储到缓冲区 sendx 所在的位置上； 如果不满足上面的两种情况，会创建一个 runtime.sudog 结构并将其加入 Channel 的 sendq 队列中，当前 Goroutine 也会陷入阻塞等待其他的协程从 Channel 接收数据； 发送数据的过程中包含几个会触发Goroutine调度的时机：\n发送数据时发现 Channel 上存在等待接收数据的 Goroutine，立刻设置处理器的 runnext 属性，但是并不会立刻触发调度; 发送数据时并没有找到接收方并且缓冲区已经满了，这时会将自己加入 Channel 的 sendq 队列并调用 runtime.goparkunlock 触发 Goroutine 的调度让出处理器的使用权； 接收数据 接收数据有两种写法，一种是只值返回接收数据，第二种是返回接收数据和channel的关闭状态两个字段，当接收到响应类型的零值时需要判断是真实的发送者发送的数据，还是channel被关闭后，返回给接收者的默认类型的零值，可以使用第二种返回channel的关闭状态。\n1 2 i := \u0026lt;- ch i, ok := \u0026lt;- ch 这两种不同的方法经过编译器的处理都会变成 ORECV 类型的节点，后者会在类型检查阶段被转换成 OAS2RECV 类型。虽然不同的接收方式会被转换成 runtime.chanrecv1 和 runtime.chanrecv2 两种不同函数的调用，但是这两个函数最终还是会调用 runtime.chanrecv。\n当我们从一个空 Channel 接收数据时会直接调用 runtime.gopark 让出处理器的使用权。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool) { if c == nil { if !block { return } gopark(nil, nil, waitReasonChanReceiveNilChan, traceEvGoStop, 2) throw(\u0026#34;unreachable\u0026#34;) } lock(\u0026amp;c.lock) if c.closed != 0 \u0026amp;\u0026amp; c.qcount == 0 { unlock(\u0026amp;c.lock) if ep != nil { typedmemclr(c.elemtype, ep) } return true, false } ","date":"2023-03-29T11:08:08+08:00","permalink":"https://x-xkang.com/p/golang--channel/","title":"Golang -- Channel"},{"content":"概述 主从复制是指将主数据库的DDL和DML操作通过二进制日志传到从库服务器中，然后从库上对这些日志重新执行（也叫重做），从而使得从库和主库的数据保持同步。\nMySQL支持一台主库同时向多台从库进行复制，从库同时也可以作为其他从服务器的主库，实现链状复制。\nMySQL复制的优点主要包括一下三个方面：\n主库出现问题，可以快速切换到从库提供服务。 实现读写分离，降低主库的访问压力。 可以在从库中执行备份，以避免备份期间影响主库服务。 原理 MySQL主从复制的原理如下：\n从上图来看，复制分成3步：\nMaster主库在事务提交时，会把数据变更记录在二进制日志文件中binlog中。 从库读取主库的二进制日志文件binlog，写入到从库的中继日志relay log。 Slave重做中继日志中的事件，将改变反应到它自己的数据上。 ","date":"2023-03-23T09:46:34+08:00","permalink":"https://x-xkang.com/p/mysql--%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/","title":"Mysql -- 主从复制"},{"content":" 介绍\n二进制日志（BINLOG）记录了所有的DDL（数据定义语言）语句和DML（数据操纵语言）语句，但不包括数据查询（SELECT、SHOW）语句。\n作用：\n灾难时的数据恢复； MySQL的主从复制。在MySQL8.x版本中，默认二进制日志是开启着的。涉及到的参数如下 日志格式\nMySQL服务器中提供了多种格式来记录二进制日志，具体格式及特点如下：\n日志格式 含义 STATEMENT 基于SQL语句的日志记录，记录的是SQL语句，对数据进行修改的SQL都会记录在日志文件中 ROW 基于行的日志记录，记录的是每一行的数据变更。（默认） MIXED 混合了STATEMENT 和ROW两种格式，默认采用STATEMENT，在某些特殊条件下回自动切换为ROW进行记录 1 show variables like \u0026#39;%binlog_format%\u0026#39;; 日志查看\n由于日志是以二进制方式存储的，不能直接读取，需要通过二进制日志查询工具 mysqlbinlog 来查看，具体语法如下：\nmysqlbinlog [参数选项] logfilename\n参数选项： -d 指定数据库名称，只列出指定的数据库相关操作。 -o 忽略掉日志中的 n 行命令。 -v 将行事件（数据变更）重构为SQL语句\n日志删除\n对于比较繁忙的业务系统，每天生成的binlog数据巨大，如果长时间不清除，将会占用大量磁盘空间，可以通过一下几种方式清理日志：\n指令 含义 reset master 删除全部 binlog 日志，删除之后日志编号将从 binlog.000001重新开始 purge master logs to 'binlog.******' 删除 ****** 编号之前的所有日志 purge master logs before 'yyyy-mm-dd hh24:mi:ss' 删除日志为 yyyy-mm-dd hh24:mi:ss 之前产生的所有日志 注：也可以在mysql配置文件中配置二进制日志的过期时间，设置了之后，二进制日志过期将会自动删除。\n1 show variables like \u0026#39;%binlog_expire_logs_seconds%\u0026#39;; ","date":"2023-03-22T20:50:30+08:00","permalink":"https://x-xkang.com/p/mysql--binlog/","title":"Mysql -- Binlog"},{"content":" 介绍\n触发器是与表有关的数据库对象，指在insert/update/delete之前或之后，触发并执行触发器中定义的SQL语句集合，触发器的这种特性可以协助应用在数据库端确保数据的完整性、日志记录、数据校验等操作。 使用别名OLD和NEW来引用触发器中发生变化的记录内容，这与其他的数据库是相似的。现在触发器还只支持行级触发，不支持语句级触发。\n触发器类型 NEW 和 OLD INSERT 型触发器 NEW 表示将要或者已经新增的数据 UPDATE 型触发器 OLD 表示修改之前的数据，NEW 表示将要或者已经修改的数据 DELETE 型触发器 OLD 表示将要或者已经删除的数据 语法\n创建 1 2 3 4 5 6 CREATE TRIGGER trigger_name BEFORE/AFTER INSERT/UPDATE/DELETE ON tb_name FOR EACH ROW -- 行级触发器 BEGIN trigger_stmt; -- 触发器语句 END; 查看 1 SHOW TRIGGERS; 删除 1 DROP TRIGGER [schema_name.]trigger_name; -- 如果没有指定 schema_name，默认为当前数据库。 ","date":"2023-03-20T15:27:27+08:00","permalink":"https://x-xkang.com/p/mysql--%E8%A7%A6%E5%8F%91%E5%99%A8-trigger/","title":"Mysql -- 触发器 Trigger"},{"content":" 介绍\n存储过程是事先经过编译并存储在数据库中的一段SQL语句的集合，调用存储过程可以简化应用开发人员的很多工作，减少数据在数据库和应用服务器之间的传输，对于提高数据处理的效率是有好处的。 存储过程思想上很简单，就是数据库SQL语言层面的代码封装与重用。\n特点\n封装、复用 可以接收参数，也可以返回数据 减少网络交互，提升效率 创建存储过程\n1 2 3 4 5 6 CREATE PROCEDURE 存储过程名称([参数列表]) BIGEN -- SQL语句 END; 调用 1 CALL 名称([参数]); 查看存储过程 1 2 3 4 5 -- 查询指定数据库的存储过程及状态信息 SELECT * FROM INFORMATION_SCHEMA.ROUTINES WHERE ROUTINE_SCHEMA = \u0026#39;XXX\u0026#39;; -- 查询某个存储过程的定义 SHOW CREATE PROCEDURE 存储过程名称; 删除存储过程 1 DROP PROCEDURE [IF EXISTS] 存储过程名称; 注意：在命令行中，执行创建存储过程的SQL时，需要通过关键字delimiter指定SQL语句的结束符\n1 delimiter $$; 变量 系统变量是MySQL服务器提供，不是用户定义的，属于服务器层面，分为全局变量(GLOBAL)、会话变量(SESSION)。\n查看系统变量 1 2 3 4 5 SHOW [SESSION | GLOBAL] VARIABLES; -- 查看所有系统变量 SHOW [SESSION | GLOBAL] VARIABLES LIKE \u0026#39;...\u0026#39;; -- 可以通过 like 模糊匹配方式查找变量 SELECT @@[SESSION | GLOBAL] 系统变量名; -- 查看指定变量的值 设置系统变量 1 2 3 SET [SESSION | GLOBAL] 系统变量名=值； SET @@[SESSION | GLOBAL] 系统变量名=值; 【待续。。。】\n","date":"2023-03-10T22:35:13+08:00","permalink":"https://x-xkang.com/p/mysql--%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B/","title":"Mysql -- 存储过程"},{"content":" 介绍\n视图（View）是一种虚拟存在的表，视图中经的数据并不在数据库中实际存在，行和列数据来自定义视图的查询中使用的表，并且是在使用视图时动态生成的。 通俗的讲，视图只保存了查询的SQL逻辑，不保存查询结果。所以我们在创建视图的时候，主要的工作就落在创建这条SQL查询语句上。\n作用\n简单 视图不仅可以简化用户对数据的理解，也可以简化他们的操作。那些被经常使用的查询可以被定义为视图，从而使得用户不必为以后的操作每次指定全部的条件\n安全 数据库可以授权，但不能授权到数据库特定行和特定列上。通过视图用户只能查询和修改他们所能见到的数据。\n数据独立 视图可以帮助用户屏蔽真实表结构变化带来的影响。\n语法如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # 创建视图 CREATE OR REPLACE VIEW view_u AS SELECT id, username, password FROM user WHERE id \u0026lt;= 100; # 查询视图 # 1. 查看创建视图语句 SHOW CREATE VIEW view_u; # 2. 查看视图数据 SELECT * FROM VIEW view_u; # 修改视图 # 方式一 # CREATE [OR REPLACE] VIRE 视图名称[(列名列表)] AS SELECT语句 [WITH[CASCADED | LOCAL] CHECK OPTION] CREATE OR REPLACE VIEW view_u AS SELECT id, username FROM `user` WHERE id \u0026lt;= 10; # 方式二 # ALTER VIEW 视图名称[(列名列表)] AS SELECT 语句 [WITH[CASCADED | LOCAL] CHECK OPTION] ALTER VIEW view_u AS SELECT id, username FROM `user` WHERE id \u0026lt;= 20; # 删除视图 # DROP VIEW [IF EXISTS] 视图名称 [, 视图名称]... DROP VIEW IF EXISTS view_u; 视图的检查选项 当使用WITH CHECK OPTION 子句创建视图时，MySQL会通过视图检查正在更改的每个行，例如插入、更新、删除，以使其符合视图的定义，MySQL允许基于另一个视图创建视图，它还会检查依赖视图中的规则以保持一致性，为了确定检查的范围，mysql提供了两个选项：CASCADED 和 LOCAL，默认值为CASCADED。\n1 2 3 4 5 # 基于 tbale 创建视图 create or replace view view_u as select id, username from `user` where id \u0026lt;= 10 with cascaded check option; # 基于 view 创建视图 create or replace view view_u_2 as select id, username from `view_u` where id \u0026lt;= 10 with cascaded check option; cascaded:在创建视图时若使用 cascaded关键字，插入或更新数据时不仅会校验当前视图的约束条件，也会校验关联的父视图和父视图引用的视图，以此类推。若使用local关键字，则只会校验当前视图的约束条件。\n视图的更新 要使视图可更新，视图中的行与基础表中的行之间必须存在一对一的关系。如果视图包含以下任何一项，则该视图不可更新：\n聚合函数或窗口函数(SUM(), MIN(), MAX(), COUNT() 等) DISTINCT GROUP BY HAVING UNION 或者 UNION ALL ","date":"2023-03-08T07:36:27+08:00","permalink":"https://x-xkang.com/p/mysql--view%E8%A7%86%E5%9B%BE/","title":"MySQL -- view视图"},{"content":"插入数据 批量插入 1 insert into `user` values(\u0026#39;user_1\u0026#39;, \u0026#39;password_1\u0026#39;), (\u0026#39;user_2\u0026#39;, \u0026#39;password_2\u0026#39;); 手动提交事务 1 2 3 4 5 6 7 8 9 start transaction; insert into `user` values(\u0026#39;user_1\u0026#39;, \u0026#39;password_1\u0026#39;), (\u0026#39;user_2\u0026#39;, \u0026#39;password_2\u0026#39;); insert into `user` values(\u0026#39;user_3\u0026#39;, \u0026#39;password_3\u0026#39;), (\u0026#39;user_4\u0026#39;, \u0026#39;password_4\u0026#39;); insert into `user` values(\u0026#39;user_5\u0026#39;, \u0026#39;password_5\u0026#39;), (\u0026#39;user_6\u0026#39;, \u0026#39;password_6\u0026#39;); commit; 主键顺序插入 1 2 # 主键乱序插入： 8 1 9 21 88 2 4 15 89 5 7 3 # 主键顺序插入： 1 2 3 4 5 7 8 9 15 21 88 89 大批量插入数据 如果一次性插入大批量数据，使用insert语句插入性能较低，此时可以使用MySQL数据库提供的load指令进行插入。操作如下：\n1 2 3 4 5 6 7 8 9 # 客户端连接服务端时，加上参数 --local-infile mysql --local-infile -u root -p #设置全局参数 local_infile为1，开启本地加在文件导入数据的开关 set global local_infile = 1; # 执行load指令将准备好的数据，加在到表结构中 load data local infile \u0026#39;/root/sql1.log\u0026#39; into table `user` fields terminated by \u0026#39;,\u0026#39; lines terminated by \u0026#39;\\n\u0026#39;; # 主键优化 数据组织优化\n在InnoDB存储引擎中，表数据都是根据主键顺序组织存放的，这种存储方式的表称为索引组织表(index organized table IOT)。\n页分裂\n页可以为空，也可以填充一半，也可以填充100%，每个页包含了2-N行数据（如果一行数据过大，会行溢出），根据主键排列。\n乱序插入时，若插入数据在当前页存储不下时，会将当前页的一半数据移动至新的数据页，然后将待插入数据存储至当前页，并将当前页的下一页指针指向新开辟的数据页。\n页合并\n当删除一行数据时，实际上记录并没有被物理删除，只是记录被标记（flaged）为删除并且它的空间变的允许被其他记录声明使用。当页中删除的记录大奥 MERGE_THRESHOLD(默认为页的50%)，Innodb会开始寻找最靠近的页（前或后）看看是否可以将两个页合并以优化空间使用。\n主键设计原则\n满足业务需求的情况下，尽量降低主键的长度。 插入数据时，尽量选择顺序插入，选择使用 AUTO_INCREMENT自增主键。 尽量不要使用UUID做主键或者是其他自然主键，如身份证号。 ORDER BY 优化 Using filesort: 通过表的索引或全表扫描，读取满足条件的数据行，然后在排序缓冲区 sort buffer 中完成排序操作，所有不是通过索引值直接返回排序结果的排序都叫 FileSort 排序。 Using index: 通过有序索引顺序扫描直接返回有序数据，这种情况即为 using index，不需要额外排序，操作效率高。\nGROUP BY 优化 在分组操作时，可以通过索引来提高效率。 分组操作时，索引的使用也是满足最左前缀法则的。 LIMIT 优化 一个常见又头疼的问题就是limit 2000000, 10,此时需要MySQL排序前2000010记录，仅仅返回2000000 - 2000010的记录，其他记录丢弃，查询排序的代价非常大。\n优化思路：一般分页查询时，通过创建覆盖索引能够较好地提高性能，可以通过覆盖索引加子查询形式进行优化。\n优化前：\n1 select * from user limit 2000000, 10; 优化后：\n1 select u.* from user u, (select id from user order by id limit 2000000, 10) a where u.id = a.id; COUNT 优化 MyISAM 引擎把一个表的总行数存在了磁盘上，因此执行count(*)的时候会直接返回这个数，效率很高； Innodb 引擎就麻烦了，它执行count(*)的时候，需要把数据一行一行地从引擎中读出来，然后累计计数。 优化思路：自己计数。（基于内存的k-v缓存）\ncount 的几种用法 count()是一个聚合函数，对于返回的结果集，一行行的判断，如果count函数的参数不是NULL，累计值就加 1，否则不加，最后返回累计值。 用法：count(*), count(主键), count(字段), count(1) count(主键) Innodb引擎会遍历整张表，把每一行的主键ID值都取出来，返回给服务层。服务层拿到主键后，直接按行进行累加（主键不可能为null）。 count(字段) 没有 not null 约束：Innodb引擎会遍历整张表把每一行的字段值都取出来，返回给服务层，判断值是否为null， 不为null，计数累加。 有 not null 约束：Innodb引擎会遍历正常标把每一行的字段都取出来，返回给服务层，直接按行进行累加。 UPDATE 优化 1 update student set no = \u0026#39;2000100100\u0026#39; where id = 1; 1 update student set no = \u0026#39;2000100105\u0026#39; where name = \u0026#39;韦一笑\u0026#39;; Innodb的行锁是针对索引加的锁，不是针对记录加的锁，并且该索引不能失效，否则会从行锁升级为表锁。\n","date":"2023-03-06T20:44:07+08:00","permalink":"https://x-xkang.com/p/mysql--sql%E4%BC%98%E5%8C%96/","title":"MySQL -- SQL优化"},{"content":"最左前缀法则 如果索引了多列（联合索引），要遵守最左前缀法则。最左前缀法则指的是查询从索引的最左列开始，并且不跳过索引中的列。如果跳过某一列，索引将部分失效（后面的字段索引失效）。 假设user表有username varchar(50)、phone varchar(20)、email varchar(50)三个字段组成的联合索引idx_user_username_phone_email\n按照最左前缀法则有以下情况\n使用索引最左侧字段进行条件查询，命中索引，且索引长度为152，表示只用到索引中的username字段\n1 2 3 4 5 6 7 mysql\u0026gt; explain select * from `user` where `username` = \u0026#39;Bob\u0026#39;; +----+-------------+-------+------------+------+-------------------------------+-------------------------------+---------+-------+------+----------+-------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+-------------------------------+-------------------------------+---------+-------+------+----------+-------+ | 1 | SIMPLE | user | NULL | ref | idx_user_username_phone_email | idx_user_username_phone_email | 152 | const | 1 | 100.00 | NULL | +----+-------------+-------+------------+------+-------------------------------+-------------------------------+---------+-------+------+----------+-------+ 使用索引中的全部三个字段，命中索引 且key_len为366，说明三个字段全部命中\n1 2 3 4 5 6 7 mysql\u0026gt; explain select * from `user` where `username` = \u0026#39;Bob\u0026#39; and `phone`=\u0026#39;18888888888\u0026#39; and `email`=\u0026#39;1888888888@163.com\u0026#39;; +----+-------------+-------+------------+------+-------------------------------+-------------------------------+---------+-------------------+------+----------+-------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+-------------------------------+-------------------------------+---------+-------------------+------+----------+-------+ | 1 | SIMPLE | user | NULL | ref | idx_user_username_phone_email | idx_user_username_phone_email | 366 | const,const,const | 1 | 100.00 | NULL | +----+-------------+-------+------------+------+-------------------------------+-------------------------------+---------+-------------------+------+----------+-------+ 使用 username和email查询，结果 命中索引，但只有username字段是有效的，因为按照最左前缀原则，如果跳过中间列(phone)，则索引将部分失效\n1 2 3 4 5 6 7 mysql\u0026gt; explain select * from `user` where `username` = \u0026#39;Bob\u0026#39; and `email`=\u0026#39;1888888888@163.com\u0026#39;; +----+-------------+-------+------------+------+-------------------------------+-------------------------------+---------+-------+------+----------+-----------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+-------------------------------+-------------------------------+---------+-------+------+----------+-----------------------+ | 1 | SIMPLE | user | NULL | ref | idx_user_username_phone_email | idx_user_username_phone_email | 152 | const | 1 | 10.00 | Using index condition | +----+-------------+-------+------------+------+-------------------------------+-------------------------------+---------+-------+------+----------+-----------------------+ 索引失效情况 使用索引中的第二列phone和第三列email查询，未命中索引，因为查询条件中的字段没有从索引的最左列开始（最左前缀法则）\n1 2 3 4 5 6 7 mysql\u0026gt; explain select * from `user` where `phone`=\u0026#39;18888888888\u0026#39; and `email`=\u0026#39;1888888888@163.com\u0026#39;; +----+-------------+-------+------------+------+---------------+------+---------+------+-------+----------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+---------------+------+---------+------+-------+----------+-------------+ | 1 | SIMPLE | user | NULL | ALL | NULL | NULL | NULL | NULL | 99677 | 1.00 | Using where | +----+-------------+-------+------------+------+---------------+------+---------+------+-------+----------+-------------+ 使用模糊匹配进行查询，未命中索引\n1 2 3 4 5 6 7 mysql\u0026gt; explain select * from `user` where `phone` like \u0026#39;188%\u0026#39;; +----+-------------+-------+------------+------+---------------+------+---------+------+-------+----------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+---------------+------+---------+------+-------+----------+-------------+ | 1 | SIMPLE | user | NULL | ALL | NULL | NULL | NULL | NULL | 99677 | 11.11 | Using where | +----+-------------+-------+------------+------+---------------+------+---------+------+-------+----------+-------------+ 条件语句中使用函数，截取用户名username的前缀进行查询，未命中索引\n1 2 3 4 5 6 7 mysql\u0026gt; explain select * from `user` where substring(username, 1, 3) = \u0026#39;dev\u0026#39;; +----+-------------+-------+------------+------+---------------+------+---------+------+-------+----------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+---------------+------+---------+------+-------+----------+-------------+ | 1 | SIMPLE | user | NULL | ALL | NULL | NULL | NULL | NULL | 99677 | 100.00 | Using where | +----+-------------+-------+------------+------+---------------+------+---------+------+-------+----------+-------------+ 字符串不加单引号''，只命中了最左侧的索引字段 未加单引号的字段 未命中索引\n1 2 3 4 5 6 7 mysql\u0026gt; explain select * from user where username = \u0026#39;bob\u0026#39; and phone = 18895766335; +----+-------------+-------+------------+------+-------------------------------+-------------------------------+---------+-------+------+----------+-----------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+-------------------------------+-------------------------------+---------+-------+------+----------+-----------------------+ | 1 | SIMPLE | user | NULL | ref | idx_user_username_phone_email | idx_user_username_phone_email | 152 | const | 1 | 10.00 | Using index condition | +----+-------------+-------+------------+------+-------------------------------+-------------------------------+---------+-------+------+----------+-----------------------+ OR 作为连接的条件，用OR分开的条件，如果OR前的条件中的列有索引，而后面的列中没有索引，那么涉及的索引都 不会命中。\n1 2 3 4 5 6 7 mysql\u0026gt; explain select * from user where id = 10 or phone = \u0026#39;18895766335\u0026#39;; +----+-------------+-------+------------+------+---------------+------+---------+------+-------+----------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+---------------+------+---------+------+-------+----------+-------------+ | 1 | SIMPLE | user | NULL | ALL | PRIMARY | NULL | NULL | NULL | 99677 | 10.00 | Using where | +----+-------------+-------+------------+------+---------------+------+---------+------+-------+----------+-------------+ 数据分布影响 如果MySQL评估使用索引比全表更慢，则不使用索引。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 结果是使用索引的，因为记录中没有为null的数据，相对整体记录数占比较小，但如果字段属性加了 `not null` 的限制，不管查询条件是is null 还是 is not null，都将不会使用索引 mysql\u0026gt; explain select * from user where username is null; +----+-------------+-------+------------+------+-------------------------------+-------------------------------+---------+-------+------+----------+-----------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+-------------------------------+-------------------------------+---------+-------+------+----------+-----------------------+ | 1 | SIMPLE | user | NULL | ref | idx_user_username_phone_email | idx_user_username_phone_email | 153 | const | 1 | 100.00 | Using index condition | +----+-------------+-------+------------+------+-------------------------------+-------------------------------+---------+-------+------+----------+-----------------------+ # 查询条件为 not null 时没有使用索引。 mysql\u0026gt; explain select * from user where username is not null; +----+-------------+-------+------------+------+-------------------------------+------+---------+------+-------+----------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+-------------------------------+------+---------+------+-------+----------+-------------+ | 1 | SIMPLE | user | NULL | ALL | idx_user_username_phone_email | NULL | NULL | NULL | 99484 | 50.00 | Using where | +----+-------------+-------+------------+------+-------------------------------+------+---------+------+-------+----------+-------------+ SQL提示 SQL提示，是优化数据库的一个重要手段，简单来说，就是在SQL语句中加入一些人为的提示来达到优化操作的目的。\nuse index 1 explain select * from user use index(idx_user_username_phone_email) where username = \u0026#39;Bob\u0026#39;; ignore index 1 explain select * from user ignore index(idx_user_username_phone_email) where username=\u0026#39;Bob\u0026#39;; force index 1 explain select * from user force index(idx_user_username_phone_email) where username=\u0026#39;Bob\u0026#39;; 覆盖索引 尽量使用覆盖索引（查询使用了覆盖索引，并且需要返回的列，在该索引中已经全部能够找到），减少select *。\n1 2 3 4 # select 中的字段在索引中都可以直接查到，因此不用回表，若查询字段使用`*`， # 或者加入不在索引中的字段,比如`password`，查询需要先通过联合索引查到二级索引值， # 取出主键ID，根据主键ID再回表查询出`password`字段值。 explain select id, username, email, `password` from user where username = \u0026#39;user_99999\u0026#39;; 前缀索引 当字段类型为字符串（varchar, text等）时，有时候需要索引很长的字符串，这会让索引变的很大，查询时，浪费大量的磁盘IO，影响查询效率。此时可以只将字符串的一部分前缀，建立索引，这样可以大大节约索引空间，从而提高索引效率。\n语法： 1 create index idx_tb_column on tb(column(n)); 前缀长度 可以根据索引的选择性来决定，而选择性是指不重复的索引值（基数）和数据表的记录总数的比值，索引选择性越高则查询效率越高，唯一索引的选择性是1，这是最好的索引选择性，性能也是最好的。可根据一下结果做参考：\n1 2 3 select count(distinct email) / count(*) from user; select count(distinct substring(email, 15)) / count(*) from user; 单列索引和联合索引 单列索引：即一个索引值包含单个列 联合索引：即一个索引包含了多个列 😀 提示： 在业务场景中，如果存在多个查询条件，考虑对查询字段建立索引时，建议建立联合索引，而非单列索引。\n索引设计原则 1.针对数据量较大，且查询比较频繁的表建立索引。 2.针对常作为查询条件（WHERE）、排序（ORDER BY）、分组（GROUP BY）操作的字段建立索引。 3.尽量选择区分度高的列作为索引，尽量建立唯一索引，区分度越高，使用索引的频率越高。 4.如果是字符串类型的字段，字段的长度较长，可以针对字段的特点，建立前缀索引。 5.尽量使用联合索引，减少单列索引，查询时，联合索引很多时候可以覆盖索引，节省存储空间，避免回表，提高查询效率。 6.要控制索引的数量，索引并不是多多益善，索引越多，维护索引结构的代价就越大，会影响增删改的效率。 7.如果索引列不能存储NULL值，请在创建表时使用NOT NULL约束它。当优化器知道每列是否包含NULL值时，它可以更好的确定哪个索引最有效的用于查询。 ","date":"2023-03-01T12:06:34+08:00","permalink":"https://x-xkang.com/p/mysql--%E7%B4%A2%E5%BC%95%E4%BD%BF%E7%94%A8%E8%A7%84%E5%88%99/","title":"MySQL -- 索引使用规则"},{"content":"查看SQL执行频率 MySQL 客户端连接成功后，通过show[session|global] status 命令可以提供服务器状态信息。通过如下指令可以查看当前数据库的INSERT、UPDATE、DELETE、SELECT的访问频次：\n1 SHOW GLOBAL STATUS LIKE \u0026#39;Com_______\u0026#39;; # 一个 \u0026#39;_\u0026#39; 代表一个字符 结果如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 mysql\u0026gt; show global status like \u0026#39;Com_______\u0026#39;; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | Com_binlog | 0 | | Com_commit | 0 | | Com_delete | 0 | | Com_import | 0 | | Com_insert | 0 | | Com_repair | 0 | | Com_revoke | 0 | | Com_select | 8 | | Com_signal | 0 | | Com_update | 0 | | Com_xa_end | 0 | +---------------+-------+ 11 rows in set (0.00 sec) 查询慢日志 慢查询日志记录了所有执行时间超过指定参数（long_query_time，单位：秒，默认10秒）的所有SQL语句的日志。MySQL的慢查询日志默认没有开启，执行一下SQL查看慢日志开启状态：\n1 SHOW VARIABLES LIKE \u0026#39;slow_query_log\u0026#39;; 需要在MySQL的配置文件（/etc/my.cnf）中配置如下信息：\n1 2 3 4 5 # 开启MySQL慢日志查询开关 slow_query_log=1 # 设置慢日志的时间为2秒，SQL语句执行时间炒锅2秒就会视为慢查询，记录慢查询日志 long_query_time=2 重启MySQL服务，本机环境为win10的WSL2，直接执行以下命令：\n1 sudo service mysql restart profile详情 show profiles 能够在做SQL优化时帮助我们了解时间都耗费到哪去了。通过have_profiling参数，能够看到当前MySQL是否支持profile操作：\n1 2 # 查看是否支持 SELECT @@have_profiling; 默认profiling是关闭的，可以通过set语句在session/global级别开启profiling:\n1 2 3 4 5 # 查看开关状态 SELECT @@profiling; # 设置开关状态 SET profiling = 1; 执行完查询语句后再执行show profiles，结果如下：\n1 2 3 4 5 6 7 8 9 +----------+------------+------------------------+ | Query_ID | Duration | Query | +----------+------------+------------------------+ | 1 | 0.00025800 | select @@profiling | | 2 | 0.00125875 | show tables | | 3 | 0.00014975 | select * form user | | 4 | 0.01059300 | select * from user | | 5 | 0.00152175 | select * from user_log | +----------+------------+------------------------+ explain 执行计划 EXPLAIN 或者 DESC 命令获取MySQL如何执行SELECT语句的信息，包括在SELECT语句执行过程中表如何连接和连接的顺序。语法如下：\n1 2 3 # 直接在select语句之前加上关键字 explain/desc EXPLAIN SELECT 字段列表 FROM 表名 WHERE 条件; 结果如下：\n1 2 3 4 5 6 7 mysql\u0026gt; explain select * from user; +----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+ | 1 | SIMPLE | user | NULL | ALL | NULL | NULL | NULL | NULL | 1 | 100.00 | NULL | +----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+ 1 row in set, 1 warning (0.00 sec) EXPLAIN 输出列如下：\nColumn JSON Name Meaning id select_id SELECT 标识符 select_type None SELECT 类型 table table_name 输出行table partitions partitions 匹配的分区 type access_type 连接类型 possible_keys possible_keys 可能的索引选择 key key 实际选择的索引 key_len key_length 所选键的长度 ref ref 与索引比较的列 rows rows 估计要检查的行 filtered filtered 按table条件过滤的行百分比 Extra None 扩展信息 😀 Note 详细解释可参考 [官方文档]\nid 行标识，如果没有子查询或者联合查询，这个值是1，id列值越大越先执行，如果一样大，那么就从上往下依次执行\nselect_type 查询的类型，可以是下表的任何一种类型\nselect_type 类型说明 SIMPLE 简单SELECT（不使用UNION或子查询） PROMARY 最外层的SELECT UNION UNION中第二个或之后的SELECT语句 DEPENDENT UNION UNION中第二个或之后的SELECT语句取决于外面的查询 UNION RESULT UNION的结果 SUBQUERY 子查询中的第一个SELECT DEPENDENT SUBQUERY 子查询中的第一个SELECT，取决于外面的查询 DERIVED 衍生表（FROM自居中的子查询） MATERIALIZED 物化子查询 UNCACHEABLE SUBQUERY 结果集无法缓存的子查询，必须重新评估外部查询的每一行 UNCACHEABLE UNION UNION中第二个或之后的SELECT，属于无法缓存的子查询 ⭐ DEPENDENT 意味着使用了关联子查询\ntable：输出行所引用表的名称，也可以是以下值之一：\n\u0026lt;unionM,N\u0026gt;: 引用id为M和N UNION之后的结果。 : 引用id为N的结果派生出的表，派生表可以是一个结果集，例如派生自FROM中子查询的结果。 : 引用id为N的子查询结果物化得到的表，即生成一个临时表保存子查询的结果。 partitions：v5.7之前该项是explain partitions显示的选项，v5.7之后成为了默认选项，该列显示的为分区表中的分区情况，非分区表该字段为空（null）。\ntype( 重要 )：连接类型\nsystem 表中只有一行数据或者是空表，这是const类型的一个特例，且只能用于MyISAM和Memory表，如果是InnoDB引擎表，type列在这个情况通常都是all或者index\nconst 最多只有一行记录匹配。当联合主键或唯一索引的所有字段跟常量值比较时，join类型为const。其他数据库也叫做唯一索引扫描\neq_ref 多表join时，对于来自前面表的每一行，在当前表中只能找到一行，这可能是除了system和const之外最好的类型，当主键或唯一非NULL索引的所有字段都被当做join连接时会出现此类型。 eq_ref可用于使用\u0026rsquo;=\u0026lsquo;操作符作比较的索引列，比较的值可以是常量，也可以是使用此表之前读取的表的列表达式。\nref 对于来自前面表的每一行，在此表的索引中可以匹配到多行，若连接只用到索引的最左前缀或索引不是主键或唯一索引时，使用 ref 类型（也就是说，此类型能够匹配到多行连接）。 ref 可用于使用 = 或 \u0026lt;=\u0026gt; 操作符作比较的索引列。\nfulltext 用于全文索引的时候是这个类型，要注意，全文索引的优先级很高，若全文索引和普通索引同时存在，mysql不管代价，优先选择使用全文索引\nref_or_null 类似于ref类型，只是增加了null值得比较，实际用的不多\nindex_merge 此类型表示查询使用两个以上索引，最后取交集或者并集，常见 and, or的条件使用了不同的索引，官方排序这个在ref_or_null之后，但是实际上由于要读取多个索引，性能可能大部分时间都不如range。\nunique_subquery 用于WHERE中的 IN 形式子查询，子查询返回不重复值唯一值，可以完全替换子查询，效率更高。 该类型替换了下面形式的 IN 子查询的 ref: value IN (select primary_key FROM single_table WHERE some_expr)\nindex_subquery 该连接类型类似于unique_subquery。适用于非唯一索引，可以返回重复值\nrange 索引范围查询，常见于使用= \u0026lt;\u0026gt; \u0026gt; \u0026gt;= \u0026lt; \u0026lt;= IS NULL \u0026lt;=\u0026gt; BETWEEN IN 或者 LIKE 等运算符的查询中。\nindex 索引全表扫描，把索引从头到尾扫一遍。这里包含两种情况： 一种是查询使用了覆盖索引，那么它只需要扫描索引就可以获得数据，这个效率要比全表扫描要快，因为索引通常比数据表小，而且还能避免二次查询。在 extra 中显示 Using index，反之，如果在索引上进行全表扫描，没有 Using index的提示。\nall 全表扫描，性能较差\npossible_keys：查询可能用到的索引，如果此列是 NULL，则没有用到索引。\nkey：表示MySQL实际决定使用的索引，如果MySQL决定使用其中一个 possible_keys 索引来查找行，则该索引将作为键值列出。\nkey_length：查询用到的索引长度（字节）， 如果是单列索引，那就整个索引长度算进去，如果是多列索引，那么查询不一定都能使用到所有的列，用多少算多少。例如组合索引为idx_username_create_time，当查询语句为 select * from user where username='qqq'时，只用到了username字段，那么key_len只会计算username字段占用的字节数，假如username字段类型为varchar(50) NOT NULL，字符编码为utf8mb3（utf8编码一个字符占用3个字节，gbk编码一个字符占2字节），记录字符串长度占用2字节，记录字段不为空占用1字节（若没有NOT NULL限制则不占用这1个字节）那么key_len为 50 * 3 + 2 + 1 = 153字节\nref：如果使用的是常数等值查询，这里会显示const，如果是连接查询，被驱动表的执行计划这里会显示驱动表的关联字段，如果是条件使用了表达式或者函数，或者条件列表发生了内部隐式转换，这里可能显示为func。\nrows( 重要 ):rows也是一个重要的字段，这是MySQL估算的需要扫描的行数（不是精确值）。这个值非常直观显示SQL的效率好坏，原则上rows越少越好。\nfiltered：该字段表示存储引擎返回的数据在server层过滤后，剩下多少满足查询的记录数量的比例，注意是百分比，不是具体记录数。\nExtra( 重要 )：EXPLAIN中很多额外的信息会在Extra字段显示，常见的有以下：\ndistinct: 在select部分使用了distinct关键字 Using filesort: 当Extra中有Using filesort时，表示MySQL需额外的排序操作，不能通过索引顺序到达排序效果，一般有 Using filesort 都建议优化去掉，因为这样的查询CPU资源消耗较大。 Using index: “覆盖索引扫描”，表示查询在索引树种就可查找所需数据，不用扫描表数据文件，往往说明性能不错。 Using temporary: 查询有使用临时表，一般出现于排序，分组和多表join的情况，查询效率不高，建议优化。 ","date":"2023-02-28T10:29:46+08:00","permalink":"https://x-xkang.com/p/mysql--%E7%B4%A2%E5%BC%95%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/","title":"MySQL -- 索引性能分析"},{"content":"概念 约束是作用于表中字段行的规则，用于限制存储在表中的数据\n目的 保证数据库中数据的正确性、有效性和完整性\n分类 约束 描述 关键字 非空约束 限制该字段的数据不能为null NOT NULL 唯一约束 保证该字段的所有数据都是唯一、不重复的 UNIQUE 主键约束 主键是一行数据的唯一标识，要求非空且唯一 PRIMARY KEY 默认约束 保存数据时，如果未指定该字段的值，则采用默认值 DEFAULT 检查约束（v8.0.16之后） 保证字段值满足某一个条件 CHECK 外键约束 用来让两张表的数据之间建立连接，保证数据的一致性和完整性 FOREIGN KEY 演示 1 2 3 4 5 create table `user_info` ( `id` int(11) not null primary key auto_increment comment \u0026#39;主键ID\u0026#39;, # 非空约束和主键约束 `phone` varchar(32) not null unique default \u0026#39;\u0026#39; comment \u0026#39;手机号码\u0026#39;, # 非空约束和唯一约束 `nickname` varchar(255) default \u0026#39;user\u0026#39; comment \u0026#39;昵称\u0026#39; # 默认约束 ); ","date":"2023-02-28T10:24:41+08:00","permalink":"https://x-xkang.com/p/mysql--%E7%BA%A6%E6%9D%9F/","title":"MySQL -- 约束"},{"content":"内连接 内连接查询的是两张表交集的部分\n隐式内连接 1 SELECT 字段列表 FROM 表1, 表2 WHERE 查询条件; 显式内连接 1 SELECT 字段列表 FROM 表1 [INNER] JOIN 表2 ON 连接条件; 外连接 左外连接 相当于查询表1（左表）的所有数据包含表1和表2交集部分的数据\n1 SELECT 字段列表 FROM 表1 LEFT [OUTER] JOIN 表2 ON 条件; 右外连接 相当于查询表2（右表）的所有数据 包含 表1和表2交集部分的数据\n自连接 自连接查询，可以是内连接查询，也可以是外连接查询。\n1 SELECT 字段列表 FROM 表A 别名A JOIN 表A 别名B ON 条件; 联合查询-union, union all 对于union查询，就是把多次查询的结果并起来，形成一个新的查询结果集。\n1 2 3 SELECT 字段列表 FROM 表A UNION [ALL] SELECT 字段列表 FROM 表B; 子查询 SQL语句中嵌套SELECT语句，称为嵌套查询，又称子查询。\n1 SELECT * FROM 表1 WHERE 字段1=(SELECT 字段1 FROM 表2); 子查询外部的语句可以是 INSERT / UPDATE / DELETE / SELECT 的任何一个。\n根据子查询结果不同，分为：\n标量子查询（子查询结果为单个值） 列子查询（子查询结果为一列） 行子查询（子查询结果为一行） 表子查询（子查询结果为多行多列） 根据子查询位置，分为：WHERE之后、FROM之后、SELECT之后。\n子查询 \u0026ndash; 标量子查询 标量子查询返回的结果是单个值（数字、字符串、日期等），最简单的形式，这种子查询称为标量子查询。 常用的操作符： = \u0026lt;\u0026gt; \u0026gt; \u0026gt;= \u0026lt; \u0026lt;=\n1 SELECT 字段列表 FROM 表1 WHERE 字段1 = (SELECT 字段2 FROM 表2 WHERE 条件); 子查询 \u0026ndash; 列子查询 子查询返回的结果是一列（可以是多行），这种子查询称为列子查询。 常用的操作符：IN NOT IN ANY SOME ALL\n操作符 描述 IN 在指定的集合范围之内，多选一 NOT IN 不在指定的集合范围内 ANY 子查询返回列表中，有任意一个满足即可 SOME 与ANY等同，使用SOME的地方都可以使用ANY ALL 子查询返回列表的所有值都必须满足 1 SELECT 字段列表 FROM 表1 WHERE 字段1 IN (SELECT 字段2 FROM 表2 WHERE 条件); 子查询 \u0026ndash; 行子查询 子查询返回的结果是一行（可以是多列），这种查询称为行子查询。\n常用的操作符：= \u0026lt;\u0026gt; IN NOT IN\n1 2 # 子查询返回一行数据，可以有多个字段 SELECT 字段列表 FROM 表1 WHERE (字段1， 字段2) = (SELECT 字段3， 字段4 FROM 表2 WHERE 表2); 子查询 \u0026ndash; 表子查询 子查询返回的结果是多行多列，这种子查询称为表子查询。 常用的操作符：IN\n1 2 # 子查询返回的是多行多列数据，字段1和字段2分别匹配子查询结果各行中的字段3和字段4 SELECT 字段列表 FROM 表1 WHERE 字段1, 字段2 IN (SELECT 字段3， 字段4 FROM 表2); ","date":"2023-02-28T10:12:34+08:00","permalink":"https://x-xkang.com/p/mysql--%E5%A4%9A%E8%A1%A8%E6%9F%A5%E8%AF%A2/","title":"MySQL -- 多表查询"},{"content":"排序算法可分为内部排序和外部排序，内部排序是数据记录在内存中进行排序，而外部排序是因为排序的数据很大，一次不能容纳全部的排序记录，在排序过程中需要访问外存。常见的内部排序算法有：插入排序、希尔排序、选择排序、冒泡排序、归并排序、快速排序、堆排序、基数排序等。\n排序算法 平均时间复杂度 最好情况 最坏情况 空间复杂度 排序方式 稳定性 冒泡排序 O(n^2^) O(n) O(n^2^) O(1) In-place 稳定 选择排序 O(n^2^) O(n^2^) O(n^2^) O(1) In-place 不稳定 插入排序 O(n^2^) O(n) O(n^2^) O(1) In-place 稳定 希尔排序 O(n log n) O(n log^2^ n) O(n log^2^ n) O(1) In-place 不稳定 归并排序 O(n log n) O(n log n) O(n log n) O(n) Out-place 稳定 快速排序 O(n log n) O(n log n) O(n^2^) O(log n) In-place 不稳定 堆排序 O(n log n) O(n log n) O(n log n) O(1) In-place 不稳定 计数排序 O(n + k) O(n + k) O(n + k) O(k) Out-place 稳定 冒泡排序 算法步骤 比较相邻的元素，如果第一个比第二个大，那么交换他们两个，对每一对相邻的元素做相同的操作，从开始第一对到结尾的最后一对，这一步做完后，最后一个元素会是最大的数， 针对所有元素重复衣裳步骤，除了最后一个，\n代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 func BubbleSort(arr []int) []int { length := len(arr) for i := 0; i \u0026lt; length; i++ { for j := 0; j \u0026lt; length-i-1; j++ { if arr[j] \u0026gt; arr[j+1] { arr[j], arr[j+1] = arr[j+1], arr[j] } } } return arr } 选择排序 算法步骤 首先在未排序序列中找到最小（大）的元素，存放到序列的起始位置。 再从剩余未排序的未排序元素中继续寻找最小（大）的元素，然后放到已排序序列的末尾。 重复第二步，直到所有元素均排序完毕。\n代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 func SelectSort(arr []int) []int { length := len(arr) var minIndex int // var temp int for i := 0; i \u0026lt; length-1; i++ { minIndex = i for j := i + 1; j \u0026lt; length; j++ { if arr[j] \u0026lt; arr[minIndex] { minIndex = j } } arr[i], arr[minIndex] = arr[minIndex], arr[i] } return arr } 插入排序 算法步骤 将第一待排序序列第一个元素看做一个有序序列，把第二个元素到最后一个元素当成是未排序序列。 从头到尾依次扫描未排序序列，将扫描到的每个元素插入有序序列的适当位置。（如果待插入的元素与有序序列中的某个元素相等，则将待插入元素插入到相等元素的后面。）\n代码实现 1 2 3 4 5 6 7 8 9 10 11 12 func InsertionSort(arr []int) []int { for i := range arr { preIndex := i - 1 current := arr[i] for preIndex \u0026gt;= 0 \u0026amp;\u0026amp; arr[preIndex] \u0026gt; current { arr[preIndex+1] = arr[preIndex] preIndex -= 1 } arr[preIndex+1] = current } return arr } 希尔排序 算法步骤 选择一个增量序列 t1，t2，……，tk，其中 ti \u0026gt; tj, tk = 1； 按增量序列个数 k，对序列进行 k 趟排序； 每趟排序，根据对应的增量 ti，将待排序列分割成若干长度为 m 的子序列，分别对各子表进行直接插入排序。仅增量因子为 1 时，整个序列作为一个表来处理，表长度即为整个序列的长度\n代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func shellSort(arr []int) []int { length := len(arr) gap := 1 for gap \u0026lt; length/3 { gap = gap*3 + 1 } for gap \u0026gt; 0 { for i := gap; i \u0026lt; length; i++ { temp := arr[i] j := i - gap for j \u0026gt;= 0 \u0026amp;\u0026amp; arr[j] \u0026gt; temp { arr[j+gap] = arr[j] j -= gap } arr[j+gap] = temp } gap = gap / 3 } return arr } 归并排序 算法步骤 申请空间，使其大小为两个已经排序序列之和，该空间用来存放合并后的序列； 设定两个指针，最初位置分别为两个已经排序序列的起始位置； 比较两个指针所指向的元素，选择相对小的元素放入到合并空间，并移动指针到下一位置； 重复步骤 3 直到某一指针达到序列尾； 将另一序列剩下的所有元素直接复制到合并序列尾 代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 func MergeSort(arr []int) []int { length := len(arr) if length \u0026lt; 2 { return arr } middle := length / 2 left := arr[0:middle] right := arr[middle:] return merge(mergeSort(left), mergeSort(right)) } func merge(left []int, right []int) []int { var result []int for len(left) != 0 \u0026amp;\u0026amp; len(right) != 0 { if left[0] \u0026lt;= right[0] { result = append(result, left[0]) left = left[1:] } else { result = append(result, right[0]) right = right[1:] } } for len(left) != 0 { result = append(result, left[0]) left = left[1:] } for len(right) != 0 { result = append(result, right[0]) right = right[1:] } return result } 快速排序 算法步骤 从数列中挑出一个元素，称为 \u0026ldquo;基准\u0026rdquo;（pivot）; 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作； 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序；\n代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 func QuickSort(arr []int) []int { return _quickSort(arr, 0, len(arr)-1) } func _quickSort(arr []int, left, right int) []int { if left \u0026lt; right { partitionIndex := partition(arr, left, right) _quickSort(arr, left, partitionIndex-1) _quickSort(arr, partitionIndex+1, right) } return arr } func partition(arr []int, left, right int) int { pivot := left index := pivot + 1 for i := index; i \u0026lt;= right; i++ { if arr[i] \u0026lt; arr[pivot] { swap(arr, i, index) index += 1 } } swap(arr, pivot, index-1) return index - 1 } func swap(arr []int, i, j int) { arr[i], arr[j] = arr[j], arr[i] } 堆排序 算法步骤 创建一个堆 H[0……n-1]； 把堆首（最大值）和堆尾互换； 把堆的尺寸缩小 1，并调用 shift_down(0)，目的是把新的数组顶端数据调整到相应位置； 重复步骤 2，直到堆的尺寸为 1。 代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 func HeapSort(arr []int) []int { arrLen := len(arr) buildMaxHeap(arr, arrLen) for i := arrLen - 1; i \u0026gt;= 0; i-- { swap(arr, 0, i) arrLen -= 1 heapify(arr, 0, arrLen) } return arr } func buildMaxHeap(arr []int, arrLen int) { for i := arrLen / 2; i \u0026gt;= 0; i-- { heapify(arr, i, arrLen) } } func heapify(arr []int, i, arrLen int) { left := 2*i + 1 right := 2*i + 2 largest := i if left \u0026lt; arrLen \u0026amp;\u0026amp; arr[left] \u0026gt; arr[largest] { largest = left } if right \u0026lt; arrLen \u0026amp;\u0026amp; arr[right] \u0026gt; arr[largest] { largest = right } if largest != i { swap(arr, i, largest) heapify(arr, largest, arrLen) } } func swap(arr []int, i, j int) { arr[i], arr[j] = arr[j], arr[i] } 计数排序 算法步骤 找出待排序的数组中最大和最小的元素 （2）统计数组中每个值为i的元素出现的次数，存入数组C的第i项 （3）对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加） （4）反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1\n代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 func CountingSort(arr []int, maxValue int) []int { bucketLen := maxValue + 1 bucket := make([]int, bucketLen) // 初始为0的数组 sortedIndex := 0 length := len(arr) for i := 0; i \u0026lt; length; i++ { bucket[arr[i]] += 1 } for j := 0; j \u0026lt; bucketLen; j++ { for bucket[j] \u0026gt; 0 { arr[sortedIndex] = j sortedIndex += 1 bucket[j] -= 1 } } return arr } ","date":"2023-02-19T15:15:15+08:00","permalink":"https://x-xkang.com/p/%E5%87%A0%E7%A7%8D%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/","title":"几种常用的排序算法"},{"content":"一、全局锁 对整个数据库实例加锁，加锁后整个数据库实例处于只读状态，后续的DML的写语句，DDL语句已经更新操作的事务提交语句都将阻塞。 其典型的使用场景是做全库的逻辑备份，对所有的表进行锁定，从而获取一致性的视图，保证数据的完整性。 加锁操作：\n1 2 3 4 5 6 7 8 9 10 11 -- 加锁 mysql\u0026gt; flush tables with read lock; -- TODO. 逻辑备份 mysqldump -uroot -p123456 [dbname] \u0026gt; dbname.sql -- TODO. 中间穿插的 insert/update/delete 语句将阻塞 mysql\u0026gt; update user set `password`=\u0026#39;654321\u0026#39; where `id` = 1; -- 释放锁 mysql\u0026gt; unlock tables; 特点：\n数据库中加全局锁是比较重的操作，存在一下问题\n如果在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆。 如果在从库上备份，那么在备份期间从库不能执行从主库同步过来的二进制文件(binlog)，会导致主从延迟。 在Innodb存储引擎中，我们可以再备份时加上参数 \u0026ndash;single-transaction 参数来完成不加锁的一致性数据备份\n1 mysqldump --single-transaction -uroot -p123456 [dbname] \u0026gt; dbname.sql 二、表级锁 介绍 每次操作锁住整张表，锁定力度大，发生锁冲突的概率最高，并发度最低。应用在Innodb,MyIsam，BDB等存储引擎中。\n表级锁主要分为以下几类：\n1. 表锁 表共享读锁 表独占写锁 操作如下：\n1 2 3 4 5 -- 加锁 mysql\u0026gt; lock tables [table_name] read/write; -- 释放锁 mysql\u0026gt; unlock tables; 读锁不会阻塞其他客户端的读操作，但是会阻塞写操作。 写锁既会阻塞其他客户端的读，也会阻塞其他客户端的写。 2. 元数据锁(meta data lock, mdl) MDL加锁过程是系统自动控制的，无需显式使用，在访问一张表的时候会自动加上，主要作用是维护表元数据的数据一致性，在表上有活动事务的时候，不可以对元数据进行写操作，为了避免MDL和DDL的冲突，保证读写的正确性 在MySQL5.5中加入了MDL，当对一张表进行增删改查的时候，加MDL读锁（共享锁），当对表结构进行修改的时候，加MDL写锁。 查看元数据锁\n1 mysql\u0026gt; select object_type, object_schema, object_name, lock_type, lock_duration from performance_schema.metadata_locks; 对应SQL 锁类型 说明 lock tables [t_name] read/write SHARED_READ_ONLY / SHARED_NO_READ_WRITE select、select \u0026hellip; lock in share mode SHARED_READ 与SHARED_READ/SHARED_WRITE兼容，与EXCLUSIVE 互斥 insert、update、delete、select \u0026hellip; for update SHARED_WRITE 与SHARE_READ/ SHARED_WRITE兼容，与EXCLUSIVE 互斥 alter table \u0026hellip; EXCLUSIVE 与其他的 MDL 都互斥 3. 意向锁 为了避免MDL在执行时，加的表锁与行锁冲突，在InnoDB中引入了意向锁，使得表锁不用检查每行数据是否加锁，使用意向锁来减少表锁的检查。\n意向共享锁（IS）：由语句 select \u0026hellip; lock in share mode 添加，与表锁共享锁（read）兼容，与表锁排它锁（write）互斥。 意向排它锁（IX）：由语句 insert、update、delete、select \u0026hellip; for update 添加，与表锁共享锁（read）和表锁互斥锁（write）都互斥。意向锁之间不会互斥。 查看意向锁：\n1 mysql\u0026gt; select object_schema, object_name, index_name, lock_type, lock_mode, lock_data from performance_schema.data_locks; 三、行级锁 每次操作锁住对应的数据行。锁定粒度最小，发生锁冲突的概率最低，并发度最高。应用在Innodb存储引擎中。\nInnodb的数据是鲫鱼索引组织的，行锁是通过对索引上的索引项加锁来实现的，而不是对记录加的锁，对于行级锁，主要分为以下三类：\n1. 行锁 Record Lock：锁定单个记录的锁，防止其他事务对此进行update和delete，在 RC 和 RR 隔离级别下都支持。\nInnodb实现了以下两种类型的行锁： 共享锁（S）：允许一个事务去读一行，阻止其他事务获得相同数据集的排它锁。 排它锁（X）：允许获取排它锁的事务更新数据，阻止其他事务获得相同数据集的共享锁和排它锁。 当前锁类型 \\ 请求锁类型 S（共享锁） X（排它锁） S （共享锁） 兼容 冲突 X （排它锁） 冲突 冲突 不同的SQL使用的锁类型： SQL 行锁类型 说明 INSERT \u0026hellip; 排它锁 自动加锁 UPDATE \u0026hellip; 排它锁 自动加锁 DELETE \u0026hellip; 排它锁 自动加锁 SELECT（正常） 不加任何锁 SELECT \u0026hellip; LOCK IN SHARE MODE 共享锁 需要手动在 SELECT 之后加 LOCK IN SHARE MODE SELECT \u0026hellip; FOR UPDATE 排它锁 需要手动在 SELECT 之后加 FOR UPDATE 行锁-演示： 默认情况下，InnoDB在REPEATABLE READ 事务隔离界别运行，InnoDB 使用next key 锁进行搜索和索引扫描，以防止幻读。\n针对唯一索引进行检索时，对已存在的记录进行等值匹配时，将会自动优化为行锁。 InnoDB的行锁是针对于索引加的锁不通过索引条件检索数据，那么InnoDB将对表中的所有数据进行加锁，此时就会升级为表锁 可以通过一下SQL，查看意向锁以及行锁的加锁情况：\n1 mysql\u0026gt; select object_schema, object_name, index_name, lock_type, lock_mode, lock_data from performance_schema.data_locks; 2. 间隙锁 Gap Lock: 锁定索引记录间隙（不包含该记录），确保索引记录间隙不变，防止其他事务在这个间隙进行insert，产生幻读，在RR隔离级别下都支持。\n3. 临键锁 Next-Key Lock: 行锁和间隙锁组合，同时锁住数据，并锁住数据前面的间隙Gap。在RR隔离级别下支持。\n4. 间隙锁/临键锁 演示 默认情况下，InnoDB在Repeatable read 事务隔离级别运行，InnoDB使用 Next-key 锁进行搜索和索引扫描，以防止幻读。\n索引上的等值查询（唯一索引），给不存在的记录加锁时优化为间隙锁。 索引上的等值查询（普通索引），向右遍历时最后一个值不满足查询需求时，next-key lock 退化为间隙锁。 索引上的范围查询（唯一索引），会访问到不满足条件的第一个值为止。 注意：间隙锁的唯一目的是防止其他事务插入间隙，间隙锁可以共存，一个事务采用的间隙锁不会阻止另一个事务在同一间隙上使用间隙锁 ","date":"2022-12-16T17:54:33+08:00","permalink":"https://x-xkang.com/p/mysql--%E9%94%81%E5%88%86%E6%9E%90/","title":"MySQL -- 锁分析"},{"content":"环境准备 OS: win10 docker engine: v20.10.14 创建MySQL服务容器 创建 master 节点服务 编辑数据库配置文件，将配置文件以及数据存储目录挂载到宿主机上 master节点配置文件mysql-master.cnf如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 [mysqld] # 定义服务id server-id=1 #启用二进制日志 log-bin=mysql-bin # 设置不要复制的数据库(可设置多个) # binlog-ignore-db=information_schema # 设置需要复制的数据库 需要复制的主数据库名字 binlog-do-db=testdb #设置logbin格式 binlog_format=STATEMENT # 开启gtid enforce-gtid-consistency=on gtid-mode=on 创建master节点服务的容器mysql-master 1 docker run -d -p 3310:3306 -v D:\\workspace\\docker-volumes\\mysql\\master\\mysql-master.cnf:/etc/mysql/conf.d/mysql-master.cnf -v D:\\workspace\\docker-volumes\\mysql\\master\\data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 --name mysql-master mysql:5.7 创建 slave 节点服务 创建配置文件 mysql-slave-1.cnf 如下： 1 2 3 4 5 6 7 8 [mysqld] # 设置 server-id 注意不要与 master 节点重复 server-id=2 binlog-format=STATEMENT relay-log=mysql-relay gtid-mode=ON enforce-gtid-consistency=true read-only=1 创建 slave 节点服务 1 docker run -d -p 3311:3306 -v D:\\workspace\\docker-volumes\\mysql\\slave-1\\mysql-slave-1.cnf:/etc/mysql/conf.d/mysql-slave-1.cnf -v D:\\workspace\\docker-volumes\\mysql\\slave-1\\data\\:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 --name mysql-slave-1 mysql:5.7 创建同步数据使用的用户 进入master节点 1 docker exec -it mysql-master mysql -uroot -p123456 创建用户，记住此时创建的用户密码 1 2 CREATE USER \u0026#39;slave\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;123456\u0026#39;; GRANT REPLICATION SLAVE ON *.* TO \u0026#39;slave\u0026#39;@\u0026#39;%\u0026#39;; 查看 master 节点状态，在master 节点执行 1 mysql\u0026gt; show master status; File Position Binlog_Do_DB Binlog_Ignore_DB Executed_Gtid_Set mysql-bin.000002 157 testdb mysql,information_schema 记住File和Position，后面关联主从节点的时候会用\n查看 master 节点的IP地址，关联主从节点的时候要用 1 docker inspect mysql-master 进入 slave 节点 1 docker exec -it mysql-slave-1 mysql -uroot -p123456 执行SQL关联主从复制节点\n1 2 3 4 5 6 7 mysql\u0026gt; CHANGE MASTER TO MASTER_HOST=\u0026#39;172.17.0.2\u0026#39;, # master 节点IP MASTER_PORT=3306, # master 节点端口 MASTER_USER=\u0026#39;slave\u0026#39;, # 上面创建的主从同步用户 MASTER_PASSWORD=\u0026#39;123456\u0026#39;, # 用户密码 MASTER_LOG_FILE=\u0026#39;mysql-bin.000003\u0026#39;, # master 节点的日志文件 MASTER_LOG_POS=0; 开启 slave，\n1 2 mysql\u0026gt; start slave; Query OK, 0 rows affected, 1 warning (0.03 sec) 查看 slave开启状态\n1 mysql\u0026gt; show slave status\\G; 创建连接用户并授权 进入 master 节点，执行 1 2 3 4 5 # 创建用户 CREATE USER \u0026#39;test\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;123456\u0026#39;; # 授权 grant select,insert,update,delete on testdb.* to \u0026#39;test\u0026#39;@\u0026#39;%\u0026#39;; [注]测试过程中不要使用root账号测试从库的read-only权限，因为拥有 super 权限的账号会忽略限制\n在 master 节点上的testdb 库新建数据表 user 查看 slave 节点数据库，user 已经同步成功，至此mysql 的主从架构集群就已经部署完成，如果想增加 slave节点的话，重复2.2\n[注意]\n关联主从节点时，确保IP和端口是通的，不然在从节点执行 start slave 时，查看结果 show slave status 连接结果是失败的; 测试账号不要用 root 权限，会忽略从节点的读写限制，从节点也一样可以插入、更新、删除数据; slave 节点服务一定要将 read_only 设置成1，可以在.cnf配置文件中配置，也可以在mysql终端中设置set global read_only=1;不然有写权限的账号在 slave 节点也一样可以插入或更新数据; ","date":"2022-12-02T11:52:10+08:00","permalink":"https://x-xkang.com/p/mysql--docker%E9%83%A8%E7%BD%B2%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E6%96%B9%E6%A1%88/","title":"MySQL -- docker部署集群之主从复制方案"},{"content":" 分布式 将一个完整的系统，按照业务功能拆分成一些独立的子系统，它们独立运行在不同的服务器上。\n集群 集群就是单机的多实例，在多个服务器上部署同一个系统，这些服务器的集合叫做集群\n一、单机架构出现的问题 性能问题 单个服务器无论是计算资源还是存储资源都非常有限，当请求量非常大的时候，会出现响应变慢等情况，严重影响用户体验 单体架构的多个服务中，某个服务占用资源多，时间长，导致总体响应显慢 可用性问题 当出现单机故障（进程崩溃、断电、磁盘损坏等）时，整个应用不可用。 开发问题 单体架构下，我们将各个模块放在一个系统中，系统过于庞大臃肿，维护成本高，各个功能模块之间的耦合度偏高，难以针对单个模块进行优化，出现 BUG 较难定位。 交付周期长，所有功能得一起上线，一起构建，一起部署。任何一个环节出错，都可能影响交付。 二、为什么引入分布式 性能提高 相较于单体架构，在请求量非常大的时候，整个系统的响应速度不会有明显的下降，因为每个服务都占有各自独有的服务器资，资源竞争小，任务处理较快 提高可用性 在单体架构中，一旦出现了例如服务器宕机，磁盘损坏等故障问题，会导致整个服务不可用，但分布式架构将各个功能模块拆分独立部署后，即使部分服务器出现故障，也不会影响其他的功能 提高开发效率 1、针对传统的单体架构，必须将整个系统的功能开发完毕才可以上线部署，分布式可以将各个功能模块拆开，独立开发，独立测试，独立部署，同时也降低了系统之间的耦合度，系统与系统之间的边界也会更加清晰 2、服务的复用性更高，系统中某些比较通用的功能拆分出来作为独立的服务部署后，可以作为通用服务调用，不用在每一个系统中开发重复且耗时的功能，比如用户系统，支付系统等 三、为什么引入集群 性能提高 传统的单体架构，在请求量非常大的时候，整个系统的响应速度都会下降，对于集群来说，通过负载均衡将请求分发至集群中的不同节点中，这样可以有效降低每个服务节点的压力。 提高可用性 若集群中的某个节点出现故障时，可将请求转发至其他可用的服务节点上，这样可以保证服务的高可用。 四、分布式与集群的区别 从提升效率的角度比较\n分布式是以缩短单个任务的执行时间来提升效率 集群是以提高单位时间内执行的任务数来提高效率 从服务部署的角度比较\n分布式是将一个大的服务拆分成独立的小的服务并部署在不同的服务器上 集群则是将几台服务器集中在一起提供相同的服务 分布式中的每一个节点都可以做集群，反之则不一定可以 五、分布式与集群的关系 根据分布式的特点可以总结出，其主要作用是将系统模块化，将系统进行解耦，有利于开发和维护，但并不能解决大并发的问题，也无法保证服务在运行期间出现宕机等问题后的正常运转， 而集群刚好弥补了这个缺陷，集群，通俗来说就是多个服务器处理相同的业务，这里可以改善一些并发量大的问题，保证了高性能；另一方面，如果集群中的某个节点出现故障导致不可用，但对于整个集群来说，服务还是可用的，即保证了服务的高可用。 六、分布式集群的优点 系统可用性的提升\n一个系统全年可用时间在99.999%，5个9的服务可用率在设计合理的分布式集群系统中不是一个触不可及的数字 传统的集中式计算或集中式存储在遇见单点故障时很容易造成整个服务的不可用，分布式集群下的服务体系，单台机器有故障，不至于造成整个服务不可用 系统并发能力的提升\n请求通过负载均衡被分发到不同的服务器上，执行同样服务的服务器可以有1台或者N台，通常情况下可以根据用户访问量增加或减少服务器的数量，可以做到随时水平扩展 系统容错能力的提升\n同一组服务器分别部署在不同的城市，若其中一个机房发生故障（火灾或者停电），其他城市的服务器还是可用状态，将故障机房的流量转发到正常的城市机房中，可以有效提高服务的可用性 低延时\n服务器部署在不同城市之后，可以根据用户的IP将请求分发至最近的机房，可以达到降低延时的效果 低耦合\n（低耦度低）系统之间的耦合度大大降低，可以独立开发、独立部署、独立测试，系统与系统之间的边界非常明确，排错也变得相当容易，开发效率大大提升 （扩展性强）系统更易于扩展。我们可以针对性地扩展某些服务，换句话说，就是对子系统进行集群。例如在双十一时，订单子系统、支付子系统需要集群，账户管理子系统不需要集群。 （复用性高）服务的复用性更高。比如，当我们将用户系统作为单独的服务后，该公司所有的产品都可以使用该系统作为用户系统，无需重复开发 七、分布式集群带来的挑战 依赖网络，会因为网络问题导致系统数据丢失或者不一致 服务器之间的通信依赖网络，不可靠网络包括网络延时，丢包，中断，异步，一个完整的请求依赖于一连串的服务调用，任意一个服务节点出现网络故障都可能造成本次服务调用的失败\n系统复杂化，系统监控维护，版本迭代发布变的相对复杂，成本高 传统单体架构的服务只需要维护一个站点即可，分布式服务系统被拆分成若干个小服务，服务从1变成几十上百个服务之后，增加运维成本\n一致性，可用性，分区容错性无法同时满足 这个是最主要的，就是常说的CAP理论，在分布式系统中，这三种特性最多同时满足两种，无法同时满足全部，需要根据实际情况调整策略\n","date":"2022-11-28T11:02:35+08:00","permalink":"https://x-xkang.com/p/%E5%88%86%E5%B8%83%E5%BC%8F%E5%92%8C%E9%9B%86%E7%BE%A4%E7%9A%84%E5%8C%BA%E5%88%AB/","title":"分布式和集群的区别"},{"content":"一、索引的分类 1、根据数据结构可分为 Btree索引（B+tree, b-tree） 哈希索引 full-text 全文索引 2、根据物理存储可分为 聚簇索引 二级索引(辅助索引) 3、根据字段特性可分为 主键索引 普通索引 前缀索引 4、根据字段个数可分为 单列索引 联合索引（符合索引，组合索引） 二、根据数据结构分类 MySQL按数据结构分类可分为：B+tree索引，Hash索引，Full-text索引\n- InnoDB MyIsam Memory B+tree索引 √ √ √ Hash索引 × × √ B-tree索引 √（MySQL5.6+） √ × 注：InnoDB 实际上也支持Hash索引，但是InnoDB中Hash索引的创建由存储引擎自动优化创建，不能人为干预 是否为表创建Hash索引，\nB+tree是MySQL中被存储引擎采用最多的索引类型，B+tree中的B代表平衡（balance），而不是二叉（binary），因为B+tree是从最早的平衡二叉树演化而来的，下面演示B+tree数据结构与其他数据结构的对比。\n1. B+tree 和 B-tree的对比 [网络图片] B-tree示意图\nB+tree非叶子结点只存储键值信息，数据记录都存放在叶子节点中，而B-tree的非叶子节点也存储数据，B+tree单个节点的存储量更小，在相同的磁盘IO情况下可以查到更多节点数据。 B+tree 所有叶子结点之间都采用单链表连接，适合MySQL中常见的基于范围的顺序检索场景，而B-tree无法做到这一点。 [网络图片] B+tree示意图\n2. B+tree 和 红黑树的对比 [网络图片] 红黑树示意图\n红黑树是一种弱平衡二叉查找树，通过任何一条从根到叶子的路径上各个节点着色方式的限制，红黑树确保没有一条路径会比其他路径长出两倍\n对于有N个节点的B+tree，其搜索的时间复杂度为O(logdN)，其中**d(Degree)**为B+tree的度，表示节点允许的最大子节点数为d个，在实际应用当中，d值一般是大于100的，即使数据量达到千万级别时B+tree的高度依然维持在3-4左右，保证了3-4次磁盘I/O操作就能查询到目标数据。\n红黑树是二叉树，节点子节点个数为两个，意味着其搜索的算法时间复杂度为 O(logN)，树的高度也会比 B+tree 高出不少，因此红黑树检索到目标数据所需经历的磁盘I/O次数更多。\n3. B+tree 和 Hash的对比 Hash 索引结构的特殊性，其检索效率非常高，索引的检索可以一次定位，不像B-Tree 索引需要从根节点到枝节点，最后才能访问到页节点这样多次的IO访问，所以 Hash 索引的查询效率要远高于 B-Tree 索引。虽然 Hash 索引效率高，但是 Hash 索引本身由于其特殊性也带来了很多限制和弊端，主要有以下这些。\nHash 索引仅仅能满足 = , IN 和 \u0026lt;=\u0026gt;(表示NULL安全的等价) 查询，不能使用范围查询 由于 Hash 索引比较的是进行 Hash 运算之后的 Hash值，所以它只能用于等值的过滤，不能用于基于范围的过滤，因为经过相应的 Hash算法处理之后的 Hash 值的大小关系，并不能保证和Hash运算前完全一样\nHash 索引无法适用数据的排序操作 由于 Hash 索引中存放的是经过 Hash 计算之后的 Hash值，而且Hash值的大小关系并不一定和 Hash运算前的键值完全一样，所以数据库无法利用索引的数据来避免任何排序运算；\nHash 索引不能利用部分索引键查询 对于组合索引，Hash 索引在计算 Hash 值的时候是组合索引键合并后再一起计算 Hash 值，而不是单独计算 Hash值，所以通过组合索引的前面一个或几个索引键进行查询的时候，Hash 索引也无法被利用\nHash 索引依然需要回表扫描 Hash 索引是将索引键通过 Hash 运算之后，将 Hash运算结果的 Hash值和所对应的行指针信息存放于一个 Hash 表中，由于不同索引键可能存在相同 Hash 值，所以即使取满足某个 Hash 键值的数据的记录条数，也无法从 Hash索引中直接完成查询，还是要通过访问表中的实际数据进行相应的比较，并得到相应的结果。\nHash索引遇到大量Hash值相等的情况后性能并不一定就会比B-Tree索引高 选择性比较低的索引键，如果创建 Hash 索引，那么将会存在大量记录指针信息存于同一个Hash值相关联。这样要定位某一条记录时就会非常麻烦，会浪费多次表数据的访问，而造成整体性能低下\n由于范围查询是MySQL数据库查询中常见的场景，Hash表不适合做范围查询，它更适合做等值查询。另外Hash表还存在Hash函数选择和Hash值冲突等问题。因此，B+tree索引要比Hash表索引有更广的适用场景\n三、根据物理存储分类 MySQL根据叶子节点是否存储的是完整数据将索引分为：聚簇索引，二级索引（辅助索引）。\n1.聚簇索引 聚簇索引的每个叶子节点都存储了一行完整的表数据，叶子节点之间按照id列升序连接，可以方便的进行顺序检索。 [网络图片] MySQL中的InnoDB 存储引擎要求必须有聚簇索引，默认在主键字段上建立聚簇索引，在没有主键的情况下，数据表的第一个非空唯一索引将被建立为聚簇索引， 在前两者都没有的情况下，InnoDB将自动生成一个隐式的自增id列，并在此列上建立聚簇索引。\nMyISAM存储引擎不存在聚簇索引，主键索引和非主键索引的结构是一样的，索引的叶子节点不存储表数据，存放的是表数据的地址，所以MyISAM存储引擎的表可以没有主键。MyISAM表的索引和数据是分开存储的，\n2.二级索引 [网络图片]\n二级索引的叶子节点并不存储一行完整的数据，而是存储了聚簇索引所在的列。\n回表查询：由于二级索引的叶子节点不存储完整的表数据，索引当通过二级索引查询聚簇索引列值后，还需要回到聚簇索引也就是表本身进一步获取数据 [网络图片]\n回表查询需要额外的B+tree搜索过程，必然增加查询消耗, 需要注意的是，通过二级索引查询时，回表不是必须的过程，当select 的所有字段在单个二级索引中都能找到时，就不需要回表，MySQL称此时的二级索引为覆盖索引或者触发了索引覆盖。 可以用explain 命令查看SQL语句的执行计划，执行计划的extra字段中若出现Using index，表示查询触发了索引覆盖。\n四、按字段特性分类 MySQL索引按照字段特性可以分为：主键索引，普通索引，前缀索引\n主键索引 建立在主键上的索引称为主键索引，一张数据表只能有一个主键索引，索引列值不允许有空值，通常在创建表时一起创建。\n唯一索引 建立在 Unique字段上的索引称为唯一索引，一张表可以有多个唯一索引，索引列值允许为空，列值中出现多个空值不会出现重复冲突。\n普通索引 建立在普通字段上的索引被称为普通索引\n前缀索引 前缀索引是指对字符类型字段的前几个字符或对二进制类型字段的前几个bytes建立的索引，而不是在整个字段上建索引。前缀索引可以建立在类型为char、varchar、binary、varbinary的列上，可以大大减少索引占用的存储空间，也能提升索引的查询效率\n","date":"2022-11-26T11:17:45+08:00","permalink":"https://x-xkang.com/p/mysql--%E7%B4%A2%E5%BC%95%E6%B5%85%E6%9E%90/","title":"MySQL -- 索引浅析"},{"content":"Slice的数据结构 切片是对数组的一个连续片段的引用，所以切片是一个引用类型，一个长度可变的数组。 源码中的数据结构定义如下：\n源码位置： src/runtime/slice.go#L22\n1 2 3 4 5 type slice struct { array unsafe.Pointer // 引用的底层数组指针 len int // 切片的长度 cap int // 切片的容量 } 创建切片 创建方式 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 func main() { // 1. 通过关键字 var 声明，此时未初始化，无法对s1[i]进行取值、赋值 var s1 []int // 2. 使用 new() 创建，由于 new()返回的是一个指针，通过 *获取指针的值，未初始化，同样也无法对s2[i]进行取值赋值 s2 := *new([]int) // 3. 直接创建并初始化，已初始化，因此可以对s3[i]进行取值赋值 s3 := []int{1, 2, 3} // 4. make() 创建，第一个参数指定类型，第二个参数为长度，第三个参数为容量（可不指定，默认为长度值） s4 := make([]int, 4, 5) // 5. 通过截取数组或切片 array := [7]int{1, 2, 3, 4, 5, 6, 7} s5 := array[3:5] // (左闭右开)，输出 [4,5]，引用数组 array 的部分片段 } 创建逻辑 src/runtime/slice.go#L88\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // input: et: slice类型元信息，slice长度，slice容量 func makeslice(et *_type, len, cap int) unsafe.Pointer { // 调用MulUintptr函数：获取创建该切片需要的内存，是否溢出(超过2^64) // 2^64是64位机能够表示的最大内存地址 mem, overflow := math.MulUintptr(et.size, uintptr(cap)) if overflow || mem \u0026gt; maxAlloc || len \u0026lt; 0 || len \u0026gt; cap { mem, overflow := math.MulUintptr(et.size, uintptr(len)) // 如果溢出或超过能够分配的最大内存(2^32 - 1) | 非法输入, 报错并返回 if overflow || mem \u0026gt; maxAlloc || len \u0026lt; 0 { panicmakeslicelen() } panicmakeslicecap() } // 调用mallocgc函数分配一块连续内存并返回该内存的首地址 // 该函数实现涉及到了go语言内存管理，比较复杂，不是本文的主题 // 关于 mallocgc 逻辑后续再讲 return mallocgc(mem, et, true) } 根据slice的类型和长度以及容量，计算出所需要的内存空间大小， 如果合法则调用mallocgc函数申请相应的连续内存并返回首地址， 下面来看一下是如何计算所需要的内存大小的\nsrc\\runtime\\internal\\math\\math.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 // MulUintptr returns a * b and whether the multiplication overflowed. // On supported platforms this is an intrinsic lowered by the compiler. func MulUintptr(a, b uintptr) (uintptr, bool) { // 如果slice类型大小为0(如struct{}类型) 或 (a | b) \u0026lt; 2 ^ 32，肯定不会发生溢出 // sys.PtrSize = 8 if a|b \u0026lt; 1\u0026lt;\u0026lt;(4*sys.PtrSize) || a == 0 { return a * b, false } // 如果a * b \u0026gt; MaxUintptr，即类型大小 * 切片容量 \u0026gt; MaxUintPtr，则说明发生了溢出 // MaxUintptr = ^uintptr(0) = 2 ^ 64 overflow := b \u0026gt; MaxUintptr/a return a * b, overflow } 追加元素 append() src/reflect/value.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 // Append appends the values x to a slice s and returns the resulting slice. // As in Go, each x\u0026#39;s value must be assignable to the slice\u0026#39;s element type. func Append(s Value, x ...Value) Value { s.mustBe(Slice) s, i0, i1 := grow(s, len(x)) for i, j := i0, 0; i \u0026lt; i1; i, j = i+1, j+1 { s.Index(i).Set(x[j]) } return s } // grow grows the slice s so that it can hold extra more values, allocating // more capacity if needed. It also returns the old and new slice lengths. func grow(s Value, extra int) (Value, int, int) { i0 := s.Len() i1 := i0 + extra if i1 \u0026lt; i0 { panic(\u0026#34;reflect.Append: slice overflow\u0026#34;) } m := s.Cap() if i1 \u0026lt;= m { return s.Slice(0, i1), i0, i1 } if m == 0 { m = extra } else { // const threshold = 256 // 1.18增加 for m \u0026lt; i1 { if i0 \u0026lt; 1024 { m += m } else { m += m / 4 } } } t := MakeSlice(s.Type(), i1, m) Copy(t, s) return t, i0, i1 } 追加元素流程如下：\n计算当前slice长度i0与追加元素的长度总和i1，若i1 \u0026lt; i0（追加后的slice长度小于当前的slice长度），则直接抛出异常 若当前切片容量为0，则将容量改为追加元素的长度 循环遍历增加容量， 若追加的元素长度小于阈值（v1.18之前是1024，从v1.18开始改为256），则容量成倍增长， 否则容量每次增加1/4,(注：v1.18开始计算逻辑改为 m = (m + 3 * threshold) / 4) ， 创建新的切片，并将当前切片拷贝到新的切片中，返回新的切片 将追加的元素存储到新的切片指定位置 切片的扩容逻辑：\n原cap扩充一倍，即doublecap 如果指定cap(追加元素后的切片长度) 大于 doublecap，则取cap，否则如下： cap 小于1024，取doublecap 每次增长 1/4，直至不小于cap ","date":"2022-09-05T09:01:56+08:00","permalink":"https://x-xkang.com/p/golang%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E4%B9%8B-slice-%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0/","title":"Golang数据类型之 slice 源码实现"},{"content":"一、删除策略 惰性删除\n每次获取 key 的时候会在 expire 字典中查询是否有当前key，如果有的话会校验当前key的过期时间，过期则删除，缺点是如果存在键已过期，但长期不使用的情况，实际上数据还是存在内存中的\n以下是 get 命令的部分源码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // src/t_string.c line 78 void setGenericCommand(client *c, int flags, robj *key, robj *val, robj *expire, int unit, robj *ok_reply, robj *abort_reply) { long long milliseconds = 0; /* initialized to avoid any harmness warning */ int found = 0; int setkey_flags = 0; if (expire \u0026amp;\u0026amp; getExpireMillisecondsOrReply(c, expire, flags, unit, \u0026amp;milliseconds) != C_OK) { return; } if (flags \u0026amp; OBJ_SET_GET) { if (getGenericCommand(c) == C_ERR) return; } found = (lookupKeyWrite(c-\u0026gt;db,key) != NULL); // 重点，开始查找当前key // ... 省略部分源码 } 顺着 lookupKeyWrite 找下去\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 // src/db.c line 158 robj *lookupKeyWrite(redisDb *db, robj *key) { return lookupKeyWriteWithFlags(db, key, LOOKUP_NONE); // 继续跳转 } // src/db.c line 154 robj *lookupKeyWriteWithFlags(redisDb *db, robj *key, int flags) { return lookupKey(db, key, flags | LOOKUP_WRITE); // 继续跳转 } // src/db.c line 81 robj *lookupKey(redisDb *db, robj *key, int flags) { dictEntry *de = dictFind(db-\u0026gt;dict,key-\u0026gt;ptr); robj *val = NULL; if (de) { val = dictGetVal(de); /* Forcing deletion of expired keys on a replica makes the replica * inconsistent with the master. We forbid it on readonly replicas, but * we have to allow it on writable replicas to make write commands * behave consistently. * * It\u0026#39;s possible that the WRITE flag is set even during a readonly * command, since the command may trigger events that cause modules to * perform additional writes. */ int is_ro_replica = server.masterhost \u0026amp;\u0026amp; server.repl_slave_ro; int force_delete_expired = flags \u0026amp; LOOKUP_WRITE \u0026amp;\u0026amp; !is_ro_replica; // 这里开始判断是否过期 if (expireIfNeeded(db, key, force_delete_expired)) { /* The key is no longer valid. */ val = NULL; } } } // src/db.c line 1666 // 判断逻辑 int expireIfNeeded(redisDb *db, robj *key, int force_delete_expired) { if (!keyIsExpired(db,key)) return 0; if (server.masterhost != NULL) { if (server.current_client == server.master) return 0; if (!force_delete_expired) return 1; } if (checkClientPauseTimeoutAndReturnIfPaused()) return 1; /* Delete the key */ deleteExpiredKeyAndPropagate(db,key); return 1; } 定期删除\n每隔一段时间检查一次数据库，随机删除一些过期键，需要注意的是：Redis 每次扫描并不是遍历过期字典中的所有键，而是采用随机抽取判断并删除过期键的形式执行的 Redis 默认每秒进行10次过期扫描，此配置可以通过 redis.conf 的 hz配置项进行配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // src/server.c line 971 void databasesCron(void) { /* Expire keys by random sampling. Not required for slaves * as master will synthesize DELs for us. */ if (server.active_expire_enabled) { // 区分主从 if (iAmMaster()) { activeExpireCycle(ACTIVE_EXPIRE_CYCLE_SLOW); // Master 节点开始处理过期键 } else { expireSlaveKeys(); } } // 省略部分源码... } Redis使用使用的是惰性删除加定期删除的策略\n二、淘汰策略 只有在 Redis 的运行内存达到了某个阀值，才会触发内存淘汰机制，这个阀值就是我们设置的最大运行内存，此值在 Redis 的配置文件中可以找到，配置项为 maxmemory。\n查看最大运行内存\n1 2 3 127.0.0.1:6379\u0026gt; config get maxmemory 1) \u0026#34;maxmemory\u0026#34; 2) \u0026#34;0\u0026#34; Redis在32位系统下默认阈值为3G（最大运行内存为4G，为保证系统正常运行，预留1G资源），64位系统下默认阈值为0，表示没有大小限制\n查看内存淘汰策略\n1 2 3 127.0.0.1:6379\u0026gt; config get maxmemory-policy 1) \u0026#34;maxmemory-policy\u0026#34; 2) \u0026#34;noeviction\u0026#34; noeviction 表示当运行内存超过最大设置内存时，不淘汰任何数据，但新增数据操作会报错\n内存淘汰策略分类\n1. noeviction: 不淘汰任何数据，当运行内存不足时，新增数据会直接报错 2. allkeys-lru: 淘汰所有键中最长时间未使用的 3. allkeys-random: 随机淘汰任意键 4. allkeys-lfu: 淘汰所有键中最少使用的； 5. volatile-lru: 淘汰所有设置了过期时间的键中最长时间未使用的 6. volatile-random: 随机淘汰设置了过期时间的键 7. volatile-ttl: 优先淘汰更快过期的键 8. volatile-lfu: 优先淘汰设置了过期时间的键中最少使用的\nLRU算法\nLRU 算法全称是Least Recently Used 译为最近最少使用 LRU 算法基于链表结构，链表中的元素按照操作顺序从前往后排列，最新操作的会被移动到表头，当需要内存淘汰时，只需要删除链表尾部的元素即可 Redis使用的是一种近似 LRU 算法，目的是为了更好的节约内存，它的实现方式是给现有的数据结构添加一个额外的字段，用于记录此键的最后一次访问时间，Redis内存淘汰时，会使用随机采样的方式淘汰数据，他是随机取N个值（可配置），然后淘汰醉酒没有使用的那个\nLFU算法\nLFU 全称是 Least Frequently Used 翻译为最不常用，最不常用的算法是根据总访问次数来淘汰数据的，它的核心思想是”如果数据过去被访问的次数很多，那么将来被访问的频率也会很高“。 LFU 解决了偶尔访问一次之后，数据就不会被淘汰的问题，相比于 LRU 算法也更合理一些\n","date":"2022-06-30T10:01:40+08:00","permalink":"https://x-xkang.com/p/redis-%E7%9A%84%E8%BF%87%E6%9C%9F%E5%88%A0%E9%99%A4%E7%AD%96%E7%95%A5%E4%BB%A5%E5%8F%8A%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/","title":"Redis 的过期删除策略以及淘汰策略"},{"content":"Redis提供两种持久化方式：一种是默认的RDB持久化方式，另一种是AOF（append only file）持久化方式\n一、 RDB 是什么？\nRDB是通过创建快照的方式进行持久化，保存某个时间点的全部数据，RDB是Redis默认的持久化方案，原理是Redis会通过单独创建（fork）一个与当前进程一模一样的子进程来进行持久化，这个子进程的所有数据（变量、环境变量、程序计数器等）都和原进程一模一样，会先将数据写入到一个临时文件中，待持久化结束了，再用这个临时文件替换上次持久化好的文件，整个过程冲，主进程不进行任何的io操作，这就确保了极高的性能。\n1、持久化文件在哪 启动redis-server 的目录下\n2、什么时候fork子进程，或者什么时候触发 rdb持久化机制 RDB 方式持久化数据是通过 fork 子进程，在子进程中进行数据同步\nshutdown时，如果没有开启aof，会触发配置文件中默认的快照配置 执行命令 save 或者 bgsave save是只管保存，不管其他，全部阻塞，使用主进程进行持久化 bgsave redis 会在后台异步进行快照操作，同时可以响应客户端的请求\n3、优点 存储紧凑，节省内存空间 恢复速度快 适合全量备份，全量复制的场景，经常用于灾难恢复（对数据的完整性和一致性要求较低）\n4、缺点 容易丢失数据，两次数据快照备份之间如果出现故障，则会丢失期间产生或修改的数据 通过 fork 子进程对内存快照进行全量备份，频繁执行性能消耗较大\n手动触发\nsave， 在命令行执行save命令，将以同步的方式创建rdb文件保存快照，会阻塞服务器的主进程，生产环境中不要用 bgsave, 在命令行执行bgsave命令，将通过fork一个子进程以异步的方式创建rdb文件保存快照，除了fork时有阻塞，子进程在创建rdb文件时，主进程可继续处理请求 自动触发\n在redis.conf中配置​​save m n​​​ 定时触发，如​​save 900 1​​表示在900s内至少存在一次更新就触发 主从复制时，如果从节点执行全量复制操作，主节点自动执行bgsave生成RDB文件并发送给从节点 执行debug reload命令重新加载Redis时 执行shutdown且没有开启AOF持久化 二、AOF 将Redis的操作日志以追加的方式写入文件，读操作是不记录的\n为什么会出现AOF持久化方式\n1、这个持久化文件在哪 启动 redis-server 的目录下会生成 appendonly.aof文件\n2、触发机制（根据配置文件的配置项\u0026ndash;appendfsync） no: 表示操作系统进行数据缓存同步到磁盘（快，持久化没保证：写满缓冲区才会同步，若在缓冲区未写满前 shutdown 或其他意外关机，则这部分数据会丢失） always: 同步持久化，每次发生数据变更时（增删改操作），立即记录到磁盘（慢，安全） everysec: 表示每秒同步一次（默认值，很快，但可能会丢失1秒的数据）\n3、AOF 重写机制 重写 AOF 文件会 fork 子进程去执行，会将内存中的数据写入新的 AOF 文件，并且是以RDB 的方式写入，重写结束后会替代旧的AOF 文件，后续的客户端命令操作又重新以 AOF的格式写入，redis.conf 中配置触发 AOF 文件重写的文件大小值auto-aof-rewrite-percentage 不宜太小，因为会频繁触发重写\n触发时机 redis.conf 的配置项 auto-aof-rewrite-min-size 默认值是 64mb， 当 AOF 文件大小超过这个配置值时会自动开启重写 `。 redis.conf 的配置项 auto-aof-rewrite-percentage 默认值是100， 当 AOF 文件大小的增长率大于配置值时会自动开启重写。 4、优点 保证数据安全\n5、缺点 数据恢复慢\n","date":"2022-06-29T15:34:24+08:00","permalink":"https://x-xkang.com/p/redis-%E6%8C%81%E4%B9%85%E5%8C%96%E7%AD%96%E7%95%A5/","title":"Redis 持久化策略"},{"content":"一、Redis 缓存雪崩 出现场景 如果使用redis记录大量的热点数据，且过期时间为同一个常量，那么可能会出现大批的缓存数据会在同一时间或较短的时间区间内失效，redis会根据淘汰策略进行优化，如果数据量比较大会导致线程出现短暂的阻塞；另外，因为大量的缓存失效，会导致请求直接落在DB上，请求数较大情况下会直接导致数据库瘫痪，然后整个业务系统变为不可用状态\n解决方案 针对这种情况，我们可以在设置过期时间时加上一个随机值，类似 redis.set(key, value, expiredTime + Math.random()*10000)，这样设置就不会出现在短时间内大量缓存key失效的情况。\n二、Redis 缓存穿透 出现场景 如果用户请求的热点数据本身是不存在的，比如id为-1，或者id=\u0026lsquo;\u0026lsquo;的数据，查询缓存不存在后会将请求直接打到DB上，最终在DB中也没有查到此数据，此时Redis缓存就是去了作用，搞并发的情况下会降低数据库性能，甚至瘫痪\n解决方案 增加参数校验，拦截掉大量的非法参数请求； 缓存空值，因为数据库中本来就不存在这些数据，因此可以在第一次重建缓存时将value 记录为 null，下次请求时从Redis获取到 null 值直接返回（注意，要对redis查询的返回值进行严格校验，区分key不存在返回的空值和主动设置的空值null）； 布隆过滤器，将DB中的热点数据加载至布隆过滤器中（布隆过滤器的特性：若在过滤器中存在，不一定真是存在；若不存在时，一定不存在），每次请求前先校验布隆过滤器是否存在该key，不存在的话直接return； 三、Redis 缓存击穿 出现场景 高并发请求同一个热点数据，在热点数据失效的瞬间，大量请求在缓存中没有命中会直接落在DB上进行查询，导致DB压力瞬间增加\n解决方案 增加互斥锁，在第一个请求没有在缓存命中开始在DB进行查询并重加缓存时加上一个互斥锁，在缓存重建完成之前，对这同一热点数据的请求将会被互斥锁拦截，被拦截的这些请求根据业务需求，可以延时重试直到拿到数据或直接返回失败等； 热点数据不设置过期时间（不建议，随着热点数据的增加，无过期时间的key也越来越多，或导致Redis的存储压力增加）\n","date":"2022-06-28T14:58:25+08:00","permalink":"https://x-xkang.com/p/redis%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E4%B9%8B%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9%E5%87%BB%E7%A9%BF%E5%92%8C%E7%A9%BF%E9%80%8F/","title":"Redis基础知识之缓存雪崩、击穿和穿透"},{"content":"HTTP/0.9 - 单行协议\n最初版本的HTTP协议并没有版本号，后来它的版本号被定位在0.9以区分后来的版本。HTTP/0.9极其简单：请求由单行指令构成，以唯一可用方法GET开头，其后跟目标资源的路径（一旦连接到服务器，协议、服务器、端口号这些都不是必须的）。 跟后来的版本不同，HTTP/0.9的响应内容并不包含HTTP头，这意味着只有 HTML 文件可以传送，无法传送其他类型的文件；也没有状态码或者错误代码；一旦出现问题，一个特殊的包含问题描述信息的HTML文件将被发回，供人们查看。\nHTTP/1.0 - 构建可扩展性\n由于 HTTP/0.9协议的应用十分有限，浏览器和服务器迅速扩展内容使其用途更广：\n协议版本信息现在会随着每个请求发送（HTTP/1.0被追加到了 GET行） 状态码会在响应开始时发送，使浏览器能了解请求执行成功或失败，并响应调整行为（如更新或使用本地缓存） 引入了 HTTP 头的概念，无论是对于请求还是响应，允许传输元数据，使协议变得非常灵活，更具扩展性。 在新 HTTP 头的的帮助下，具备了传输纯文本HTML文件以外其他类型文档的能力（凭借Content-Type头） 一个典型的请求看起来就像这样：\n1 2 3 4 5 6 7 8 9 10 11 GET /mypage.html HTTP/1.0 User-Agent: NCSA_Mosaic/2.0 (Windows 3.1) 200 OK Date: Tue, 15 Nov 1994 08:12:31 GMT Server: CERN/3.0 libwww/2.17 Content-Type: text/html \u0026lt;HTML\u0026gt; 一个包含图片的页面 \u0026lt;IMG SRC=\u0026#34;/myimage.gif\u0026#34;\u0026gt; \u0026lt;/HTML\u0026gt; 接下来是第二个请求：\n1 2 3 4 5 6 7 8 GET /myimage.gif HTTP/1.0 User-Agent: NCSA_Mosaic/2.0 (Windows 3.1) 200 OK Date: Tue, 15 Nov 1994 08:12:32 GMT Server: CERN/3.0 libwww/2.17 Content-Type: text/gif (这里是图片内容) 在 1991-1995年，这些新扩展并没有被引入到标准中以促进下注工作，而仅仅作为一种尝试：服务器和浏览器天界这些新扩展功能，但出现了大量的互操作问题。知道 1996年11月，为了解决这些问题，一份新文档（RFC 1945） 被发表出来，泳衣描述如何操作实践这些新扩展功能，文档 RFC 1945 定义了 HTTP/1.0，但它是狭义的，并不是官方标准\nHTTP/1.1 - 标准化的协议\nHTTP/1.0 多种不同的实现方式在实际运用中显得有些混乱，自 1995 年开始，即 HTTP/1.0 文档发布的下一年，就开始修订 HTTP 的第一个标准化版本。在 1997 年初，HTTP1.1 标准发布，就在 HTTP/1.0 发布的几个月后。\nHTTP/1.1 消除了大量歧义内容并引入了多项改进：\n连接可以复用，节省了多次打开 TCP 连接加载网页文档资源的时间。 增加管线化技术，允许在第一个应答被完全发送之前就发送第二个请求，以降低通信延迟。 支持响应分块。 引入额外的缓存控制机制。 引入内容协商机制，包括语言，编码，类型等，并允许客户端和服务器之间约定以最合适的内容进行交换。 凭借Host头，能够使不同域名配置在同一个 IP 地址的服务器上。 一个典型的请求流程， 所有请求都通过一个连接实现，看起来就像这样：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 GET /en-US/docs/Glossary/Simple_header HTTP/1.1 Host: developer.mozilla.org User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:50.0) Gecko/20100101 Firefox/50.0 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8 Accept-Language: en-US,en;q=0.5 Accept-Encoding: gzip, deflate, br Referer: https://developer.mozilla.org/en-US/docs/Glossary/Simple_header 200 OK Connection: Keep-Alive Content-Encoding: gzip Content-Type: text/html; charset=utf-8 Date: Wed, 20 Jul 2016 10:55:30 GMT Etag: \u0026#34;547fa7e369ef56031dd3bff2ace9fc0832eb251a\u0026#34; Keep-Alive: timeout=5, max=1000 Last-Modified: Tue, 19 Jul 2016 00:59:33 GMT Server: Apache Transfer-Encoding: chunked Vary: Cookie, Accept-Encoding (content) HTTP/1.1 在 1997 年 1 月以 RFC 2068 文件发布。\nHTTP 用于安全传输\nHTTP 最大的变化发生在 1994 年底。HTTP 在基本的 TCP/IP 协议栈上发送信息，网景公司（Netscape Communication）在此基础上创建了一个额外的加密传输层：SSL 。SSL 1.0 没有在公司以外发布过，但 SSL 2.0 及其后继者 SSL 3.0 和 SSL 3.1 允许通过加密来保证服务器和客户端之间交换消息的真实性，来创建电子商务网站。SSL 在标准化道路上最终成为 TLS，随着版本 1.0, 1.1, 1.2 的出现成功地关闭漏洞。TLS 1.3 目前正在形成。\n与此同时，人们对一个加密传输层的需求也愈发高涨：因为 Web 最早几乎是一个学术网络，相对信任度很高，但如今不得不面对一个险恶的丛林：广告客户、随机的个人或者犯罪分子争相劫取个人信息，将信息占为己有，甚至改动将要被传输的数据。随着通过 HTTP 构建的应用程序变得越来越强大，可以访问越来越多的私人信息，如地址簿，电子邮件或用户的地理位置，即使在电子商务使用之外，对 TLS 的需求也变得普遍。\nHTTP 用于复杂应用\nTim Berners-Lee 对于 Web 的最初设想不是一个只读媒体。 他设想一个 Web 是可以远程添加或移动文档，是一种分布式文件系统。 大约 1996 年，HTTP 被扩展到允许创作，并且创建了一个名为 WebDAV 的标准。 它进一步扩展了某些特定的应用程序，如 CardDAV 用来处理地址簿条目，CalDAV 用来处理日历。 但所有这些 *DAV 扩展有一个缺陷：它们必须由要使用的服务器来实现，这是非常复杂的。并且他们在网络领域的使用必须保密。\n在 2000 年，一种新的使用 HTTP 的模式被设计出来：representational state transfer (或者说 REST)。 由 API 发起的操作不再通过新的 HTTP 方法传达，而只能通过使用基本的 HTTP / 1.1 方法访问特定的 URI。 这允许任何 Web 应用程序通过提供 API 以允许查看和修改其数据，而无需更新浏览器或服务器：所有需要的内容都被嵌入到由网站通过标准 HTTP/1.1 提供的文件中。 REST 模型的缺点在于每个网站都定义了自己的非标准 RESTful API，并对其进行了全面的控制；不同于 *DAV 扩展，客户端和服务器是可互操作的。 RESTful API 在 2010 年变得非常流行。\n自2005年以来，可用于 Web 页面的API大大增加，其中几个API为特定目的扩展了HTTP协议，大部分是新的特定HTTP头：\nServer-sent events 服务器可以偶尔推送消息到浏览器 Websocket 一个新协议，可以通过升级现有 HTTP 协议来建立 放松安全措施-基于当前的web模型\nHTTP 和 Web安全模型 \u0026ndash; 同源策略是互不相关的。事实上，当前的Web安全模型是在HTTP被创造出来后才被发展的！这些年来，已经在证实了它如果能通过在特定的约束下移除一些这个策略的限制来管的宽松些的话，将会更有用。这些策略导致大量的成本和时间被话费在通过转交到服务端来添加一些新的HTTP头来发送。这些被定义在了 Cross-Origin Resource Sharing(CORS) or the Content Security Policy(CSP)规范里。\n不只是这大量的扩展，很多的其他的头也被加了进来，有些只是实验性的，比较著名的有 Do Not Track(DNT)来控制隐私， X-Frame-Option，还有很多\nHTTP/2 - 为了更优异的表现\n这些年来，网页愈渐变得的复杂，甚至演变成了独有的应用，可见媒体的播放量，增进交互的脚本大小也增加了许多：更多的数据通过 HTTP 请求被传输。HTTP/1.1 链接需要请求以正确的顺序发送，理论上可以用一些并行的链接（尤其是 5 到 8 个），带来的成本和复杂性堪忧。比如，HTTP 管线化（pipelining）就成为了 Web 开发的负担。\n在 2010 年到 2015 年，谷歌通过实践了一个实验性的 SPDY 协议，证明了一个在客户端和服务器端交换数据的另类方式。其收集了浏览器和服务器端的开发者的焦点问题。明确了响应数量的增加和解决复杂的数据传输，SPDY 成为了 HTTP/2 协议的基础。\nHTTP/2 和 HTTP/1.1 有几处基本的不同：\nHTTP/2 是二进制协议而不是文本协议。不在可读，也不是无障碍的手动创建，改善的优化技术现在可被实施。 这是一个复用协议。并行的请求能在同一个链接中处理，移除了HTTP/1.x 中顺序和阻塞的约束。 压缩了headers，因为 headers 在一系列请求中常常是相似的，其移除了重复和传输重复数据的成本。 其允许服务器在客户端缓存中填充数据，通过一个叫服务器推送的机制来提前请求。 HTTP/3 - 基于UDP的QUIC协议实现\n在HTTP/3中，将启用TCP协议，改为使用基于UDP协议的QUIC协议实现，此改变是为了解决HTTP/2中存在的队头阻塞问题，由于HTTP/2在单个TCP连接上使用了多路复用，收到TCP拥塞控制的影响，少量的丢包就可能导致整个TCP连接上的所有流被阻塞。\nQUIC（快速UDP网络连接）是一种实验性的网络传输协议，由Google开发，该协议旨在使网页传输更快。\n","date":"2022-06-17T10:09:23+08:00","permalink":"https://x-xkang.com/p/http-%E5%8D%8F%E8%AE%AE%E7%89%88%E6%9C%AC%E8%BF%9B%E5%8C%96/","title":"Http 协议版本进化"},{"content":"DNS 基本概述 DNS（Domain Name System）域名系统是一个将ip地址映射称为域名或者将域名映射成为ip地址的一种服务。DNS协议是应用层协议，运行在UDP之上，使用53端口。DNS使用客户 - 服务器模式运行在通信的端系统之间，在通信的端系统之间通过端到端的传输协议来传送DNS报文。\n下面通过一个示例来简单描述DNS的解析过程，例如，你在浏览器中输入了 \u0026ldquo;mail.qq.com\u0026rdquo;，实际会发生以下操作：\n同一台用户主机上运行着 DNS 应用的客户端 浏览器将主机名 \u0026ldquo;mail.qq.com\u0026rdquo; 发送给 DNS应用的客户端 DNS 客户端向 DNS 服务端发送一个包含主机名的请求 DNS 客户端最终会收到一份回答报文，其中包含该目标主机的IP地址 一旦浏览器收到目标主机的IP地址后，它就能够向位于该IP地址80端口的HTTP服务器进程发起一个TCP连接 除了提供IP地址到主机名的转换，DNS还提供了下面几个重要的服务\n主机别名（host aliasing）,有着复杂的主机名的主机能够拥有一个或者多个其他别名，比如说一台名为 aaabbbcdef.com 的主机，同时会拥有 a.com 和 b.com 两个主机别名，在这种情况下 aaabbbcdef.com 也称为规范主机名，而主机别名比规范主机名更加容易记忆。应用程序可以调用 DNS 来获得主机别名对应的规范主机名以及对应的IP地址。 邮件服务器别名（mail server aliasing），同样的，电子邮件的应用程序也可以调用 DNS 对提供的主机名进行解析。 负载均衡（load distribution），DNS也用于冗余的服务器之间进行负载分配。繁忙的站点例如 cnn.com 被冗余分布在多台服务器上，每台服务器运行在不同的端系统之间，每个都有着不同的IP地址。由于这些冗余的Web服务器，一个IP地址集合因此与同一个规范主机名联系。DNS数据库中存储着这些IP地址的集合。由于客户端每次都会发起 HTTP 请求，所以DNS就会在所有这些冗余的Web服务器之间循环分配了负载。 DNS的域名空间结构 域名系统作为一个层次结构和分布式数据库，包含各种类型的数据，包括主机名和域名。DNS数据库中的名称形成一个分层树状结构称为命名空间\n了解域名结构\n以mail.qq.com域名为例，com为顶级域名，qq.com 为二级域名， mail.qq.com为三级域名\n顶级域名服务器\n顶级域名为最后一个.右侧部分的内容，如mail.qq.com的com就是顶级域名，顶级域名对应的服务器称之为顶级域名服务器\n二级域名服务器\n域名 mail.qq.com中的倒数第二个.右侧部分qq.com成为二级域名\n根域名服务器\n在2016年之前全球一共拥有13台根服务器，1台主根服务器在美国，其他12台为辅根服务器，其中美国9台，英国1台，瑞典1台，日本1台，这13台根服务器主要管理互联网的主目录，主要作用IPV4。 2016年，中国下一代互联网工程中心领衔发起雪人计划，旨在为下一代互联网(IPV6)提供更多的根服务器解决方案，该计划于2017年完成，其中包含3台主根服务器，中国1台，美国1台，日本1台，22台辅根服务器，中国3台，美国2台，印度和法国分别有3台，德国2台，俄罗斯、意大利、西班牙、奥地利、智利、南非、澳大利亚、瑞士、荷兰各1台，共22台，从此形成了13台原有根加25台IPV6根服务器的新格局\n本地域名服务器\n本地域名服务器的范围非常广，没有一个详细的定位，可能是某个运营商部署在该城市的一台服务器，也可能是某个公司的一台服务器，甚至可能是某个学校的服务器\n根域名 DNS 域名使用中规定由尾部句点 . 来指定名称位于根或者更高层次的域层次结构\n顶级域名 用来表示国家、地区或者组织。采用三个字符，如 com 表示商业公司，edu 表示教育机构，net 表示网络公司， gov 表示非军事政府机构等等。\n二级域名 表示归属于某个公司或组织的域名\nDNS 工作概述 假设运行在用户主机上的某些应用程序（如Web浏览器或邮件阅读器）需要将主机名转换成 IP 地址。这些应用程序将调用 DNS 的客户端，并指明需要被转换的主机名。用户主机上的 DNS 收到后，会使用 UDP 通过 53 端口向网络上发送一个 DNS 查询报文，经过一段时间后，用户主机以上的 DNS 会收到一个主机名对应的DNS回答报文。因此，从用户主机的角度来看，DNS就像是一个黑盒子，其内部的操作你无法看到。但是实际上，实现DNS这个服务的黑盒子非常复杂，它由分布于全球的大量DNS服务器以及定义了DNS服务器与查询主机通信方式的应用层协议组成。\nDNS 最早的设计是只有一台 DNS 服务器。这台服务器会包含所有的 DNS 映射。这是一种集中式的设计，这种设计并不适用于当今的互联网，因为互联网有着数量巨大并且持续增长的主机，这种集中式的设计会存在以下几个问题：\n单点故障（a single point of failure），如果 DNS 服务器崩溃，那么整个网络随之瘫痪。 通信容量(traffic volume)，单个 DNS 服务器不得不处理所有的DNS查询，这种查询级别可能是百万、千万级 远距离集中式数据库（distant centralized databases），单个 DNS 服务器不可能邻近所有的用户，假设在美国的 DNS 服务器不可能邻近让澳大利亚的查询使用，其中查询请求势必会经过低速和拥堵的链路，造成严重的时延。 维护（maintenance），维护成本巨大，而且需要频繁更新。 存放地址\n浏览器缓存 系统缓存 本地域名服务器 根域名服务器 顶级域名服务器 二级域名服务器 \u0026hellip; \u0026hellip; 查询顺序\n检查浏览器缓存中是否存在改域名与IP地址的映射关系，如果有则解析结束，没有则继续 到系统本地查找映射关系，一般存在 hosts 文件中，如果有则解析结束，否则继续 本地域名服务器去查询 根域名服务器，该过程不会返回映射关系，只会告诉你去下级服务器（顶级域名服务器）查询 到本地域名服务器去查询，有则结束，否则继续 本地域名服务器查询顶级域名服务器（即 com 服务器），同样不会返回映射关系，只会引导你去二级域名服务器查询 本地域名服务器查询二级域名服务器（即 qq.com 服务器），引导去三级域名服务器 本地域名服务器查询三级域名服务器（即 mail.qq.com）， 此时已经是最后一级，如果有则返回映射关系，并且本地服务器加入自身的映射表中，方便下次查询，同时返回给用户的计算机，没有找到则网页报错 ","date":"2022-06-16T17:00:53+08:00","permalink":"https://x-xkang.com/p/dns-%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E6%B5%81%E7%A8%8B/","title":"DNS 域名解析流程"},{"content":" OSI是什么？ OSI(Open System Interconnect)，即开放式系统互联。一般都叫OSI参考模型，是ISO(International Organization for Standardization)国际标准化组织在1985年研究的网络互联模型 ISO 为了更好的使网络应用更为普及，推出了OSI参考模型，其含义就是推荐所有公司使用这个规范来控制网络，这样所有公司都有相同的规范，就能互联了。\n数据封装的概念 封装即每一层接收上层数据时，都会添加自己特定的头部数据（有时也会有尾部数据）。这些头部数据实际上就是实现协议的规定。即加上相应的“暗号”。\nOSI七层模型与TCP/IP四层模型对比 OSI 七层模型 TCP/IP 四层模型 对应网络协议 应用层 应用层 HTTP、TFTP, FTP, NFS, WAIS、SMTP 表示层 Telnet, Rlogin, SNMP, Gopher 会话层 SMTP, DNS 传输层 传输层 TCP, UDP 网络层 网络层 IP, ICMP, ARP, RARP, AKP, UUCP 数据链路层 数据链路层 FDDI, Ethernet, Arpanet, PDN, SLIP, PPP 物理层 IEEE 802.1A, IEEE 802.2到IEEE 802.11 应用层：为应用程序或用户请求提供请求服务。OSI参考模型最高层，也是最靠近用户的一层，为计算机用户、各种应用程序以及网络提供端口，也为用户直接提供各种网络服务。传输的数据格式为消息体（Message）\n表示层：数据编码、格式转换、数据加密。提供各种用于应用层的编码和转换功能，确保一个系统的应用层发送的数据能被另一个系统的应用层识别。如果必要，该层可提供一种标准表示形式，用于讲计算机内部的各种数据格式转换成通信中采用的标准表示形式。数据压缩和加密也是表示层可提供的转换功能之一。\n会话层：创建、管理和维护会话。接收来自传输层的数据，负责建立、管理和终止表示层实体之间的通信会话，支持它们之间的数据交换。该层的通信由不同设备中的应用程序之间的服务请求和响应组成。\n传输层：数据通信。建立主机端到端的链接，为会话层和网络层提供端到端可靠的和透明的数据传输服务，确保数据能完整的传输到网络层。传输的数据格式为报文段（Segment）\n网络层：IP选址及路由选择。通过路由选择算法，为报文或通信子网选择最适当的路径。控制数据链路层与传输层之间的信息转发，建立、维持和终止网络的连接。数据链路层的数据在这一层被转换为数据包(或数据报)，然后通过路径选择、分段组合、顺序、进/出路由等控制，将信息从一个网络设备传送到另一个网络设备。传输的数据格式是数据报（Datagram）\n数据链路层：提供介质访问和链路管理。接收来自物理层的位流形式的数据，封装成帧，传送到网络层；将网络层的数据帧，拆装为位流形式的数据转发到物理层；负责建立和管理节点间的链路，通过各种控制协议，将有差错的物理信道变为无差错的、能可靠传输数据帧的数据链路。传输的数据格式是数据帧（Frame）\n物理层：管理通信设备和网络媒体之间的互联互通。传输介质为数据链路层提供物理连接，实现比特流的透明传输。实现相邻计算机节点之间比特流的透明传送，屏蔽具体传输介质和物理设备的差异。传输的数据格式是比特(Bit)\n","date":"2022-06-15T15:57:49+08:00","permalink":"https://x-xkang.com/p/osi-%E4%B8%83%E5%B1%82%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/","title":"OSI 七层网络模型"},{"content":"一、HTTP状态码分类 HTTP 状态码由三个十进制数字组成，第一个十进制数字定义了状态码的类型，响应分为五类：消息响应（100-199），成功响应（200-299），重定向消息（300-399），客户端错误响应（400-499）和服务器错误响应（500-599）\n分类 分来描述 1** 信息，服务器收到请求，需要请求者继续执行操作 2** 操作被成功接收并处理 3** 重定向，需要进一步操作以完成请求 4** 客户端错误，请求包含语法错误或无法完成请求 5** 服务器错误，服务器在处理请求的过程发生了错误 二、状态码分类描述 状态码 英文状态码 中文描述 100 Continue 这个临时响应表明，迄今为止的所有内容都是可行的，客户端应该继续请求，如果以完成，请忽略它。 101 Switching Protocols 该代码是响应客户端 Upgrade(en-US). 请求头发送的，指明服务器即将切换的协议 102 Processing 此代码表示服务器以收到并且正在处理该请求，但当前没有响应可用 103 Early Hints 次状态码主要用于与 Link 链接头一起使用，以允许用户代理在服务器响应阶段时开始预加载 preloading 资源 200 OK 请求成功 201 Created 该请求已成功，并因此创建了一个新的资源。这通常是在 POST 请求，或是某些 PUT 请求之后返回的响应 202 Accepted 请求已经接收到，但还未响应，没有结果。意味着不会有一个异步的响应去表明当前请求的结果，预期另外的进程和服务去处理请求，或者批处理。 203 Non-Authoritative Information 服务器已成功处理了请求，但返回的实体头部元信息不是在原始服务器上有效的确定集合，而是来自本地或者第三方的拷贝。当前的信息可能是原始版本的子集或者超集。例如，包含资源的元数据可能导致原始服务器知道元信息的超集。使用此状态码不是必须的，而且只有在响应不使用此状态码便会返回200 OK的情况下才是合适的。 204 No Content 对于该请求没有的内容可发送，但头部字段可能有用。用户代理可能会用此时请求头部信息来更新原来资源的头部缓存字段。 205 Reset Content 告诉用户代理重置发送此请求的文档 206 Partial Content 当从客户端发送Range范围标头以只请求资源的一部分时，将使用此响应代码。 207 Multi Status 对于多个状态代码都可能合适的情况，传输有关多个资源的信息。 208 Already Reported 在 DAV 里面使用 dav:propstat 响应元素以避免重复枚举多个绑定的内部成员到同一个集合。 226 IM Used 服务器已经完成了对资源的GET请求，并且响应是对当前实例应用的一个或多个实例操作结果的表示。 300 Multiple Choice 请求拥有不只一个的可鞥响应。用户带来或者用户应当从中选择一个。 (没有标准化的方法来选择其中一个响应，但是建议使用指向可能性的 HTML 链接，以便用户可以选择。) 301 Moved Permanently 请求资源的 URL 已永久更改。在响应中给出了新的 URL。 302 Found 此响应代码表示所请求资源的 URI 已 暂时 更改。未来可能会对 URI 进行进一步的改变。因此，客户机应该在将来的请求中使用这个相同的 URI。 303 See Other 服务器发送此响应，以指示客户端通过一个 GET 请求在另一个 URI 中获取所请求的资源 304 Not Modified 这是用于缓存的目的。它告诉客户端响应还没有被修改，因此客户端可以继续使用相同的缓存版本的响应。 305 Use Proxy 在 HTTP 规范中定义，以指示请求的响应必须被代理访问。由于对代理的带内配置的安全考虑，它已被弃用 306 unused 此响应代码不再使用；它只是保留。它曾在 HTTP/1.1 规范的早期版本中使用过。 307 Temporary Redirect 服务器发送此响应，以指示客户端使用在前一个请求中使用的相同方法在另一个 URI 上获取所请求的资源。这与 302 Found HTTP 响应代码具有相同的语义，但用户代理 不能 更改所使用的 HTTP 方法：如果在第一个请求中使用了 POST，则在第二个请求中必须使用 POST 308 Permanent Redirect 这意味着资源现在永久位于由Location: HTTP Response 标头指定的另一个 URI。 这与 301 Moved Permanently HTTP 响应代码具有相同的语义，但用户代理不能更改所使用的 HTTP 方法：如果在第一个请求中使用 POST，则必须在第二个请求中使用 POST。 400 Bad Request 由于被认为是客户端错误（例如，错误的请求语法、无效的请求消息帧或欺骗性的请求路由），服务器无法或不会处理请求。 401 Unauthorized 虽然 HTTP 标准指定了\u0026quot;unauthorized\u0026quot;，但从语义上来说，这个响应意味着\u0026quot;unauthenticated\u0026quot;。也就是说，客户端必须对自身进行身份验证才能获得请求的响应。 402 Payment Required 此响应代码保留供将来使用。创建此代码的最初目的是将其用于数字支付系统，但是此状态代码很少使用，并且不存在标准约定。 403 Forbidden 客户端没有访问内容的权限；也就是说，它是未经授权的，因此服务器拒绝提供请求的资源。与 401 Unauthorized 不同，服务器知道客户端的身份。 404 Not Found 服务器找不到请求的资源。在浏览器中，这意味着无法识别 URL。在 API 中，这也可能意味着端点有效，但资源本身不存在。服务器也可以发送此响应，而不是 403 Forbidden，以向未经授权的客户端隐藏资源的存在。这个响应代码可能是最广为人知的，因为它经常出现在网络上。 405 Method Not Allowed 服务器知道请求方法，但目标资源不支持该方法。例如，API 可能不允许调用DELETE来删除资源。 406 Not Acceptale 当 web 服务器在执行 服务端驱动型内容协商机制](/zh-CN/docs/Web/HTTP/Content_negotiation#服务端驱动型内容协商机制)后，没有发现任何符合用户代理给定标准的内容时，就会发送此响应。 407 Proxy Authentication Required 类似于 401 Unauthorized 但是认证需要由代理完成。 408 Request Timeout 此响应由一些服务器在空闲连接上发送，即使客户端之前没有任何请求。这意味着服务器想关闭这个未使用的连接。由于一些浏览器，如 Chrome、Firefox 27+ 或 IE9，使用 HTTP 预连接机制来加速冲浪，所以这种响应被使用得更多。还要注意的是，有些服务器只是关闭了连接而没有发送此消息。 409 Conflict 当请求与服务器的当前状态冲突时，将发送此响应。 410 Gone 当请求的内容已从服务器中永久删除且没有转发地址时，将发送此响应。客户端需要删除缓存和指向资源的链接。HTTP 规范打算将此状态代码用于“有限时间的促销服务”。API 不应被迫指出已使用此状态代码删除的资源。 411 Length Required 服务端拒绝该请求因为 Content-Length 头部字段未定义但是服务端需要它。 412 Precondition Failed 客户端在其头文件中指出了服务器不满足的先决条件。 413 Payload Too Large 请求实体大于服务器定义的限制。服务器可能会关闭连接，或在标头字段后返回重试 Retry-After。 414 URI Too Long 客户端请求的 URI 比服务器愿意接收的长度长。 415 Unsupported Media Type 服务器不支持请求数据的媒体格式，因此服务器拒绝请求。 416 Rabge Not Satisfiale 无法满足请求中 Range 标头字段指定的范围。该范围可能超出了目标 URI 数据的大小。 417 Expectation Failed 此响应代码表示服务器无法满足 Expect 请求标头字段所指示的期望。 418 I'm Teapot 服务端拒绝用茶壶煮咖啡。笑话，典故来源茶壶冲泡咖啡 421 Misdirected Request 请求被定向到无法生成响应的服务器。这可以由未配置为针对请求 URI 中包含的方案和权限组合生成响应的服务器发送。 422 Unproccessable Entity 请求格式正确，但由于语义错误而无法遵循。 423 Locked 正在访问的资源已锁定。 424 Failed Dependency 由于前一个请求失败，请求失败。 425 Too Early 表示服务器不愿意冒险处理可能被重播的请求。 426 Upgrade Required 服务器拒绝使用当前协议执行请求，但在客户端升级到其他协议后可能愿意这样做。 服务端发送带有 Upgrade (en-US) 字段的 426 响应 来表明它所需的协议（们）。 428 Precondition Required 用户在给定的时间内发送了太多请求（\u0026ldquo;限制请求速率\u0026rdquo;） 431 Request Header Fields Too Large 服务器不愿意处理请求，因为其头字段太大。在减小请求头字段的大小后，可以重新提交请求。 451 Unavailable For Legal Reasons 用户代理请求了无法合法提供的资源，例如政府审查的网页。 500 Internal Server Error 服务器遇到了不知道如何处理的情况。 501 Not Implement 服务器不支持请求方法，因此无法处理。服务器需要支持的唯二方法（因此不能返回此代码）是 GET and HEAD. 502 Bad Gateway 此错误响应表明服务器作为网关需要得到一个处理这个请求的响应，但是得到一个错误的响应。 503 Service Unavailable 服务器没有准备好处理请求。常见原因是服务器因维护或重载而停机。请注意，与此响应一起，应发送解释问题的用户友好页面。这个响应应该用于临时条件和如果可能的话，HTTP 标头 Retry-After 字段应该包含恢复服务之前的估计时间。网站管理员还必须注意与此响应一起发送的与缓存相关的标头，因为这些临时条件响应通常不应被缓存。 504 Gateway Timeout 当服务器充当网关且无法及时获得响应时，会给出此错误响应。 505 HTTP Version Not Supported 服务器不支持请求中使用的 HTTP 版本。 506 Variant Also Negotiates 服务器存在内部配置错误：所选的变体资源被配置为参与透明内容协商本身，因此不是协商过程中的适当终点 507 Insufficient Storage 无法在资源上执行该方法，因为服务器无法存储成功完成请求所需的表示。 508 Loop Detected 服务器在处理请求时检测到无限循环。 510 Not Extented 服务器需要对请求进行进一步扩展才能完成请求。 511 Network Authentication Required 指示客户端需要进行身份验证才能获得网络访问权限。 ","date":"2022-06-14T21:33:58+08:00","permalink":"https://x-xkang.com/p/http-%E7%8A%B6%E6%80%81%E7%A0%81/","title":"Http 状态码"},{"content":" 常用正则表达式 字符 描述 \\ 将下一个字符标记为一个特殊字符、或一个原义字符、或一个向后引用、或一个八进制转义符。例如，“n”匹配字符“n”。“\\n”匹配一个换行符。串行“\\”匹配“\\”而“(”则匹配“(”。 ^ 匹配输入字符串的开始位置。如果设置了RegExp对象的Multiline属性，^也匹配“\\n”或“\\r”之后的位置。 $ 匹配输入字符串的结束位置。如果设置了RegExp对象的Multiline属性，$也匹配“\\n”或“\\r”之前的位置。 * 匹配前面的子表达式零次或多次。例如，zo*能匹配“z”以及“zoo”。*等价于{0,}。 + 匹配前面的子表达式一次或多次。例如，“zo+”能匹配“zo”以及“zoo”，但不能匹配“z”。+等价于{1,}。 ? 匹配前面的子表达式零次或一次。例如，“do(es)?”可以匹配“does”或“does”中的“do”。?等价于{0,1}。 {n} n是一个非负整数。匹配确定的n次。例如，“o{2}”不能匹配“Bob”中的“o”，但是能匹配“food”中的两个o。 {n,} n是一个非负整数。至少匹配n次。例如，“o{2,}”不能匹配“Bob”中的“o”，但能匹配“foooood”中的所有o。“o{1,}”等价于“o+”。“o{0,}”则等价于“o*”。 {n,m} m和n均为非负整数，其中n\u0026lt;=m。最少匹配n次且最多匹配m次。例如，“o{1,3}”将匹配“fooooood”中的前三个o。“o{0,1}”等价于“o?”。请注意在逗号和两个数之间不能有空格。 ? 当该字符紧跟在任何一个其他限制符（*,+,?，{n}，{n,}，{n,m}）后面时，匹配模式是非贪婪的。非贪婪模式尽可能少的匹配所搜索的字符串，而默认的贪婪模式则尽可能多的匹配所搜索的字符串。例如，对于字符串“oooo”，“o+?”将匹配单个“o”，而“o . 匹配除“\\n”之外的任何单个字符。要匹配包括“\\n”在内的任何字符，请使用像“(. (pattern) 匹配pattern并获取这一匹配。所获取的匹配可以从产生的Matches集合得到，在VBScript中使用SubMatches集合，在JScript中则使用$0…$9属性。要匹配圆括号字符，请使用“(”或“)”。 (?:pattern) 匹配pattern但不获取匹配结果，也就是说这是一个非获取匹配，不进行存储供以后使用。这在使用或字符“( (?=pattern) 正向肯定预查，在任何匹配pattern的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用。例如，“Windows(?=95 (?!pattern) 正向否定预查，在任何不匹配pattern的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用。例如“Windows(?!95 (?\u0026lt;=pattern) 反向肯定预查，与正向肯定预查类拟，只是方向相反。例如，“(?\u0026lt;=95 (?\u0026lt;!pattern) 反向否定预查，与正向否定预查类拟，只是方向相反。例如“(?\u0026lt;!95 x|y 匹配x或y。例如，“z [xyz] 字符集合。匹配所包含的任意一个字符。例如，“[abc]”可以匹配“plain”中的“a”。 [^xyz] 负值字符集合。匹配未包含的任意字符。例如，“[^abc]”可以匹配“plain”中的“p”。 [a-z] 字符范围。匹配指定范围内的任意字符。例如，“[a-z]”可以匹配“a”到“z”范围内的任意小写字母字符。 [^a-z] 负值字符范围。匹配任何不在指定范围内的任意字符。例如，“[^a-z]”可以匹配任何不在“a”到“z”范围内的任意字符。 \\b 匹配一个单词边界，也就是指单词和空格间的位置。例如，“er\\b”可以匹配“never”中的“er”，但不能匹配“verb”中的“er”。 \\B 匹配非单词边界。“er\\B”能匹配“verb”中的“er”，但不能匹配“never”中的“er”。 \\cx 匹配由x指明的控制字符。例如，\\cM匹配一个Control-M或回车符。x的值必须为A-Z或a-z之一。否则，将c视为一个原义的“c”字符。 \\d 匹配一个数字字符。等价于[0-9]。 \\D 匹配一个非数字字符。等价于[^0-9]。 \\f 匹配一个换页符。等价于\\x0c和\\cL。 \\n 匹配一个换行符。等价于\\x0a和\\cJ。 \\r 匹配一个回车符。等价于\\x0d和\\cM。 \\s 匹配任何空白字符，包括空格、制表符、换页符等等。等价于[ \\f\\n\\r\\t\\v]。 \\S 匹配任何非空白字符。等价于[^ \\f\\n\\r\\t\\v]。 \\t 匹配一个制表符。等价于\\x09和\\cI。 \\v 匹配一个垂直制表符。等价于\\x0b和\\cK。 \\w 匹配包括下划线的任何单词字符。等价于“[A-Za-z0-9_]”。 \\W 匹配任何非单词字符。等价于“[^A-Za-z0-9_]”。 \\xn 匹配n，其中n为十六进制转义值。十六进制转义值必须为确定的两个数字长。例如，“\\x41”匹配“A”。“\\x041”则等价于“\\x04\u0026amp;1”。正则表达式中可以使用ASCII编码。. \\num 匹配num，其中num是一个正整数。对所获取的匹配的引用。例如，“(.)\\1”匹配两个连续的相同字符。 \\n 标识一个八进制转义值或一个向后引用。如果\\n之前至少n个获取的子表达式，则n为向后引用。否则，如果n为八进制数字（0-7），则n为一个八进制转义值。 \\nm 标识一个八进制转义值或一个向后引用。如果\\nm之前至少有nm个获得子表达式，则nm为向后引用。如果\\nm之前至少有n个获取，则n为一个后跟文字m的向后引用。如果前面的条件都不满足，若n和m均为八进制数字（0-7），则\\nm将匹配八进制转义值nm。 \\nml 如果n为八进制数字（0-3），且m和l均为八进制数字（0-7），则匹配八进制转义值nml。 \\un 匹配n，其中n是一个用四个十六进制数字表示的Unicode字符。例如，\\u00A9匹配版权符号（©）。 ","date":"2022-01-11T09:29:51+08:00","permalink":"https://x-xkang.com/p/%E5%B8%B8%E7%94%A8%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/","title":"常用正则表达式"},{"content":"go:build go:linkname 1 //go:linkname localname importpath.name 该指令指示编译器使用 importpath.name 作为源码中声明为 localname 的变量的或函数的目标文件符号名称，但是由于这个伪指令可以破坏类型系统和包模块化，只有引用了 unsafe 包才可以使用。 简单来讲，就是 importpath.name 是 localname 的符号别名，编译器实际上会调用 localname，使用的前提是引入了 unsafe 包才能使用。\n例如 time/time.go\n1 2 // Provided by package runtime. func now() (sec int64, nsec int32, mono int64) runtime/timestub.go\n1 2 3 4 5 6 7 import _ \u0026#34;unsafe\u0026#34; // for go:linkname //go:linkname time_now time.now func time_now() (sec int64, nsec int32, mono int64) { sec, nsec = walltime() return sec, nsec, nanotime() } now 方法并没有具体实现，注释上也描述具体实现由 runtime 包完成，看一下 runtime 包中的代码，先引入了 unsafe 包，再定义了 //go:lickname time_now time.now。\n第一个参数time_now 代表本地变量或方法，第二个参数time.now标识需要建立链接的变量、方法路径。也就是说，//go:lickname 是可以跨包使用的。\n另外 go build 是无法编译 go:linkname的，必须使用单独的 compile 命令进行编译，因为 go build 会加上 -complete 参数，这个参数会检查到没有方法体的方法，并且不通过。\ngo:noscape 1 //go:noscape 该指令指定下一个有声明但没有主体（意味着实现有可能不是 Go）的函数，不允许编译器对其做逃逸分析。\n一般情况下，该指令用于内存分配优化。编译器默认会进行逃逸分析，会通过规则判定一个变量是分配到堆上还是栈上。\n但凡事有意外，一些函数虽然逃逸分析其是存放到堆上。但是对于我们来说，它是特别的。我们就可以使用 go:noescape 指令强制要求编译器将其分配到函数栈上。\n例如 1 2 3 4 // memmove copies n bytes from \u0026#34;from\u0026#34; to \u0026#34;to\u0026#34;. // in memmove_*.s //go:noescape func memmove(to, from unsafe.Pointer, n uintptr) 我们观察一下这个案例，它满足了该指令的常见特性。如下：\nmemmove_*.s: 只有声明，没有主体，其主体是由底层汇编实现的。\nmemmove: 函数功能，在栈上处理性能会更好。\ngo:noslip 1 //go:noslip 该指令指定文件中声明的下一个函数不得包含堆栈溢出检查。\n简单来讲，就是这个函数跳过堆栈溢出的检查。\n例如 1 2 3 4 //go:nosplit func key32(p *uintptr) *uint32 { return (*uint32)(unsafe.Pointer(p)) } go:nowritebarrierrec 1 //go:nowritebarrierrec 该指令表示编译器遇到写屏障时就会产生一个错误，并且允许递归。也就是这个函数调用的其他函数如果有写屏障也会报错。\n简单来讲，就是针对写屏障的处理，防止其死循环。\n例如 1 2 3 4 //go:nowritebarrierrec func gcFlushBgCredit(scanWork int64) { ... } go:yeswritebarrierrec 1 //go:yeswritebarrierrec 该指令与 go:nowritebarrierrec 相对，在标注 go:nowritebarrierrec 指令的函数上，遇到写屏障会产生错误。\n而当编译器遇到 go:yeswritebarrierrec 指令时将会停止。\n例如 1 2 3 4 //go:yeswritebarrierrec func gchelper() { ... } go:noinline 1 //go:noinline 该指令表示该函数禁止进行内联。\n1 2 3 4 //go:noinline func unexportedPanicForTesting(b []byte, i int) byte { return b[i] } 例如 我们观察一下这个案例，是直接通过索引取值，逻辑比较简单。如果不加上 go:noinline 的话，就会出现编译器对其进行内联优化。\n显然，内联有好有坏。该指令就是提供这一特殊处理。\ngo:norace 1 //go:norace 该指令表示禁止进行竞态检测。\n常见的形式就是在启动时执行 go run -race，能够检测应用程序中是否存在双向的数据竞争，非常有用。\n例如 1 2 3 4 //go:norace func forkAndExecInChild(argv0 *byte, argv, envv []*byte, chroot, dir *byte, attr *ProcAttr, sys *SysProcAttr, pipe int) (pid int, err Errno) { ... } go:notinheap 1 //go:notinheap 该指令常用于类型声明，它表示这个类型不允许从 GC 堆上进行申请内存。\n在运行时中常用其来做较低层次的内部结构，避免调度器和内存分配中的写屏障，能够提高性能。\n例如 1 2 3 4 5 6 7 8 9 // notInHeap is off-heap memory allocated by a lower-level allocator // like sysAlloc or persistentAlloc. // // In general, it\u0026#39;s better to use real types marked as go:notinheap, // but this serves as a generic type for situations where that isn\u0026#39;t // possible (like in the allocators). // //go:notinheap type notInHeap struct{} 每日一算 描述 2个逆序的链表，要求从低位开始相加，得出结果也逆序输出，返回值是逆序结果链表的头结点\n解题思路 需要注意的是进位的问题，极端情况如下：\nInput: (9 -\u0026gt; 9 -\u0026gt; 9 -\u0026gt; 9) + (1 -\u0026gt; ) Output 0 -\u0026gt; 0 -\u0026gt; 0 -\u0026gt; 0 -\u0026gt; 1\n为了处理方法统一，可以先建立一个虚拟头结点，这个虚拟头结点的Next指向真正的head，这样head不需要单独处理，直接wihle循环即可。另外判断循环终止的条件不用是 p.Next != nil，这样最后一位还需要额外计算，终止条件应该是 p != nil。\n代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 type ListNode struct { Val int Next *ListNode } func addTwoNumbers(l1 *ListNode, l2 *ListNode) *ListNode { if l1 == nil || l2 == nil{ return nil } // 虚拟头结点 head := \u0026amp;ListNode{ Val: 0, Next: nil, } current := head carry := 0\t// 是否需要进位 // 遍历 for l1 != nil || l2 != nil { var x, y int if l1 == nil { x = 0 }else{ x = l1.Val } if l2 == nil { y = 0 }else { y = l2.Val } current.Next = \u0026amp;ListNode{ Val: (x + y + carry) % 10, Next: nil, } current = current.Next carry = (x+y+carry) / 10 if l1 != nil { l1 = l1.Next } if l2 != nil { l2 = l2.Next } fmt.Println(\u0026#34;carry:\u0026#34;, carry) } if carry \u0026gt; 0 {\t// 最后一位相加又进位，要在尾结点再加一个结点 current.Next = \u0026amp;ListNode{ Val: carry % 10, Next: nil, } } return head.Next } ","date":"2021-12-16T09:35:02+08:00","permalink":"https://x-xkang.com/p/golang-%E6%BA%90%E7%A0%81%E9%87%8C%E7%9A%84-/go-%E6%8C%87%E4%BB%A4%E9%83%BD%E4%BB%A3%E8%A1%A8%E4%BB%80%E4%B9%88%E6%84%8F%E6%80%9D/","title":"Golang 源码里的 //go: 指令，都代表什么意思？"},{"content":"ASCII ascii码使用指定的7位或8位二进制数组合表示128或256种可能的字符，标准ASCII码也叫基础ASCII码，使用7位二进制数（剩下的1位二进制为0）来表示所有的大写和小写字母，数字0到9、标点符号，以及再美式英语中使用的特殊字符，其中：\n0 ~ 31及127（共33个）是控制字符或通信专用字符（其余为可显示字符），如控制字符：LF（换行）, CR（回车）；通信专用字符：SOH（文头）、ACK（确认）等；ASCII值为8、9、10和13分别转换为退格、制表、换行和回车，它们并没有特定的图形显示，但会依不同的应用程序，而对文本显示不同的影响。 32 ~ 126（共95个）是字符（32是空格），其中48~57位0到9十个阿拉伯数字，65 ~ 90为26个大写英文字母，97 ~ 122为26个小写英文字母，其余为一些标点符号、运算符号等。 同时还要注意，在标准ASCII中，其中最高位（b7）用作奇偶校验。所谓奇偶校验，是指在代码传送过程中用来校验是否出现错误的一种方法，一般分为奇校验和偶校验两种。\n奇校验规定：正确的代码一个字节中1的个数必须是1，若非技术，则在最高位b7添1. 偶校验规定：正确的代码一个字节中1的个数必须是偶数，若非偶数，则在最高位（b7）添1. 后128个称为扩展ASCII码。许多基于x86的系统都支持使用扩展（或“高”ASCII）。扩展ASCII码允许将每个字符的第8位用于确定附加的128个特殊符号字符、外来语字母和图形符号\nUnicode Unicode是国际组织制定的可以容纳世界上所有文字和符号的字符编码方案。Unicode用数字0-0x10FFFF来映射这些字符，最多可以容纳1114112个字符，或者说有1114112个码位。码位就是可以分配给字符的数字。UTF-8、UTF-16、UTF-32都是将数字转换到程序数据的编码方案。\nUnicode 源于一个很简单的想法：将全世界所有的字符包含在一个集合里，计算机只要支持这一个字符集，就能显示所有的字符，再也不会有乱码了。\n它从 0 开始，为每个符号指定一个编号，这叫做”码点”（code point）。比如，码点 0 的符号就是 null（表示所有二进制位都是 0）。 这么多符号，Unicode 不是一次性定义的，而是分区定义。每个区可以存放 65536 个（2^16）字符，称为一个平面（plane）。目前，一共有 17 个平面，也就是说，整个 Unicode 字符集的大小现在是 2^21。 最前面的 65536 个字符位，称为基本平面（缩写 BMP），它的码点范围是从 0 一直到 2^16-1，写成 16 进制就是从 U+0000 到 U+FFFF。所有最常见的字符都放在这个平面，这是 Unicode 最先定义和公布的一个平面。 剩下的字符都放在辅助平面（缩写 SMP），码点范围从 U+010000 一直到 U+10FFFF。 Unicode 只规定了每个字符的码点，到底用什么样的字节序表示这个码点，就涉及到编码方法。 Unicode 编码方案 之前提到，Unicode 没有规定字符对应的二进制码如何存储。以汉字“汉”为例，它的 Unicode 码点是 0x6c49，对应的二进制数是 110110001001001，二进制数有 15 位，这也就说明了它至少需要 2 个字节来表示。可以想象，在 Unicode 字典中往后的字符可能就需要 3 个字节或者 4 个字节，甚至更多字节来表示了。 这就导致了一些问题，计算机怎么知道你这个 2 个字节表示的是一个字符，而不是分别表示两个字符呢？这里我们可能会想到，那就取个最大的，假如 Unicode 中最大的字符用 4 字节就可以表示了，那么我们就将所有的字符都用 4 个字节来表示，不够的就往前面补 0。这样确实可以解决编码问题，但是却造成了空间的极大浪费，如果是一个英文文档，那文件大小就大出了 3 倍，这显然是无法接受的。 于是，为了较好的解决 Unicode 的编码问题， UTF-8 和 UTF-16 两种当前比较流行的编码方式诞生了。当然还有一个 UTF-32 的编码方式，也就是上述那种定长编码，字符统一使用 4 个字节，虽然看似方便，但是却不如另外两种编码方式使用广泛。 UTF8 UTF-8 是一个非常惊艳的编码方式，漂亮的实现了对 ASCII 码的向后兼容，以保证 Unicode 可以被大众接受。\nUTF-8 是目前互联网上使用最广泛的一种 Unicode 编码方式，它的最大特点就是可变长。它可以使用 1 - 4 个字节表示一个字符，根据字符的不同变换长度。编码规则如下： 1.对于单个字节的字符，第一位设为 0，后面的 7 位对应这个字符的 Unicode 码点。因此，对于英文中的 0 - 127 号字符，与 ASCII 码完全相同。这意味着 ASCII 码那个年代的文档用 UTF-8 编码打开完全没有问题。 2.对于需要使用 N 个字节来表示的字符（N \u0026gt; 1），第一个字节的前 N 位都设为 1，第 N + 1 位设为 0，剩余的 N - 1 个字节的前两位都设位 10，剩下的二进制位则使用这个字符的 Unicode 码点来填充。 编码规则如下：\nUnicode 十六进制码点范围 UTF-8 二进制 0000 0000 - 0000 007F 0xxxxxxx 0000 0080 - 0000 07FF 110xxxxx 10xxxxxx 0000 0800 - 0000 FFFF 1110xxxx 10xxxxxx 10xxxxxx 0001 0000 - 0010 FFFF 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx 根据上面编码规则对照表，进行 UTF-8 编码和解码就简单多了。下面以汉字“汉”为利，具体说明如何进行 UTF-8 编码和解码。\n“汉”的 Unicode 码点是 0x6c49（110 1100 0100 1001），通过上面的对照表可以发现，0x0000 6c49 位于第三行的范围，那么得出其格式为 1110xxxx 10xxxxxx 10xxxxxx。接着，从“汉”的二进制数最后一位开始，从后向前依次填充对应格式中的 x，多出的 x 用 0 补上。这样，就得到了“汉”的 UTF-8 编码为 11100110 10110001 10001001，转换成十六进制就是 0xE6 0xB7 0x89。\n解码的过程也十分简单：如果一个字节的第一位是 0 ，则说明这个字节对应一个字符；如果一个字节的第一位 1，那么连续有多少个 1，就表示该字符占用多少个字节。\n每日一算 描述 在数组中找到 2 个数之和等于给定值的数字，结果返回 2 个数字在数组中的下标\n解题思路 利用map的特性，遍历数组预算计算出和给定值的差值，也就是目标元素，若值已经在map中，说明已找到，若没有，则把当前元素以 值=\u0026gt;下标 的形式存到map中\n代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func twoSum(arr []int, target int)[]int{ var m = map[int]int{} for i := 0; i \u0026lt; len(arr); i++ { another := target - arr[i] if _, ok := m[another]; ok == true { return []int{m[another], i} }else{ m[arr[i]] = i } } return []int{-1, -1} } ","date":"2021-12-02T09:31:17+08:00","permalink":"https://x-xkang.com/p/asciiunicode%E5%92%8Cutf8/","title":"ASCII、Unicode和UTF8"},{"content":"问题描述 生产环境每隔一段时间会出现mysql数据库的死锁日志:\n同一条update语句，where 条件不同，但是会触发死锁，于是查看阿里云的数据库死锁日志，如下：\nupdate 语句如果使用的是主键索引，会将主键索引锁住，如果是普通索引，会先将普通索引锁住，然后根据普通索引的主键id再锁住主键索引，同一条update语句的索引执行应该是一样的，不应该存在互相等待释放的情况，于是有点陷入僵局，google一下有遇到相似问题的帖子，查看了执行计划，之前也看了执行计划，但是忽略了type字段和extra字段，结果如下：\n索引合并查询，会同时使用idx_status_vmstatus 和 uniq_instance 扫描记录并给普通索引加锁，然后通过普通索引中的主键ID去锁定主键索引，问题就出现在这里，由于 idx_status_vmstatus 索引扫描和 uniq_instance索引扫描是同时的，如果两条update语句同时执行，则 事务2 先锁定 锁定 uniq_instance 成功后锁定对应的主键，然后事务1 锁定idx_status_vmstatus 成功后也去锁定主键,此时主键已被事务2锁定，于是阻塞等待primary释放，接着事务2开始扫描 idx_status_vmstatus 发现普通索引被事务1锁住，于是阻塞等待idx_status_vmstatus，于是出现最终的 事务2等待 事务2释放idx_status_vmstatus，事务1等待事务1释放primary，即出现死锁。\n解决方案也比较简单，先查出主键ID，使用主键ID再更新记录，因为使用主键ID直接加锁的话，锁粒度更小，及时同时更新一条记录，也不会出现同时等待对方将锁释放的场景。问题描述的比较简单，但在排查过程中还是走了不少弯路的。\n","date":"2021-11-22T10:48:54+08:00","permalink":"https://x-xkang.com/p/mysql-deadlock/","title":"Mysql Deadlock"},{"content":"一、GMT 1、什么是GMT GMT (Greenwich Mean Time) 格林威治标准时间\n它规定太阳每天经过位于英国伦敦郊区的皇家格林威治天文台的时间为中午12点。 2、GMT的历史 格林威治皇家天文台为了海上霸权的扩张计划，在十七世纪就开始进行天体观测。为了天文观测，选择了穿过英国伦敦格林威治天文台子午仪中心的一条经线作为零度参考线，这条线，简称格林威治子午线。\n1884年10月在美国华盛顿召开了一个国际子午线会议，该会议将格林威治子午线设定为本初子午线，并将格林威治平时 (GMT, Greenwich Mean Time) 作为世界时间标准（UT, Universal Time）。由此也确定了全球24小时自然时区的划分，所有时区都以和 GMT 之间的偏移量做为参考。\n1972年之前，格林威治时间（GMT）一直是世界时间的标准。1972年之后，GMT 不再是一个时间标准了。\n由于地球在它的椭圆轨道里的运动速度不均匀，这个时间可能和实际的太阳时相差16分钟。\n二、UTC 1、什么是UTC ？ UTC (Coodinated Universal Time)，协调世界时\n又称世界统一时间、世界标准时间、国际协调时间。由于英文（CUT）和法文（TUC）的缩写不同，作为妥协，简称UTC\nUTC是现在全球通用的时间标准，全球各地都同意将各自的时间进行同步协调。UTC时间是经过平均太阳时（以格林威治时间GMT为准）、地轴运动修正后的新时标以及以秒为单位的国际原子时所综合精算而成\n在军事中协调世界时会用”Z“来表示。又由于Z在无线电联络中使用”Zulu“作代称，协调世界时也会被称为”Zulu time“。\n2、UTC的组成 原子时间（TAI，International Atomic Time）：结合了劝阻400个所有的原子钟而得到的时间，它决定了我们每个人的中标中，时间流动的速度。 世界时间（UT，Universal Time）： 也称天文时间，或太阳时，它的依据是地球的自转，我们用它来确定多少原子时，对应于一个地球日的时间长度。 3、UTC的历史 1960年，国际无线电咨询委员会规范统一了UTC的概念，并在次年投入实际使用。1967年以前，UTC被数次调整过，原因是要使用润秒（leap second）来将UTC和地球自转时间进行统一。\n三、GMT与UTC GMT是前世界标准时，UTC是现世界标准时； UTC比GMT更准确，以原子时计时，适应现代社会的精准计时，但在不需要精确到秒的情况下，二者可视为等同； 每年格林尼治天文台会发调时信息，给予UTC。 ","date":"2021-09-29T13:38:27+08:00","permalink":"https://x-xkang.com/p/gmt-%E4%B8%8E-utc-%E6%97%B6%E9%97%B4%E6%A0%BC%E5%BC%8F/","title":"GMT 与 UTC 时间格式"},{"content":"我们平时使用的查询 sql 基本格式如下：\n1 2 3 4 5 6 7 8 9 SELECT DISTINCT \u0026lt;select_list\u0026gt; FROM \u0026lt;left_table\u0026gt; \u0026lt;join_type\u0026gt; JOIN \u0026lt;right_table\u0026gt; ON \u0026lt;join_condition\u0026gt; WHERE \u0026lt;where_condition\u0026gt; GROUP BY \u0026lt;group_by_condition\u0026gt; HAVING \u0026lt;having_condition\u0026gt; ORDER BY \u0026lt;order_by\u0026gt; LIMIT \u0026lt;limit_number\u0026gt;; 实际的执行顺序并不是如上书写顺序一样的：\nFROM: 对 from 左右的表计算笛卡尔积，产生虚拟表VT1； ON: 对笛卡尔积进行筛选，只有符合条件的行才会被记录到虚拟表VT2中； JOIN: 如果是 OUT JOIN，那么将保留表中（如左表或者右表）未匹配的行作为外部行添加到虚拟表VT2中，从而产生了虚拟表VT3； WHERE: 对 JOIN 之后的虚拟表VT3进行进一步的筛选，满足条件的留下生成虚拟表VT4； GROUP BY: 对虚拟表VT4进行分组，生成VT5； HAVING: 对分组后的VT5进行筛选，生成虚拟表VT6； SELECT: 选择 SELECT 指定的列，插入到虚拟表VT7中； DISTINCT: 对虚拟表VT7中的数据进行去重，产生VT8； ORDER BY: 对虚拟表VT8的中的数据进行排序生成VT9； LIMIT: 取出VT9中指定行的数据，产生虚拟表VT10，并返回数据 ","date":"2021-09-25T17:18:01+08:00","permalink":"https://x-xkang.com/p/mysql--%E5%85%B3%E9%94%AE%E5%AD%97%E7%9A%84%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F/","title":"Mysql -- 关键字的执行顺序"},{"content":"一、Int 1、string 转 int 1 2 3 4 5 6 var str = \u0026#34;1001\u0026#34; n, _ := strconv.Atoi(str) // 官方 \u0026#34;strconv\u0026#34; 包 fmt.Printf(\u0026#34;type:%T, value:%v\\n\u0026#34;, n, n) // type:int, value:1001 2、string 转 int64 1 2 3 4 5 6 var str = \u0026#34;1001\u0026#34; n, _ := strconv.ParseInt(str, 10, 64) fmt.Printf(\u0026#34;type:%T, value:%v\\n\u0026#34;, n, n) // type:int64, value:1001 3、string 转浮点数 1 2 3 4 5 6 var str = \u0026#34;3.1415926\u0026#34; n, _ := strconv.ParseFloat(str, 10) fmt.Printf(\u0026#34;type:%T, value:%v\\n\u0026#34;, n, n) // type:float64, value:3.1415926 二、String 1、int 转 string 1 2 3 4 5 6 var n int = 100 str := strconv.Itoa(n) fmt.Printf(\u0026#34;type:%T, value:%v\\n\u0026#34;, str, str) // type:string, value:100 2、int64 转 string 1 2 3 4 5 6 var n int64 = 100 str := strconv.FormatInt(n, 10) fmt.Printf(\u0026#34;type:%T, value:%v\\n\u0026#34;, str, str) // type:string, value:100 3、uint32 转 string 1 2 3 4 5 6 var n uint32 = 10 str := strconv.ParseUint(uint64(n), 10) fmt.Printf(\u0026#34;type:%T, value:%v\\n\u0026#34;, str, str) // type:string, value:100 三、Struct 1、json 转 struct 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 type Person struct { Name string `json:\u0026#34;name\u0026#34;` Age int `json:\u0026#34;age\u0026#34;` } var jsonStr = []byte(`{\u0026#34;name\u0026#34;: \u0026#34;neil\u0026#34;, \u0026#34;age\u0026#34;: 21}`) var p = Person{} err := json.Unmarshal(jsonStr, \u0026amp;p) // 官方 “json” 包 if err != nil { fmt.Println(\u0026#34;Unmarshal failed:\u0026#34;, err) return } fmt.Printf(\u0026#34;p:%#v\\n\u0026#34;, p) // p:main.Person{Name:\u0026#34;neil\u0026#34;, Age:21} 2、map 转 struct 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 type Person struct { Name string `json:\u0026#34;name\u0026#34;` Age int `json:\u0026#34;age\u0026#34;` } var m = map[string]interface{}{ \u0026#34;name\u0026#34;: \u0026#34;king\u0026#34;, \u0026#34;age\u0026#34;: 19, } jsonStr, err := json.Marshal(m) if err != nil { fmt.Println(\u0026#34;Marshal failed:\u0026#34;, err) return } err = json.Unmarshal(jsonStr, \u0026amp;p) if err != nil { fmt.Println(\u0026#34;Unmarshal failed too:\u0026#34;, err) return } fmt.Printf(\u0026#34;p:%#v\\n\u0026#34;, p) // p:main.Person{Name:\u0026#34;king\u0026#34;, Age:19} Map 1、json 转 map 1 2 3 4 5 6 7 8 9 10 11 12 13 14 var jsonStr = []byte(`{\u0026#34;name\u0026#34;: \u0026#34;neil\u0026#34;, \u0026#34;age\u0026#34;: 21}`) var m = map[string]interface{}{} err := json.Unmarshal(jsonStr, \u0026amp;m) if err != nil { fmt.Println(\u0026#34;Unmarshal failed:\u0026#34;, err) return } fmt.Printf(\u0026#34;m:%#v\\n\u0026#34;, m) // m:map[string]interface {}{\u0026#34;age\u0026#34;:21, \u0026#34;name\u0026#34;:\u0026#34;neil\u0026#34;} 2、 struct 转 map 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 type Person struct { Name string `json:\u0026#34;name\u0026#34;` Age int `json:\u0026#34;age\u0026#34;` } var p = Person{ Name: \u0026#34;king\u0026#34;, Age: 17, } var jsonStr, _ = json.Marshal(p) var m = map[string]interface{}{} _ = json.Unmarshal(jsonStr, \u0026amp;m) fmt.Printf(\u0026#34;m:%#v\\n\u0026#34;, m) // m:map[string]interface {}{\u0026#34;age\u0026#34;:17, \u0026#34;name\u0026#34;:\u0026#34;king\u0026#34;} ","date":"2021-06-24T13:58:33+08:00","permalink":"https://x-xkang.com/p/golang-%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2/","title":"Golang 常用数据类型转换"},{"content":"一、Mysql 事务 MySQL 事务主要用于处理操作量大、复杂度高的数据\nMySQL 数据库中只有 Innodb 存储引擎支持事务操作 事务处理可以用来维护数据库的完整性，保证成批的 SQL 要么全部执行，要么全部不执行 事务用来管理insert，update，delete语句 二、事务特性 一般来说，事务必须满足 4 个条件（ACID），即原子性、一致性、持久性、隔离性，具体如下：\n1、原子性 (Atomicity) 一个事务（Transaction）中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节，事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像事务没有执行过一样。\n2、一致性（Consistency） 在事务开始之前以及事务结束之后，数据库的完整性没有被破坏。这标识写入的数据必须完全符合所有的预设规则，这包含数据的精确度，串联性以及后续数据库可以自发的完成预定的工作。\n3、隔离性（Isolation） 数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致，事务隔离分为以下不同级别：\n读未提交（Read uncommited）: 允许脏读，也就是可能读到其他会话中未提交事务修改的数据。\n读已提交（Read commited）: 只能读取到已提交的数据。\n可重复读（Repeatable read）: 在同一个事务内的查询都是从开始时刻一致的，InnoDB 存储引擎默认的事务隔离级别就是可重复读，在 SQL 标准中，该隔离级别消除了不可重复读，但还是存在幻读。\n串行化（Serializable）: 完全串行化的读，每次读都需要获得表级的共享锁，读写相互都会阻塞。\n4、持久性（Durability） 事务处理结束后，对数据的修改就是永久的，几遍系统故障也不会丢失。\n三、事务的并发处理 准备工作：创建数据表，插入一条数据\n1 2 3 4 5 6 7 8 create table user( id int(10) not null auto_increment comment \u0026#39;主键ID\u0026#39;, name varchar(30) not null default \u0026#39;\u0026#39; comment \u0026#39;用户名\u0026#39;, primary key(id) ) engine=innodb charset=utf8mb4; # 插入数据 insert into `user`(`name`) values(\u0026#39;老王01\u0026#39;); 事务并发可能出现的情况：\n脏读 一个事务读到了另一个未提交事务修改过的数据\n1、会话 B 开启一个事务，把id=1的name改为老王01；\n2、会话 A 也开启一个事务，读取id=1的name，次时的查询结果为老王02；\n3、会话 B 的事务回滚了修改的操作，这样会话 A 读到的数据就是不存在的；\n这个现象就是脏读。（脏读只会在读未提交的隔离级别中才会出现）。\n不可重复读 一个事务只能读到另一个已经提交的事务修改过的数据，并且其他事务每对该数据进行一次修改并提交后，该事务都能查询得到最新值。（不可重复读在读未提交和读已提交隔离级别都可能会出现）\n1、会话 A 开启事务，查询id=1的 name 是老王01；\n2、会话 B 将id=1的 name 更新为老王02（隐式事务，autocommit=1，执行完 sql 会自动 commit）；\n3、会话 A 再查询时id=1的 name 为老王02；\n4、会话 B 又将id=1的 name 更新为老王03；\n5、会话 A 再查询id=1的 name 时，结果为老王03。\n这种现象就是不可重复读。\n幻读 一个事务先根据某些条件查出一些记录，之后另一个事务又想表中插入了符合这些条件的记录，原先的事务再次按照该条件查询时，能够把另一个事务插入的数据也查出来。 （幻读在读未提交、读已提交、可重复读隔离级别中都可能会出现）\n1、会话 A 开始事务，查询id\u0026gt;0的数据，结果只有 name=老王 01 的一条数据\n2、会话 B 像数据表中插入了一条name=老王02的数据（隐式事务，执行 sql 后自动 commit）\n3、会话 A 的事务再次查询 id\u0026gt;0的数据\n不同隔离级别下出现事务并发问题的可能 隔离级别 脏读 不可重复读 幻读 读未提交 可能 可能 可能 读已提交 不可能 可能 可能 可重复读 不可能 不可能 可能 串行化 不可能 不可能 不可能 四、事务的实现原理 首先了解一下 redo log 和 undo log\n1、redo log(重做日志) MySQL 为了提升性能不会把每次的修改都实时同步到磁盘，而是会优先存储到 Buffer Pool（缓冲池）里面，把这个当做缓存来用，然后使用后台线程去做缓冲池和磁盘之间的同步\n如果还没来得及同步数据就出现宕机或者断电，就会导致丢失部分已提交事务的修改信息，\n所以引入了redo log来记录已成功提交事务的修改信息，并且把 redo log 持久化到磁盘，系统重启之后读取 redo log 恢复最新数据\nredo log 是用来恢复数据的，用于保障已提交事务的持久化特性。\n2、undo log undo log 叫做回滚日志，用于记录数据被修改前的信息，与 redo log 记录的数据相反，redo log 是记录修改后的数据，undo log 记录的是数据的逻辑变化，为了发生错误时回滚之前的操作，需要将之前的操作都记录下来，然后在发生错误时才可以回滚\n每次写入数据或者修改数据之前都会把修改前的信息记录到 undo log\n3、事务特性的具体实现原理 事务的原子性通过 undo log 来实现的 事务的持久性是通过 redo log 实现的 事务的隔离性是通过 读写锁 + MVCC 实现的 事务的一致性是通过 **原子性、持久性、隔离性**来实现的 3.1、原子性的实现 每条数据变更（insert/update/delete）操作都会记录一条undo log，并且undo log必须先于数据持久化到磁盘上。\n所谓的回滚就是根据undo log做逆向操作，比如delete的逆向操作是insert，insert的逆向操作是delete，update的逆向操作是update等。\n为了做到同时成功或者同时失败，当系统发生错误或者执行rollback时需根据undo log进行回滚\n3.2、持久性的实现 Mysql 的数据存储机制是将数据最终持久化到磁盘上，并且频繁的进行磁盘 IO 是非常消耗性能的，为了提升性能，InnoDB 提供了缓冲池（Buffer Pool），缓冲池中包含了磁盘数据也的映射，可以当做缓存来使用\n读数据：会首先从缓冲池中读取，若没有，则从磁盘读取并放入缓冲池中\n写数据：会首先写入缓冲池中，缓冲池中的数据会定期同步到磁盘中\n那么问题来了，如果在缓冲池的数据还没有同步到磁盘上时，出现了机器宕机或者断电，可能会出现数据丢失的问题，因此我们需要记录已提交事务的数据，于是，redo log登场了， redo log 在执行数据变更（insert/update/delete）操作的时候，会变更后的结果记录在缓冲区，待commit事务之后同步到磁盘\n至于redo log也要进行磁盘 IO，为什么还要用\n(1)、redo log是顺序存储，而缓存同步是随机操作\n(2)、缓存同步是以数据页为单位，每次传输的数据大小小于redo log\n3.3、隔离性的实现 读未提交： 读写并行，读的操作不能排斥写的操作，因此会出现脏读,不可重复读,幻读的问题\n读已提交： 使用排他锁X，更新数据需要获取排他锁，已经获取排他锁的数据，不可以再获取共享锁S以及排他锁X，读取数据使用了MVCC（Mutil-Version Concurrency Control）多版本并发控制机制（后续单独展开）以及Read view的概念，每次读取都会产生新的Read view，因此可以解决脏读问题，但解决不了不可重复读和幻读的问题\n可重复读： 同上也是利用MVCC机制实现，但是只在第一次查询的时候创建Read view，后续的查询还是沿用之前的Read view，因此可以解决不可重复读的问题，具体不在这展开，但还是有可能出现幻读\n串行化 ：读操作的时候加共享锁，其他事务可以并发读，但是不能并发写，执行写操作的时候加排他锁，其他事务既不能并发写，也不能并发读，串行化可以解决事务并发中出现的脏读、不可重复读、幻读问题，但是并发性能却因为加锁的开销变得很差\n3.4、一致性的实现 一致性的实现其实是通过原子性、持久性，隔离性共同完成的\n五、结束语 了解 MySQL 的事务机制，以及实现原理，对于使用或者优化都有很大的帮助，要保持知其然和知其所以然的心态和持续学习的劲头，了解更多关于 Mysql 相关的知识！\n","date":"2021-03-24T11:18:58+08:00","permalink":"https://x-xkang.com/p/mysql--%E4%BA%8B%E5%8A%A1%E6%B5%85%E6%9E%90/","title":"MySQL -- 事务浅析"},{"content":"一、问题描述 1、生产环境主站每隔一段时间就会出现卡顿，接口响应慢，甚至超时的情况、\n2、测试环境重现不了（一抹诡异的光）\n二、问题排查 针对响应慢的接口进行优化，之前的代码风格也存在问题，还是有些滥用sync/await，一些没有依赖关系的操作，全部分开每行await同步执行，分析后把部分DB操作合并一个Promise执行 阿里云查了一下mysql的slow_log，有挺多的慢查询，优化了一部分SQL，业务逻辑太复杂，但是！没有解决问题，主站还是隔一段时间就卡 找到部分接口日志，超时的接口返回的是Knex.js数据库管理工具抛出的异常，KnexTimeoutError: Knex: Timeout acquiring a connection. The pool is probably full.，可能是连接池已满，获取连接失败导致的 看了一下数据库连接池的配置，最大连接数是30，获取连接的超时时间是60s，可能是并发量大加上部分操作未释放连接导致后续的操作无法正常获取数据库连接池的连接，大概又定位了一下可能的问题点\n1，主站的信息列表会隔几秒钟轮询，获取最新的数据，如果1000个用户在线，轮询周期内就会有1000个查询，中间也没有做缓存处理，导致并发到DB的请求会比较多 2，为了维护DB的状态统一，用户的部分操作用了事务，一些事务内包含了太多操作（感觉是长期占用连接未释放的罪魁祸首） 3，测试环境重现不了是因为没有经过压测，只测试了功能，没有测试性能，日常测试也没有大并发 三、问题验证 将数据库连接池的数量改成了1，使用事务的接口中做了延时的transaction.commit()操作，然后另外一个请求再去正常查询，\n结果显示，如果一个用户调用了事务操作的接口，然后再调用查询接口，查询会一直阻塞在获取连接的步骤，直至事务commit之后释放连接，如果在配置的timeout时间之前没有获取到，Knex就会抛出Timeout acquiring a connection. The pool is probably full的异常，\n也侧面印证了为什么测试环境复现不了这种情况，毕竟在没有压测的前提下。两个测试通过手动操作，并发量是达不到配置的数量的，也就不会出现卡顿的情况\n四、解决方案 1，优化长事务的操作，减少不必要的事务，提高处理效率（难度较大，业务逻辑比较复杂）；\n2，合理范围内增加数据库连接池的最大连接数配置，线上的mysql可连接300个，后端3个服务，现在配置是10、30、30，先把主站改50看看，连接数太大也会导致磁盘I/O效率大幅降低又会导致其他问题；\n3，轮询获取列表的操作，可以改成服务端主动去推（使用socket.io），然后加一个中间缓存（redis），毕竟列表数据变化的频率不是很高；\n4，数据库扩展成读写分离，update和insert的操作直接操作主库，大部分select操作转移到从库，即使有部分的事务操作慢，也不会导致主站的基本查询卡住\n五、写在最后 系统的业务逻辑比较复杂，从业务代码层面下手成本还是比较高，接口的耦合都比较高，重构都比改的成本低，开始的设计，可能也没有考虑扩展的问题，并发的问题等， 包括每个服务之间的通信问题，后期再慢慢优化吧，不怕有问题，就怕一直没遇到过问题！加油~。\n","date":"2021-03-12T17:42:20+08:00","permalink":"https://x-xkang.com/p/mysql--%E8%BF%9E%E6%8E%A5%E6%B1%A0%E9%97%AE%E9%A2%98/","title":"Mysql -- 连接池问题"},{"content":" 资源转自[B站]，如侵删\n","date":"2021-02-04T13:35:56+08:00","permalink":"https://x-xkang.com/p/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%89/","title":"计算机网络学习笔记（三）"},{"content":" 资源转自[B站]，如侵删\n","date":"2021-02-04T09:55:34+08:00","permalink":"https://x-xkang.com/p/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%BA%8C/","title":"计算机网络学习笔记（二）"},{"content":" 资源转自[B站]，如侵删\n","date":"2021-01-22T16:13:30+08:00","permalink":"https://x-xkang.com/p/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%80/","title":"计算机网络学习笔记（一）"},{"content":"一、物理层基本概念 物理层解决如何在连接各种计算机的传输媒体上传输数据比特流，而不是指具体的传输媒体，物理层的主要任务是确定与传输媒体接口有关的一些特性\n1、机械特性 定义物理连接的特性，规定物理连接时所采用的规格，接口形状，引线数目，引脚数量和排列情况\n2、电气特性 规定传输二进制位时，线路上信号的电压范围，阻抗匹配，传输速率和距离限制\n3、功能特性 知名某条线上出现的某一电平标识何种意义，接口不见的信号线的用途\n4、规程特性 (过程特性)定义各条物理线路的工作规程和时序关系\n二、数据通信基础知识 1、典型的数据通信模型 2、数据通信相关术语 通信的目的是传送消息\n数据：传送消息的实体，通常是有意义的符号序列\n信号：数据的电气/电磁表现，是数据在传输过程中的存在形式\n数字信号：代表消息的参数取值是离散的 模拟信号：代表消息的参数是连续的 信源：产生和发送数据的源头\n信宿：接受数据的终点\n信道：信号的传输媒介，一般用来表示向某一个方向传送信息的介质，因此一条通信线路往往包含一条发送信道和一条接受信道\n3、三种通信方式 从通信双方信息的交互方式来看，可以有三种基本方式：\n单工通信：只有一个方向的通信而没有反方向的交互，仅需一条信道\n半双工信道：通信的双方都可以发送或接受信息，但任何一方都不能同时发送和接受，需要两条信道\n全双工信道：通信双方可以同时发送和接受信息，也需要两条信道\n4、两种数据传输方式 串行传输：速度慢、费用低、适合远距离\n并行传输：速度快、费用高、适合近距离\n三、码元、波特、速率、带宽 1、码元 码元： 是指用给一个固定时长的信号波形（信号波形），代表不同离散数值的基本波形，是数字通信中数字信号的计量单位，这个时长内的信号称为k进制码元，而该时长称为码元宽度，当码元的离散状态有M个时（M大于2）此时码元为M进制码元\n1码元可以携带多个比特的信息量， 例如： 在使用二进制编码时，只有两种不同的码元，一种代表0状态，另一种代表1状态\n2、速率、波特、带宽 速率： 也叫 数据率 是指数据的传输速率，表示单位时间内传输的数据量，可以用码元传输速率和信息传输速率表示\n1）码元传输速率： 别名码元速率、波形速率、调制速率、符号速率等，它标识单位时间内数字通讯系统所传输的码元个数（也可称为脉冲个数或信号变化的次数），单位是 波特(Baud)。1波特表示数字通讯系统每秒传输一个码元，这里的码元可以是多进制的，但码元速率与进制无关。\n2）信息传输速率： 别名信息速率、比特率等，表示单位时间内数字通讯系统传输的二进制码元个数（即比特数），单位是 比特/秒（b/s）\n关系： 若一个码元携带 n bit的信息量，则M Baud的码元传输速率所对应的信息传输速率为 M x n bit/s\n带宽： 表示在单位时间内从网络中的某一点到另一点所能通过的 “最高数据率”，常用来表示网络的通信线路所能传输数据的能力，单位是b/s。\n四、奈氏准则和香农定理 1、失真 2、码间串扰 3、奈氏准则 4、香农定理 五、编码与调制 1、基带信号和宽带信号 2、编码与调制 1）非归零编码\n2）归零编码\n3）反向不归零编码\n4）曼彻斯特编码\n5）差分曼彻斯特编码\n6）4B/5B编码\n7）数字数据调制为模拟信号\n8）模拟数据编码为数字信号\n","date":"2020-01-25T17:21:56+08:00","permalink":"https://x-xkang.com/p/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C--%E7%89%A9%E7%90%86%E5%B1%82/","title":"计算机网络--物理层"},{"content":"一、模式切换 i 切换到输入模式，以输入字符。\nx 删除当前光标所在处的字符。\n: 切换到底线命令模式，以在最底一行输入命令。\n二、输入模式 在命令模式下按下i就进入了输入模式。 在输入模式中，可以使用以下按键：\n字符按键以及Shift组合，输入字符\nENTER，回车键，换行\nBACK SPACE，退格键，删除光标前一个字符\nDEL，删除键，删除光标后一个字符\n↑/↓/←/→ 方向键，在文本中移动光标\nHOME/END，移动光标到行首/行尾\nPage Up/Page Down，上/下翻页\nInsert，切换光标为输入/替换模式，光标将变成竖线/下划线\nESC，退出输入模式，切换到命令模式\n三、命令模式 1、移动光标 命令 作用 h 或 向左箭头键(←) 光标向左移动一个字符 j 或 向下箭头键(↓) 光标向下移动一个字符 k 或 向上箭头键(↑) 光标向上移动一个字符 l 或 向右箭头键(→) 光标向右移动一个字符 [Ctrl] + [f] 屏幕『向下』移动一页，相当于 [Page Down]按键 (常用) [Ctrl] + [b] 屏幕『向上』移动一页，相当于 [Page Up] 按键 (常用) [Ctrl] + [d] 屏幕『向下』移动半页 [Ctrl] + [u] 屏幕『向上』移动半页 + 光标移动到非空格符的下一行 - 光标移动到非空格符的上一行 n\u0026lt;space\u0026gt; 那个 n 表示『数字』，例如 20 。按下数字后再按空格键，光标会向右移动这一行的 n 个字符。例如 20\u0026lt;space\u0026gt; 则光标会向后面移动 20 个字符距离。 0 或功能键[Home] 这是数字『 0 』：移动到这一行的最前面字符处 (常用) $ 或功能键[End] 移动到这一行的最后面字符处(常用) H 光标移动到这个屏幕的最上方那一行的第一个字符 M 光标移动到这个屏幕的中央那一行的第一个字符 L 光标移动到这个屏幕的最下方那一行的第一个字符 G 移动到这个档案的最后一行(常用) nG n 为数字。移动到这个档案的第 n 行。例如 20G 则会移动到这个档案的第 20 行(可配合 :set nu) gg 移动到这个档案的第一行，相当于 1G 啊！ (常用) n\u0026lt;Enter\u0026gt; n 为数字。光标向下移动 n 行(常用) 2、搜索替换 命令 作用 /word 向光标之下寻找一个名称为 word 的字符串。例如要在档案内搜寻 vbird 这个字符串，就输入 /vbird 即可！ (常用) ?word 向光标之上寻找一个字符串名称为 word 的字符串。 n 这个 n 是英文按键。代表重复前一个搜寻的动作。举例来说， 如果刚刚我们执行 /vbird 去向下搜寻 vbird 这个字符串，则按下 n 后，会向下继续搜寻下一个名称为 vbird 的字符串。如果是执行 ?vbird 的话，那么按下 n 则会向上继续搜寻名称为 vbird 的字符串！ N 这个 N 是英文按键。与 n 刚好相反，为『反向』进行前一个搜寻动作。 例如 /vbird 后，按下 N 则表示『向上』搜寻 vbird 。 :n1,n2s/word1/word2/g n1 与 n2 为数字。在第 n1 与 n2 行之间寻找 word1 这个字符串，并将该字符串取代为 word2 ！举例来说，在 100 到 200 行之间搜寻 vbird 并取代为 VBIRD 则：『:100,200s/vbird/VBIRD/g』。(常用) :1,$s/word1/word2/g或 :%s/word1/word2/g 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！(常用) :1,$s/word1/word2/gc 或 :%s/word1/word2/gc 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！且在取代前显示提示字符给用户确认 (confirm) 是否需要取代！(常用) 【注】：使用 /word 配合 n 及 N 是非常有帮助的！可以让你重复的找到一些你搜寻的关键词！\n3、删除、复制与粘贴 命令 作用 x, X 在一行字当中，x 为向后删除一个字符 (相当于 [del] 按键)， X 为向前删除一个字符(相当于 [backspace] 亦即是退格键) (常用) nx n 为数字，连续向后删除 n 个字符。举例来说，我要连续删除 10 个字符， 『10x』。 dd 删除游标所在的那一整行(常用) ndd n 为数字。删除光标所在的向下 n 行，例如 20dd 则是删除 20 行 (常用) d1G 删除光标所在到第一行的所有数据 dG 删除光标所在到最后一行的所有数据 d$ 删除游标所在处，到该行的最后一个字符 d0 那个是数字的 0 ，删除游标所在处，到该行的最前面一个字符 yy 复制游标所在的那一行(常用) nyy n 为数字。复制光标所在的向下 n 行，例如 20yy 则是复制 20 行(常用) y1G 复制游标所在行到第一行的所有数据 yG 复制游标所在行到最后一行的所有数据 y0 复制光标所在的那个字符到该行行首的所有数据 y$ 复制光标所在的那个字符到该行行尾的所有数据 p, P p 为将已复制的数据在光标下一行贴上，P 则为贴在游标上一行！ 举例来说，我目前光标在第 20 行，且已经复制了 10 行数据。则按下 p 后， 那 10 行数据会贴在原本的 20 行之后，亦即由 21 行开始贴。但如果是按下 P 呢？ 那么原本的第 20 行会被推到变成 30 行。 (常用) J 将光标所在行与下一行的数据结合成同一行 c 重复删除多个数据，例如向下删除 10 行，[ 10cj ] u 复原前一个动作。(常用) [Ctrl]+r 重做上一个动作。(常用) . 不要怀疑！这就是小数点！意思是重复前一个动作的意思。 如果你想要重复删除、重复贴上等等动作，按下小数点『.』就好了！ (常用) 【注】：这个 u 与 [Ctrl]+r 是很常用的指令！一个是复原，另一个则是重做一次～ 利用这两个功能按键，你的编辑，嘿嘿！很快乐的啦！\n4、进入输入或取代的编辑模式 命令 作用 i, I 进入输入模式(Insert mode)： i 为『从目前光标所在处输入』， I 为『在目前所在行的第一个非空格符处开始输入』。 (常用) a, A 进入输入模式(Insert mode)： a 为『从目前光标所在的下一个字符处开始输入』， A 为『从光标所在行的最后一个字符处开始输入』。(常用) o, O 进入输入模式(Insert mode)： 这是英文字母 o 的大小写。o 为『在目前光标所在的下一行处输入新的一行』； O 为在目前光标所在处的上一行输入新的一行！(常用) r, R 进入取代模式(Replace mode)： r 只会取代光标所在的那一个字符一次； R 会一直取代光标所在的文字，直到按下 ESC 为止；(常用) [Esc] 退出编辑模式，回到一般模式中(常用) 【注】：上面这些按键中，在 vi 画面的左下角处会出现『\u0026ndash;INSERT\u0026ndash;』或『\u0026ndash;REPLACE\u0026ndash;』的字样。 由名称就知道该动作了吧！！特别注意的是，我们上面也提过了，你想要在档案里面输入字符时， 一定要在左下角处看到 INSERT 或 REPLACE 才能输入喔！\n5、指令行的储存、离开等指令 命令 作用 :w 将编辑的数据写入硬盘档案中(常用) :w! 若文件属性为『只读』时，强制写入该档案。不过，到底能不能写入， 还是跟你对该档案的档案权限有关啊！ :q 离开 vi (常用) :q! 若曾修改过档案，又不想储存，使用 ! 为强制离开不储存档案。注意一下啊，那个惊叹号 (!) 在 vi 当中，常常具有『强制』的意思～ :wq 储存后离开，若为 :wq! 则为强制储存后离开 (常用) ZZ 这是大写的 Z 喔！若档案没有更动，则不储存离开，若档案已经被更动过，则储存后离开！ :w [filename] 将编辑的数据储存成另一个档案（类似另存新档） :r [filename] 在编辑的数据中，读入另一个档案的数据。亦即将 『filename』 这个档案内容加到游标所在行后面 :! command 暂时离开 vi 到指令行模式下执行 command 的显示结果！例如『:! ls /home』即可在 vi 当中察看 /home 底下以 ls 输出的档案信息！ 6、vim 环境的变更 命令 作用 :set nu 显示行号，设定之后，会在每一行的前缀显示该行的行号 :set nonu 与 set nu 相反，为取消行号！ ","date":"2020-01-06T09:42:51+08:00","permalink":"https://x-xkang.com/p/vim%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","title":"Vim常用命令"},{"content":" 构建镜像 在项目根目录下新建 Dockerfile 文件并编辑保存\n1 2 3 4 5 6 7 8 FROM golang:latest # 依赖的镜像:镜像版本 ADD . /var/www/go-aimaster # 将当前工作目录copy到镜像的/var/www/go-aimaster 目录下 WORKDIR /var/www/go-aimaster # 设置镜像内的工作目录 RUN GOPROXY=\u0026#34;https://goproxy.cn,direct\u0026#34; go build -o main /var/www/go-aimaster/main.go # 运行命令(当前为golang 项目demo) CMD [\u0026#34;/var/www/go-aimaster/main\u0026#34;] # 可执行文件目录，上一步build生成的main可执行文件 EXPOSE 8080 # 暴露端口，最终暴露的端口不一定是当前的8080 端口 ENTRYPOINT [\u0026#34;./main\u0026#34;] # 入口文件 执行命令：\n1 docker image build -t 镜像名称[:版本号] . # (注意最后有个点 .) 指定Dockerfile:\n1 docker image build -t 镜像名称[:版本号] -f Dcokerfile . 上面代码中，-t 参数用来指定 image 文件的名字，后面还可以用冒号指定标签。如果不指定，默认的标签就是latest。最后的那个点表示 Dockerfile 文件所在的路径，上例是当前路径，所以是一个点。 执行结果如下：\n出现上图的Successfully成功标识表示已构建成功，执行 docker images 查看，列表中出现刚刚构建的go-aimaster镜像\n下载远端镜像 命令：docker pull 仓库名称\n1 docker pull nginx 推送本地镜像至远端仓库 命令： 1 2 3 docker image tag go-docker:v1.0 devxiaokang/go-docker:v1.0 docker push devxiaokang/go-docker:v1.0 查看镜像列表 命令：docker image ls | docker images\n1 docker images 删除本地镜像 命令：docker rmi 镜像标识|镜像名称:版本号\n【注意】若有容器正在依赖该镜像，则无法删除\n1 docker rmi go-aimamster:v0.01 生成容器 命令： docker [container] run 镜像标识 /bin/bash（简单操作）| docker [container] run -d -p 宿主机端口:容器端口 -it --name 容器名称 镜像标识 /bin/bash （常用操作）\n1 docker run nginx 或者：\n1 docker run -d -p 8080:80 -it --name nginx nginx:latest /bin/bash 以上代码中-d 代表后台运行， -p 代表宿主机端口与容器端口的映射关系，-it 代表容器的 shell 映射到当前的 shell，然后再本机窗口输入命令，就会传入容器中，--name nginx 代表定义容器名称nginx 为自定义名称，。\n执行结果如下： 查看容器列表 命令：docker ps -a[q]， -a 表示显示所有容器（包括已停止的），-q 列表值显示容器的唯一标识\n1 docker ps -a 1 docker ps -aq 进入容器 命令：docker exec -it 容器ID|容器名称 /bin/bash\n1 docker exec -it nginx /bin/bash 启动容器 命令：docker start 容器ID|容器名称\n1 docker start nginx 重启容器 命令：docker restart 容器ID|容器名称\n1 docker restart nginx 停止容器 命令：docker stop 容器ID|容器名称\n1 docker stop nginx 停止全部容器\n1 docker stop $(docker ps -qa) 删除容器 命令：docker rm 容器ID|容器名称\n删除指定容器\n1 docker rm nginx 删除全部容器\n1 docker rm $(docker ps -qa) 数据卷 数据卷：将宿主机的一个目录映射到容器的一个目录中，可以在宿主机中操作目录中的内容，那么容器内部映射的文件，也会跟着一起改变,创建数据卷之后，默认会存在一个目录下 /var/lib/docker/volumes/数据卷名称/_data\n创建数据卷\n1 docker volume create volume_name 查看数据卷\n1 docker volume inspect volume_name 查看全部数据卷\n1 docker volume ls 删除数据卷：docker volume rm 数据卷名称\n1 docker volume rm volume_name 管理多容器 .yml文件以key: value 方式来指定配置信息，多个配置信息以换行+锁紧的方式来区分，在docker-compose文件中，不要使用制表符\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # yml version: ‘3.1’ service: mysql: restart: always # 代表只要docker启动，这个容器就会跟着启动 image: daoclound.io/lib/mysql:5.7.4 # 镜像路径 container_name: mysql # 指定容器名称 ports: - 3306:3306 environment: MYSQL_ROOT_PASSWORD: 123456 TZ: Asia/Shanghai # 时区 volumes: # 数据卷 - /opt/docker_mysql/data:/var/lib/mysql tomcat: restart: always Image: daocloud.io/library/tomcat:8.5.15-jre8 # 镜像 container_name: tomcat ports: - 8080:8080 environment: TZ: Asia/Shanghai volumes: - /opt/docker_mysql_tomcat/tomcat_webapps:/usr/local/tomcat/webapps - /opt/docker_mysql_tomcat/tomcat_logs:/usr/local/tomcat/logs Docker-Compose 配置Dockerfile使用 使用docker-compose.yml文件以及Dockerfile文件在生成自定义镜像的同时启动当前镜像，并且由docker-compose去管理容器\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # yml version: ‘3.1’ services: mysql: restart: always build: context: ../ # 指定dockerfile文件所在路径 dockerfile: Dockerfile #指定dockerfile文件名称 container_name: mysql ports: - 3306:3306 environment: TZ: Asia/Shanghai 可以直接启动基于 docker-compose.yml以及Dockerfile文件构建的自定义镜像 docker-compose up -d 如果自定义镜像不存在，会帮助我们构建出自定义镜像，如果自定义镜像已存在，会直接运行这个自定义镜像，重新构建的话需执行 docker-compose build , 运行前重新构建 docker-compose up -d —build\nDocker-compose 命令：\n后台启动： docker-compose up -d 关闭并删除容器： docker-compose down 开启|关闭|重启已经存在的有docker-compose维护的容器： docker-compose start | stop | restart 查看docker-compose管理的容器： docker-compose ps 查看日志： docker-compose logs -f ","date":"2019-09-18T02:01:58+05:30","permalink":"https://x-xkang.com/p/docker-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","title":"Docker 常用命令"}]