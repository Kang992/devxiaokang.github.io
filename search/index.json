[{"content":"一、全局锁  对整个数据库实例加锁，加锁后整个数据库实例处于只读状态，后续的DML的写语句，DDL语句已经更新操作的事务提交语句都将阻塞。 其典型的使用场景是做全库的逻辑备份，对所有的表进行锁定，从而获取一致性的视图，保证数据的完整性。  加锁操作：\n1 2 3 4 5 6 7 8 9 10 11  -- 加锁 mysql\u0026gt; flush tables with read lock; -- TODO. 逻辑备份 mysqldump -uroot -p123456 [dbname] \u0026gt; dbname.sql -- TODO. 中间穿插的 insert/update/delete 语句将阻塞 mysql\u0026gt; update user set `password`=\u0026#39;654321\u0026#39; where `id` = 1; -- 释放锁 mysql\u0026gt; unlock tables;   特点：\n数据库中加全局锁是比较重的操作，存在一下问题\n 如果在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆。 如果在从库上备份，那么在备份期间从库不能执行从主库同步过来的二进制文件(binlog)，会导致主从延迟。  在Innodb存储引擎中，我们可以再备份时加上参数 \u0026ndash;single-transaction 参数来完成不加锁的一致性数据备份\n1  mysqldump --single-transaction -uroot -p123456 [dbname] \u0026gt; dbname.sql    二、表级锁 介绍 每次操作锁住整张表，锁定力度大，发生锁冲突的概率最高，并发度最低。应用在Innodb,MyIsam，BDB等存储引擎中。\n表级锁主要分为以下几类：\n1. 表锁   表共享读锁 表独占写锁   操作如下：\n1 2 3 4 5  -- 加锁 mysql\u0026gt; lock tables [table_name] read/write; -- 释放锁 mysql\u0026gt; unlock tables;     读锁不会阻塞其他客户端的读操作，但是会阻塞写操作。 写锁既会阻塞其他客户端的读，也会阻塞其他客户端的写。    2. 元数据锁(meta data lock, mdl)   MDL加锁过程是系统自动控制的，无需显式使用，在访问一张表的时候会自动加上，主要作用是维护表元数据的数据一致性，在表上有活动事务的时候，不可以对元数据进行写操作，为了避免MDL和DDL的冲突，保证读写的正确性 在MySQL5.5中加入了MDL，当对一张表进行增删改查的时候，加MDL读锁（共享锁），当对表结构进行修改的时候，加MDL写锁。   查看元数据锁\n1  mysql\u0026gt; select object_type, object_schema, object_name, lock_type, lock_duration from performance_schema.metadata_locks;      对应SQL 锁类型 说明     lock tables [t_name] read/write SHARED_READ_ONLY / SHARED_NO_READ_WRITE    select、select \u0026hellip; lock in share mode SHARED_READ 与SHARED_READ/SHARED_WRITE兼容，与EXCLUSIVE 互斥   insert、update、delete、select \u0026hellip; for update SHARED_WRITE 与SHARE_READ/ SHARED_WRITE兼容，与EXCLUSIVE 互斥   alter table \u0026hellip; EXCLUSIVE 与其他的 MDL 都互斥     3. 意向锁 为了避免MDL在执行时，加的表锁与行锁冲突，在InnoDB中引入了意向锁，使得表锁不用检查每行数据是否加锁，使用意向锁来减少表锁的检查。\n  意向共享锁（IS）：由语句 select \u0026hellip; lock in share mode 添加，与表锁共享锁（read）兼容，与表锁排它锁（write）互斥。 意向排它锁（IX）：由语句 insert、update、delete、select \u0026hellip; for update 添加，与表锁共享锁（read）和表锁互斥锁（write）都互斥。意向锁之间不会互斥。   查看意向锁：\n1  mysql\u0026gt; select object_schema, object_name, index_name, lock_type, lock_mode, lock_data from performance_schema.data_locks;   三、行级锁 每次操作锁住对应的数据行。锁定粒度最小，发生锁冲突的概率最低，并发度最高。应用在Innodb存储引擎中。\nInnodb的数据是鲫鱼索引组织的，行锁是通过对索引上的索引项加锁来实现的，而不是对记录加的锁，对于行级锁，主要分为以下三类：\n1. 行锁 Record Lock：锁定单个记录的锁，防止其他事务对此进行update和delete，在 RC 和 RR 隔离级别下都支持。\n Innodb实现了以下两种类型的行锁：    共享锁（S）：允许一个事务去读一行，阻止其他事务获得相同数据集的排它锁。 排它锁（X）：允许获取排它锁的事务更新数据，阻止其他事务获得相同数据集的共享锁和排它锁。      当前锁类型 \\ 请求锁类型 S（共享锁） X（排它锁）     S （共享锁） 兼容 冲突   X （排它锁） 冲突 冲突     不同的SQL使用的锁类型：     SQL 行锁类型 说明     INSERT \u0026hellip; 排它锁 自动加锁   UPDATE \u0026hellip; 排它锁 自动加锁   DELETE \u0026hellip; 排它锁 自动加锁   SELECT（正常） 不加任何锁    SELECT \u0026hellip; LOCK IN SHARE MODE 共享锁 需要手动在 SELECT 之后加 LOCK IN SHARE MODE   SELECT \u0026hellip; FOR UPDATE 排它锁 需要手动在 SELECT 之后加 FOR UPDATE     行锁-演示：  默认情况下，InnoDB在REPEATABLE READ 事务隔离界别运行，InnoDB 使用next key 锁进行搜索和索引扫描，以防止幻读。\n  针对唯一索引进行检索时，对已存在的记录进行等值匹配时，将会自动优化为行锁。 InnoDB的行锁是针对于索引加的锁不通过索引条件检索数据，那么InnoDB将对表中的所有数据进行加锁，此时就会升级为表锁   可以通过一下SQL，查看意向锁以及行锁的加锁情况：\n1  mysql\u0026gt; select object_schema, object_name, index_name, lock_type, lock_mode, lock_data from performance_schema.data_locks;    2. 间隙锁 Gap Lock: 锁定索引记录间隙（不包含该记录），确保索引记录间隙不变，防止其他事务在这个间隙进行insert，产生幻读，在RR隔离级别下都支持。\n 3. 临键锁 Next-Key Lock: 行锁和间隙锁组合，同时锁住数据，并锁住数据前面的间隙Gap。在RR隔离级别下支持。\n4. 间隙锁/临键锁 演示 默认情况下，InnoDB在Repeatable read 事务隔离级别运行，InnoDB使用 Next-key 锁进行搜索和索引扫描，以防止幻读。\n  索引上的等值查询（唯一索引），给不存在的记录加锁时优化为间隙锁。 索引上的等值查询（普通索引），向右遍历时最后一个值不满足查询需求时，next-key lock 退化为间隙锁。 索引上的范围查询（唯一索引），会访问到不满足条件的第一个值为止。   注意：间隙锁的唯一目的是防止其他事务插入间隙，间隙锁可以共存，一个事务采用的间隙锁不会阻止另一个事务在同一间隙上使用间隙锁 ","date":"2022-12-16T17:54:33+08:00","permalink":"https://x-xkang.com/p/mysql-%E9%94%81%E5%88%86%E6%9E%90/","title":"Mysql 锁分析"},{"content":"一、环境准备  OS: win10 docker engine: v20.10.14  二、创建MySQL服务容器 1.创建 master 节点服务  编辑数据库配置文件，将配置文件以及数据存储目录挂载到宿主机上  master节点配置文件mysql-master.cnf如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  [mysqld] # 定义服务id server-id=1 #启用二进制日志 log-bin=mysql-bin # 设置不要复制的数据库(可设置多个) # binlog-ignore-db=information_schema # 设置需要复制的数据库 需要复制的主数据库名字 binlog-do-db=testdb #设置logbin格式 binlog_format=STATEMENT # 开启gtid enforce-gtid-consistency=on gtid-mode=on    创建master节点服务的容器mysql-master  1  docker run -d -p 3310:3306 -v D:\\workspace\\docker-volumes\\mysql\\master\\mysql-master.cnf:/etc/mysql/conf.d/mysql-master.cnf -v D:\\workspace\\docker-volumes\\mysql\\master\\data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 --name mysql-master mysql:5.7   2. 创建 slave 节点服务  创建配置文件 mysql-slave-1.cnf 如下：  1 2 3 4 5 6 7 8  [mysqld] # 设置 server-id 注意不要与 master 节点重复 server-id=2 binlog-format=STATEMENT relay-log=mysql-relay gtid-mode=ON enforce-gtid-consistency=true read-only=1    创建 slave 节点服务  1  docker run -d -p 3311:3306 -v D:\\workspace\\docker-volumes\\mysql\\slave-1\\mysql-slave-1.cnf:/etc/mysql/conf.d/mysql-slave-1.cnf -v D:\\workspace\\docker-volumes\\mysql\\slave-1\\data\\:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 --name mysql-slave-1 mysql:5.7   3. 创建同步数据使用的用户  进入master节点  1  docker exec -it mysql-master mysql -uroot -p123456    创建用户，记住此时创建的用户密码  1 2  CREATE USER \u0026#39;slave\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;123456\u0026#39;; GRANT REPLICATION SLAVE ON *.* TO \u0026#39;slave\u0026#39;@\u0026#39;%\u0026#39;;    查看 master 节点状态，在master 节点执行  1  mysql\u0026gt; show master status;      File Position Binlog_Do_DB Binlog_Ignore_DB Executed_Gtid_Set     mysql-bin.000002 157 testdb mysql,information_schema     记住File和Position，后面关联主从节点的时候会用\n 查看 master 节点的IP地址，关联主从节点的时候要用  1  docker inspect mysql-master    进入 slave 节点  1  docker exec -it mysql-slave-1 mysql -uroot -p123456   执行SQL关联主从复制节点\n1 2 3 4 5 6 7  mysql\u0026gt; CHANGE MASTER TO MASTER_HOST=\u0026#39;172.17.0.2\u0026#39;, # master 节点IP MASTER_PORT=3306, # master 节点端口 MASTER_USER=\u0026#39;slave\u0026#39;, # 上面创建的主从同步用户 MASTER_PASSWORD=\u0026#39;123456\u0026#39;, # 用户密码 MASTER_LOG_FILE=\u0026#39;mysql-bin.000003\u0026#39;, # master 节点的日志文件 MASTER_LOG_POS=0;   开启 slave，\n1 2  mysql\u0026gt; start slave; Query OK, 0 rows affected, 1 warning (0.03 sec)   查看 slave开启状态\n1  mysql\u0026gt; show slave status\\G;   三、创建连接用户并授权  进入 master 节点，执行  1 2 3 4 5  # 创建用户 CREATE USER \u0026#39;test\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;123456\u0026#39;; # 授权 grant select,insert,update,delete on testdb.* to \u0026#39;test\u0026#39;@\u0026#39;%\u0026#39;;   [注]测试过程中不要使用root账号测试从库的read-only权限，因为拥有 super 权限的账号会忽略限制\n 在 master 节点上的testdb 库新建数据表 user  查看 slave 节点数据库，user 已经同步成功，至此mysql 的主从架构集群就已经部署完成，如果想增加 slave节点的话，重复2.2\n[注意]\n 关联主从节点时，确保IP和端口是通的，不然在从节点执行 start slave 时，查看结果 show slave status 连接结果是失败的; 测试账号不要用 root 权限，会忽略从节点的读写限制，从节点也一样可以插入、更新、删除数据; slave 节点服务一定要将 read_only 设置成1，可以在.cnf配置文件中配置，也可以在mysql终端中设置set global read_only=1;不然有写权限的账号在 slave 节点也一样可以插入或更新数据; ","date":"2022-12-02T11:52:10+08:00","permalink":"https://x-xkang.com/p/docker-%E9%83%A8%E7%BD%B2mysql%E9%9B%86%E7%BE%A4-replication/","title":"docker 部署Mysql集群--replication"},{"content":" 分布式  将一个完整的系统，按照业务功能拆分成一些独立的子系统，它们独立运行在不同的服务器上。\n 集群  集群就是单机的多实例，在多个服务器上部署同一个系统，这些服务器的集合叫做集群\n一、单机架构出现的问题  性能问题   单个服务器无论是计算资源还是存储资源都非常有限，当请求量非常大的时候，会出现响应变慢等情况，严重影响用户体验     单体架构的多个服务中，某个服务占用资源多，时间长，导致总体响应显慢   可用性问题   当出现单机故障（进程崩溃、断电、磁盘损坏等）时，整个应用不可用。   开发问题   单体架构下，我们将各个模块放在一个系统中，系统过于庞大臃肿，维护成本高，各个功能模块之间的耦合度偏高，难以针对单个模块进行优化，出现 BUG 较难定位。     交付周期长，所有功能得一起上线，一起构建，一起部署。任何一个环节出错，都可能影响交付。    二、为什么引入分布式  性能提高 相较于单体架构，在请求量非常大的时候，整个系统的响应速度不会有明显的下降，因为每个服务都占有各自独有的服务器资，资源竞争小，任务处理较快 提高可用性 在单体架构中，一旦出现了例如服务器宕机，磁盘损坏等故障问题，会导致整个服务不可用，但分布式架构将各个功能模块拆分独立部署后，即使部分服务器出现故障，也不会影响其他的功能 提高开发效率 1、针对传统的单体架构，必须将整个系统的功能开发完毕才可以上线部署，分布式可以将各个功能模块拆开，独立开发，独立测试，独立部署，同时也降低了系统之间的耦合度，系统与系统之间的边界也会更加清晰 2、服务的复用性更高，系统中某些比较通用的功能拆分出来作为独立的服务部署后，可以作为通用服务调用，不用在每一个系统中开发重复且耗时的功能，比如用户系统，支付系统等  三、为什么引入集群  性能提高 传统的单体架构，在请求量非常大的时候，整个系统的响应速度都会下降，对于集群来说，通过负载均衡将请求分发至集群中的不同节点中，这样可以有效降低每个服务节点的压力。 提高可用性 若集群中的某个节点出现故障时，可将请求转发至其他可用的服务节点上，这样可以保证服务的高可用。  四、分布式与集群的区别   从提升效率的角度比较\n   分布式是以缩短单个任务的执行时间来提升效率     集群是以提高单位时间内执行的任务数来提高效率    从服务部署的角度比较\n   分布式是将一个大的服务拆分成独立的小的服务并部署在不同的服务器上     集群则是将几台服务器集中在一起提供相同的服务     分布式中的每一个节点都可以做集群，反之则不一定可以    五、分布式与集群的关系  根据分布式的特点可以总结出，其主要作用是将系统模块化，将系统进行解耦，有利于开发和维护，但并不能解决大并发的问题，也无法保证服务在运行期间出现宕机等问题后的正常运转， 而集群刚好弥补了这个缺陷，集群，通俗来说就是多个服务器处理相同的业务，这里可以改善一些并发量大的问题，保证了高性能；另一方面，如果集群中的某个节点出现故障导致不可用，但对于整个集群来说，服务还是可用的，即保证了服务的高可用。  六、分布式集群的优点   系统可用性的提升\n   一个系统全年可用时间在99.999%，5个9的服务可用率在设计合理的分布式集群系统中不是一个触不可及的数字     传统的集中式计算或集中式存储在遇见单点故障时很容易造成整个服务的不可用，分布式集群下的服务体系，单台机器有故障，不至于造成整个服务不可用    系统并发能力的提升\n   请求通过负载均衡被分发到不同的服务器上，执行同样服务的服务器可以有1台或者N台，通常情况下可以根据用户访问量增加或减少服务器的数量，可以做到随时水平扩展    系统容错能力的提升\n   同一组服务器分别部署在不同的城市，若其中一个机房发生故障（火灾或者停电），其他城市的服务器还是可用状态，将故障机房的流量转发到正常的城市机房中，可以有效提高服务的可用性    低延时\n   服务器部署在不同城市之后，可以根据用户的IP将请求分发至最近的机房，可以达到降低延时的效果    低耦合\n   （低耦度低）系统之间的耦合度大大降低，可以独立开发、独立部署、独立测试，系统与系统之间的边界非常明确，排错也变得相当容易，开发效率大大提升     （扩展性强）系统更易于扩展。我们可以针对性地扩展某些服务，换句话说，就是对子系统进行集群。例如在双十一时，订单子系统、支付子系统需要集群，账户管理子系统不需要集群。     （复用性高）服务的复用性更高。比如，当我们将用户系统作为单独的服务后，该公司所有的产品都可以使用该系统作为用户系统，无需重复开发    七、分布式集群带来的挑战   依赖网络，会因为网络问题导致系统数据丢失或者不一致 服务器之间的通信依赖网络，不可靠网络包括网络延时，丢包，中断，异步，一个完整的请求依赖于一连串的服务调用，任意一个服务节点出现网络故障都可能造成本次服务调用的失败\n  系统复杂化，系统监控维护，版本迭代发布变的相对复杂，成本高 传统单体架构的服务只需要维护一个站点即可，分布式服务系统被拆分成若干个小服务，服务从1变成几十上百个服务之后，增加运维成本\n  一致性，可用性，分区容错性无法同时满足 这个是最主要的，就是常说的CAP理论，在分布式系统中，这三种特性最多同时满足两种，无法同时满足全部，需要根据实际情况调整策略\n ","date":"2022-11-28T11:02:35+08:00","permalink":"https://x-xkang.com/p/%E5%88%86%E5%B8%83%E5%BC%8F%E5%92%8C%E9%9B%86%E7%BE%A4%E7%9A%84%E5%8C%BA%E5%88%AB/","title":"分布式和集群的区别"},{"content":"一、索引的分类 1、根据数据结构可分为  Btree索引（B+tree, b-tree） 哈希索引 full-text 全文索引  2、根据物理存储可分为  聚簇索引 二级索引(辅助索引)  3、根据字段特性可分为  主键索引 普通索引 前缀索引  4、根据字段个数可分为  单列索引 联合索引（符合索引，组合索引）  二、根据数据结构分类 MySQL按数据结构分类可分为：B+tree索引，Hash索引，Full-text索引\n   - InnoDB MyIsam Memory     B+tree索引 √ √ √   Hash索引 × × √   B-tree索引 √（MySQL5.6+） √ ×      注：InnoDB 实际上也支持Hash索引，但是InnoDB中Hash索引的创建由存储引擎自动优化创建，不能人为干预 是否为表创建Hash索引，\n  B+tree是MySQL中被存储引擎采用最多的索引类型，B+tree中的B代表平衡（balance），而不是二叉（binary），因为B+tree是从最早的平衡二叉树演化而来的，下面演示B+tree数据结构与其他数据结构的对比。\n1. B+tree 和 B-tree的对比 [网络图片] B-tree示意图\n B+tree非叶子结点只存储键值信息，数据记录都存放在叶子节点中，而B-tree的非叶子节点也存储数据，B+tree单个节点的存储量更小，在相同的磁盘IO情况下可以查到更多节点数据。 B+tree 所有叶子结点之间都采用单链表连接，适合MySQL中常见的基于范围的顺序检索场景，而B-tree无法做到这一点。  [网络图片] B+tree示意图\n2. B+tree 和 红黑树的对比 [网络图片] 红黑树示意图\n红黑树是一种弱平衡二叉查找树，通过任何一条从根到叶子的路径上各个节点着色方式的限制，红黑树确保没有一条路径会比其他路径长出两倍\n对于有N个节点的B+tree，其搜索的时间复杂度为O(logdN)，其中**d(Degree)**为B+tree的度，表示节点允许的最大子节点数为d个，在实际应用当中，d值一般是大于100的，即使数据量达到千万级别时B+tree的高度依然维持在3-4左右，保证了3-4次磁盘I/O操作就能查询到目标数据。\n红黑树是二叉树，节点子节点个数为两个，意味着其搜索的算法时间复杂度为 O(logN)，树的高度也会比 B+tree 高出不少，因此红黑树检索到目标数据所需经历的磁盘I/O次数更多。\n3. B+tree 和 Hash的对比 Hash 索引结构的特殊性，其检索效率非常高，索引的检索可以一次定位，不像B-Tree 索引需要从根节点到枝节点，最后才能访问到页节点这样多次的IO访问，所以 Hash 索引的查询效率要远高于 B-Tree 索引。虽然 Hash 索引效率高，但是 Hash 索引本身由于其特殊性也带来了很多限制和弊端，主要有以下这些。\n Hash 索引仅仅能满足 = , IN 和 \u0026lt;=\u0026gt;(表示NULL安全的等价) 查询，不能使用范围查询    由于 Hash 索引比较的是进行 Hash 运算之后的 Hash值，所以它只能用于等值的过滤，不能用于基于范围的过滤，因为经过相应的 Hash算法处理之后的 Hash 值的大小关系，并不能保证和Hash运算前完全一样\n   Hash 索引无法适用数据的排序操作    由于 Hash 索引中存放的是经过 Hash 计算之后的 Hash值，而且Hash值的大小关系并不一定和 Hash运算前的键值完全一样，所以数据库无法利用索引的数据来避免任何排序运算；\n   Hash 索引不能利用部分索引键查询    对于组合索引，Hash 索引在计算 Hash 值的时候是组合索引键合并后再一起计算 Hash 值，而不是单独计算 Hash值，所以通过组合索引的前面一个或几个索引键进行查询的时候，Hash 索引也无法被利用\n   Hash 索引依然需要回表扫描    Hash 索引是将索引键通过 Hash 运算之后，将 Hash运算结果的 Hash值和所对应的行指针信息存放于一个 Hash 表中，由于不同索引键可能存在相同 Hash 值，所以即使取满足某个 Hash 键值的数据的记录条数，也无法从 Hash索引中直接完成查询，还是要通过访问表中的实际数据进行相应的比较，并得到相应的结果。\n   Hash索引遇到大量Hash值相等的情况后性能并不一定就会比B-Tree索引高    选择性比较低的索引键，如果创建 Hash 索引，那么将会存在大量记录指针信息存于同一个Hash值相关联。这样要定位某一条记录时就会非常麻烦，会浪费多次表数据的访问，而造成整体性能低下\n  由于范围查询是MySQL数据库查询中常见的场景，Hash表不适合做范围查询，它更适合做等值查询。另外Hash表还存在Hash函数选择和Hash值冲突等问题。因此，B+tree索引要比Hash表索引有更广的适用场景\n三、根据物理存储分类 MySQL根据叶子节点是否存储的是完整数据将索引分为：聚簇索引，二级索引（辅助索引）。\n1.聚簇索引 聚簇索引的每个叶子节点都存储了一行完整的表数据，叶子节点之间按照id列升序连接，可以方便的进行顺序检索。 [网络图片] MySQL中的InnoDB 存储引擎要求必须有聚簇索引，默认在主键字段上建立聚簇索引，在没有主键的情况下，数据表的第一个非空唯一索引将被建立为聚簇索引， 在前两者都没有的情况下，InnoDB将自动生成一个隐式的自增id列，并在此列上建立聚簇索引。\nMyISAM存储引擎不存在聚簇索引，主键索引和非主键索引的结构是一样的，索引的叶子节点不存储表数据，存放的是表数据的地址，所以MyISAM存储引擎的表可以没有主键。MyISAM表的索引和数据是分开存储的，\n2.二级索引 [网络图片]\n二级索引的叶子节点并不存储一行完整的数据，而是存储了聚簇索引所在的列。\n回表查询：由于二级索引的叶子节点不存储完整的表数据，索引当通过二级索引查询聚簇索引列值后，还需要回到聚簇索引也就是表本身进一步获取数据 [网络图片]\n回表查询需要额外的B+tree搜索过程，必然增加查询消耗, 需要注意的是，通过二级索引查询时，回表不是必须的过程，当select 的所有字段在单个二级索引中都能找到时，就不需要回表，MySQL称此时的二级索引为覆盖索引或者触发了索引覆盖。 可以用explain 命令查看SQL语句的执行计划，执行计划的extra字段中若出现Using index，表示查询触发了索引覆盖。\n四、按字段特性分类 MySQL索引按照字段特性可以分为：主键索引，普通索引，前缀索引\n  主键索引 建立在主键上的索引称为主键索引，一张数据表只能有一个主键索引，索引列值不允许有空值，通常在创建表时一起创建。\n  唯一索引 建立在 Unique字段上的索引称为唯一索引，一张表可以有多个唯一索引，索引列值允许为空，列值中出现多个空值不会出现重复冲突。\n  普通索引 建立在普通字段上的索引被称为普通索引\n  前缀索引 前缀索引是指对字符类型字段的前几个字符或对二进制类型字段的前几个bytes建立的索引，而不是在整个字段上建索引。前缀索引可以建立在类型为char、varchar、binary、varbinary的列上，可以大大减少索引占用的存储空间，也能提升索引的查询效率\n ","date":"2022-11-26T11:17:45+08:00","permalink":"https://x-xkang.com/p/mysql-%E7%B4%A2%E5%BC%95%E6%B5%85%E6%9E%90/","title":"Mysql 索引浅析"},{"content":"Slice的数据结构 切片是对数组的一个连续片段的引用，所以切片是一个引用类型，一个长度可变的数组。 源码中的数据结构定义如下：\n源码位置： src/runtime/slice.go#L22\n1 2 3 4 5  type slice struct { array unsafe.Pointer // 引用的底层数组指针 \tlen int // 切片的长度 \tcap int // 切片的容量 }   创建切片 创建方式 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  func main() { // 1. 通过关键字 var 声明，此时未初始化，无法对s1[i]进行取值、赋值 \tvar s1 []int // 2. 使用 new() 创建，由于 new()返回的是一个指针，通过 *获取指针的值，未初始化，同样也无法对s2[i]进行取值赋值 \ts2 := *new([]int) // 3. 直接创建并初始化，已初始化，因此可以对s3[i]进行取值赋值 \ts3 := []int{1, 2, 3} // 4. make() 创建，第一个参数指定类型，第二个参数为长度，第三个参数为容量（可不指定，默认为长度值） \ts4 := make([]int, 4, 5) // 5. 通过截取数组或切片 \tarray := [7]int{1, 2, 3, 4, 5, 6, 7} s5 := array[3:5] // (左闭右开)，输出 [4,5]，引用数组 array 的部分片段 }    创建逻辑 src/runtime/slice.go#L88\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  // input: et: slice类型元信息，slice长度，slice容量 func makeslice(et *_type, len, cap int) unsafe.Pointer { // 调用MulUintptr函数：获取创建该切片需要的内存，是否溢出(超过2^64)  // 2^64是64位机能够表示的最大内存地址 \tmem, overflow := math.MulUintptr(et.size, uintptr(cap)) if overflow || mem \u0026gt; maxAlloc || len \u0026lt; 0 || len \u0026gt; cap { mem, overflow := math.MulUintptr(et.size, uintptr(len)) // 如果溢出或超过能够分配的最大内存(2^32 - 1) | 非法输入, 报错并返回 \tif overflow || mem \u0026gt; maxAlloc || len \u0026lt; 0 { panicmakeslicelen() } panicmakeslicecap() } // 调用mallocgc函数分配一块连续内存并返回该内存的首地址  // 该函数实现涉及到了go语言内存管理，比较复杂，不是本文的主题  // 关于 mallocgc 逻辑后续再讲 \treturn mallocgc(mem, et, true) }    根据slice的类型和长度以及容量，计算出所需要的内存空间大小， 如果合法则调用mallocgc函数申请相应的连续内存并返回首地址，  下面来看一下是如何计算所需要的内存大小的\nsrc\\runtime\\internal\\math\\math.go\n1 2 3 4 5 6 7 8 9 10 11 12 13  // MulUintptr returns a * b and whether the multiplication overflowed. // On supported platforms this is an intrinsic lowered by the compiler. func MulUintptr(a, b uintptr) (uintptr, bool) { // 如果slice类型大小为0(如struct{}类型) 或 (a | b) \u0026lt; 2 ^ 32，肯定不会发生溢出  // sys.PtrSize = 8 \tif a|b \u0026lt; 1\u0026lt;\u0026lt;(4*sys.PtrSize) || a == 0 { return a * b, false } // 如果a * b \u0026gt; MaxUintptr，即类型大小 * 切片容量 \u0026gt; MaxUintPtr，则说明发生了溢出  // MaxUintptr = ^uintptr(0) = 2 ^ 64 \toverflow := b \u0026gt; MaxUintptr/a return a * b, overflow }    追加元素 append() src/reflect/value.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  // Append appends the values x to a slice s and returns the resulting slice. // As in Go, each x\u0026#39;s value must be assignable to the slice\u0026#39;s element type. func Append(s Value, x ...Value) Value { s.mustBe(Slice) s, i0, i1 := grow(s, len(x)) for i, j := i0, 0; i \u0026lt; i1; i, j = i+1, j+1 { s.Index(i).Set(x[j]) } return s } // grow grows the slice s so that it can hold extra more values, allocating // more capacity if needed. It also returns the old and new slice lengths. func grow(s Value, extra int) (Value, int, int) { i0 := s.Len() i1 := i0 + extra if i1 \u0026lt; i0 { panic(\u0026#34;reflect.Append: slice overflow\u0026#34;) } m := s.Cap() if i1 \u0026lt;= m { return s.Slice(0, i1), i0, i1 } if m == 0 { m = extra } else { // const threshold = 256 // 1.18增加 \tfor m \u0026lt; i1 { if i0 \u0026lt; 1024 { m += m } else { m += m / 4 } } } t := MakeSlice(s.Type(), i1, m) Copy(t, s) return t, i0, i1 }   追加元素流程如下：\n 计算当前slice长度i0与追加元素的长度总和i1，若i1 \u0026lt; i0（追加后的slice长度小于当前的slice长度），则直接抛出异常 若当前切片容量为0，则将容量改为追加元素的长度 循环遍历增加容量， 若追加的元素长度小于阈值（v1.18之前是1024，从v1.18开始改为256），则容量成倍增长， 否则容量每次增加1/4,(注：v1.18开始计算逻辑改为 m = (m + 3 * threshold) / 4) ， 创建新的切片，并将当前切片拷贝到新的切片中，返回新的切片 将追加的元素存储到新的切片指定位置   切片的扩容逻辑：\n 原cap扩充一倍，即doublecap 如果指定cap(追加元素后的切片长度) 大于 doublecap，则取cap，否则如下：   cap 小于1024，取doublecap     每次增长 1/4，直至不小于cap   ","date":"2022-09-05T09:01:56+08:00","permalink":"https://x-xkang.com/p/golang-slice%E5%88%87%E7%89%87%E5%88%86%E6%9E%90/","title":"Golang Slice切片分析"},{"content":"Redis提供两种持久化方式：一种是默认的RDB持久化方式，另一种是AOF（append only file）持久化方式\n一、 RDB 是什么？\n 原理是Redis会通过单独创建（fork）一个与当前进程一模一样的子进程来进行持久化，这个子进程的所有数据（变量、环境变量、程序计数器等）都和原进程一模一样，会先将数据写入到一个临时文件中，待持久化结束了，再用这个临时文件替换上次持久化好的文件，整个过程冲，主进程不进行任何的io操作，这就确保了极高的性能。\n 1、持久化文件在哪  启动redis-server 的目录下\n 2、什么时候fork子进程，或者什么时候触发 rdb持久化机制 RDB 方式持久化数据是通过 fork 子进程，在子进程中进行数据同步\n shutdown时，如果没有开启aof，会触发配置文件中默认的快照配置 执行命令 save 或者 bgsave save是只管保存，不管其他，全部阻塞，使用主进程进行持久化 bgsave redis 会在后台异步进行快照操作，同时可以响应客户端的请求\n 3、优点  适合数据恢复\n 4、缺点  数据丢失多\n  原理是将Redis的操作日志以追加的方式写入文件，读操作是不记录的\n 二、AOF 为什么会出现AOF持久化方式\n1、这个持久化文件在哪  启动 redis-server 的目录下会生成 appendonly.aof文件\n 2、触发机制（根据配置文件的配置项\u0026ndash;appendfsync）  no: 表示操作系统进行数据缓存同步到磁盘（快，持久化没保证：写满缓冲区才会同步，若在缓冲区未写满前 shutdown 或其他意外关机，则这部分数据会丢失） always: 同步持久化，每次发生数据变更时（增删改操作），立即记录到磁盘（慢，安全） everysec: 表示每秒同步一次（默认值，很快，但可能会丢失1秒的数据）\n 3、AOF 重写机制 重写 AOF 文件会 fork 子进程去执行，会将内存中的数据写入新的 AOF 文件，并且是以RDB 的方式写入，重写结束后会替代旧的AOF 文件，后续的客户端命令操作又重新以 AOF的格式写入，redis.conf 中配置触发 AOF 文件重写的文件大小值auto-aof-rewrite-percentage 不宜太小，因为会频繁触发重写\n 触发时机    redis.conf 的配置项 auto-aof-rewrite-min-size 默认值是 64mb， 当 AOF 文件大小超过这个配置值时会自动开启重写 `。 redis.conf 的配置项 auto-aof-rewrite-percentage 默认值是100， 当 AOF 文件大小的增长率大于配置值时会自动开启重写。   4、优点  保证数据安全\n 5、缺点  数据恢复慢\n","date":"2022-06-29T15:34:24+08:00","permalink":"https://x-xkang.com/p/redis-%E6%8C%81%E4%B9%85%E5%8C%96%E7%AD%96%E7%95%A5/","title":"Redis 持久化策略"},{"content":"一、Redis 缓存雪崩  出现场景   如果使用redis记录大量的热点数据，且过期时间为同一个常量，那么可能会出现大批的缓存数据会在同一时间或较短的时间区间内失效，redis会根据淘汰策略进行优化，如果数据量比较大会导致线程出现短暂的阻塞；另外，因为大量的缓存失效，会导致请求直接落在DB上，请求数较大情况下会直接导致数据库瘫痪，然后整个业务系统变为不可用状态\n  解决方案   针对这种情况，我们可以在设置过期时间时加上一个随机值，类似 redis.set(key, value, expiredTime + Math.random()*10000)，这样设置就不会出现在短时间内大量缓存key失效的情况。\n 二、Redis 缓存穿透  出现场景   如果用户请求的热点数据本身是不存在的，比如id为-1，或者id=\u0026lsquo;\u0026lsquo;的数据，查询缓存不存在后会将请求直接打到DB上，最终在DB中也没有查到此数据，此时Redis缓存就是去了作用，搞并发的情况下会降低数据库性能，甚至瘫痪\n  解决方案    增加参数校验，拦截掉大量的非法参数请求； 缓存空值，因为数据库中本来就不存在这些数据，因此可以在第一次重建缓存时将value 记录为 null，下次请求时从Redis获取到 null 值直接返回（注意，要对redis查询的返回值进行严格校验，区分key不存在返回的空值和主动设置的空值null）； 布隆过滤器，将DB中的热点数据加载至布隆过滤器中（布隆过滤器的特性：若在过滤器中存在，不一定真是存在；若不存在时，一定不存在），每次请求前先校验布隆过滤器是否存在该key，不存在的话直接return；   三、Redis 缓存击穿  出现场景   高并发请求同一个热点数据，在热点数据失效的瞬间，大量请求在缓存中没有命中会直接落在DB上进行查询，导致DB压力瞬间增加\n  解决方案   增加互斥锁，在第一个请求没有在缓存命中开始在DB进行查询并重加缓存时加上一个互斥锁，在缓存重建完成之前，对这同一热点数据的请求将会被互斥锁拦截，被拦截的这些请求根据业务需求，可以延时重试直到拿到数据或直接返回失败等； 热点数据不设置过期时间（不建议，随着热点数据的增加，无过期时间的key也越来越多，或导致Redis的存储压力增加）\n","date":"2022-06-28T14:58:25+08:00","permalink":"https://x-xkang.com/p/redis%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E4%B9%8B%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9%E5%87%BB%E7%A9%BF%E5%92%8C%E7%A9%BF%E9%80%8F/","title":"Redis基础知识之缓存雪崩、击穿和穿透"},{"content":"HTTP/0.9 - 单行协议\n 最初版本的HTTP协议并没有版本号，后来它的版本号被定位在0.9以区分后来的版本。HTTP/0.9极其简单：请求由单行指令构成，以唯一可用方法GET开头，其后跟目标资源的路径（一旦连接到服务器，协议、服务器、端口号这些都不是必须的）。 跟后来的版本不同，HTTP/0.9的响应内容并不包含HTTP头，这意味着只有 HTML 文件可以传送，无法传送其他类型的文件；也没有状态码或者错误代码；一旦出现问题，一个特殊的包含问题描述信息的HTML文件将被发回，供人们查看。\n  HTTP/1.0 - 构建可扩展性\n 由于 HTTP/0.9协议的应用十分有限，浏览器和服务器迅速扩展内容使其用途更广：\n 协议版本信息现在会随着每个请求发送（HTTP/1.0被追加到了 GET行） 状态码会在响应开始时发送，使浏览器能了解请求执行成功或失败，并响应调整行为（如更新或使用本地缓存） 引入了 HTTP 头的概念，无论是对于请求还是响应，允许传输元数据，使协议变得非常灵活，更具扩展性。 在新 HTTP 头的的帮助下，具备了传输纯文本HTML文件以外其他类型文档的能力（凭借Content-Type头）   一个典型的请求看起来就像这样：\n1 2 3 4 5 6 7 8 9 10 11  GET /mypage.html HTTP/1.0 User-Agent: NCSA_Mosaic/2.0 (Windows 3.1) 200 OK Date: Tue, 15 Nov 1994 08:12:31 GMT Server: CERN/3.0 libwww/2.17 Content-Type: text/html \u0026lt;HTML\u0026gt; 一个包含图片的页面 \u0026lt;IMG SRC=\u0026#34;/myimage.gif\u0026#34;\u0026gt; \u0026lt;/HTML\u0026gt;   接下来是第二个请求：\n1 2 3 4 5 6 7 8  GET /myimage.gif HTTP/1.0 User-Agent: NCSA_Mosaic/2.0 (Windows 3.1) 200 OK Date: Tue, 15 Nov 1994 08:12:32 GMT Server: CERN/3.0 libwww/2.17 Content-Type: text/gif (这里是图片内容)    在 1991-1995年，这些新扩展并没有被引入到标准中以促进下注工作，而仅仅作为一种尝试：服务器和浏览器天界这些新扩展功能，但出现了大量的互操作问题。知道 1996年11月，为了解决这些问题，一份新文档（RFC 1945） 被发表出来，泳衣描述如何操作实践这些新扩展功能，文档 RFC 1945 定义了 HTTP/1.0，但它是狭义的，并不是官方标准\n  HTTP/1.1 - 标准化的协议\nHTTP/1.0 多种不同的实现方式在实际运用中显得有些混乱，自 1995 年开始，即 HTTP/1.0 文档发布的下一年，就开始修订 HTTP 的第一个标准化版本。在 1997 年初，HTTP1.1 标准发布，就在 HTTP/1.0 发布的几个月后。\n HTTP/1.1 消除了大量歧义内容并引入了多项改进：\n 连接可以复用，节省了多次打开 TCP 连接加载网页文档资源的时间。 增加管线化技术，允许在第一个应答被完全发送之前就发送第二个请求，以降低通信延迟。 支持响应分块。 引入额外的缓存控制机制。 引入内容协商机制，包括语言，编码，类型等，并允许客户端和服务器之间约定以最合适的内容进行交换。 凭借Host头，能够使不同域名配置在同一个 IP 地址的服务器上。   一个典型的请求流程， 所有请求都通过一个连接实现，看起来就像这样：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  GET /en-US/docs/Glossary/Simple_header HTTP/1.1 Host: developer.mozilla.org User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:50.0) Gecko/20100101 Firefox/50.0 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8 Accept-Language: en-US,en;q=0.5 Accept-Encoding: gzip, deflate, br Referer: https://developer.mozilla.org/en-US/docs/Glossary/Simple_header 200 OK Connection: Keep-Alive Content-Encoding: gzip Content-Type: text/html; charset=utf-8 Date: Wed, 20 Jul 2016 10:55:30 GMT Etag: \u0026#34;547fa7e369ef56031dd3bff2ace9fc0832eb251a\u0026#34; Keep-Alive: timeout=5, max=1000 Last-Modified: Tue, 19 Jul 2016 00:59:33 GMT Server: Apache Transfer-Encoding: chunked Vary: Cookie, Accept-Encoding (content)   HTTP/1.1 在 1997 年 1 月以 RFC 2068 文件发布。\nHTTP 用于安全传输\nHTTP 最大的变化发生在 1994 年底。HTTP 在基本的 TCP/IP 协议栈上发送信息，网景公司（Netscape Communication）在此基础上创建了一个额外的加密传输层：SSL 。SSL 1.0 没有在公司以外发布过，但 SSL 2.0 及其后继者 SSL 3.0 和 SSL 3.1 允许通过加密来保证服务器和客户端之间交换消息的真实性，来创建电子商务网站。SSL 在标准化道路上最终成为 TLS，随着版本 1.0, 1.1, 1.2 的出现成功地关闭漏洞。TLS 1.3 目前正在形成。\n与此同时，人们对一个加密传输层的需求也愈发高涨：因为 Web 最早几乎是一个学术网络，相对信任度很高，但如今不得不面对一个险恶的丛林：广告客户、随机的个人或者犯罪分子争相劫取个人信息，将信息占为己有，甚至改动将要被传输的数据。随着通过 HTTP 构建的应用程序变得越来越强大，可以访问越来越多的私人信息，如地址簿，电子邮件或用户的地理位置，即使在电子商务使用之外，对 TLS 的需求也变得普遍。\nHTTP 用于复杂应用\nTim Berners-Lee 对于 Web 的最初设想不是一个只读媒体。 他设想一个 Web 是可以远程添加或移动文档，是一种分布式文件系统。 大约 1996 年，HTTP 被扩展到允许创作，并且创建了一个名为 WebDAV 的标准。 它进一步扩展了某些特定的应用程序，如 CardDAV 用来处理地址簿条目，CalDAV 用来处理日历。 但所有这些 *DAV 扩展有一个缺陷：它们必须由要使用的服务器来实现，这是非常复杂的。并且他们在网络领域的使用必须保密。\n在 2000 年，一种新的使用 HTTP 的模式被设计出来：representational state transfer (或者说 REST)。 由 API 发起的操作不再通过新的 HTTP 方法传达，而只能通过使用基本的 HTTP / 1.1 方法访问特定的 URI。 这允许任何 Web 应用程序通过提供 API 以允许查看和修改其数据，而无需更新浏览器或服务器：所有需要的内容都被嵌入到由网站通过标准 HTTP/1.1 提供的文件中。 REST 模型的缺点在于每个网站都定义了自己的非标准 RESTful API，并对其进行了全面的控制；不同于 *DAV 扩展，客户端和服务器是可互操作的。 RESTful API 在 2010 年变得非常流行。\n自2005年以来，可用于 Web 页面的API大大增加，其中几个API为特定目的扩展了HTTP协议，大部分是新的特定HTTP头：\n Server-sent events 服务器可以偶尔推送消息到浏览器 Websocket 一个新协议，可以通过升级现有 HTTP 协议来建立  放松安全措施-基于当前的web模型\n HTTP 和 Web安全模型 \u0026ndash; 同源策略是互不相关的。事实上，当前的Web安全模型是在HTTP被创造出来后才被发展的！这些年来，已经在证实了它如果能通过在特定的约束下移除一些这个策略的限制来管的宽松些的话，将会更有用。这些策略导致大量的成本和时间被话费在通过转交到服务端来添加一些新的HTTP头来发送。这些被定义在了 Cross-Origin Resource Sharing(CORS) or the Content Security Policy(CSP)规范里。\n 不只是这大量的扩展，很多的其他的头也被加了进来，有些只是实验性的，比较著名的有 Do Not Track(DNT)来控制隐私， X-Frame-Option，还有很多\nHTTP/2 - 为了更优异的表现\n这些年来，网页愈渐变得的复杂，甚至演变成了独有的应用，可见媒体的播放量，增进交互的脚本大小也增加了许多：更多的数据通过 HTTP 请求被传输。HTTP/1.1 链接需要请求以正确的顺序发送，理论上可以用一些并行的链接（尤其是 5 到 8 个），带来的成本和复杂性堪忧。比如，HTTP 管线化（pipelining）就成为了 Web 开发的负担。\n在 2010 年到 2015 年，谷歌通过实践了一个实验性的 SPDY 协议，证明了一个在客户端和服务器端交换数据的另类方式。其收集了浏览器和服务器端的开发者的焦点问题。明确了响应数量的增加和解决复杂的数据传输，SPDY 成为了 HTTP/2 协议的基础。\nHTTP/2 和 HTTP/1.1 有几处基本的不同：\n HTTP/2 是二进制协议而不是文本协议。不在可读，也不是无障碍的手动创建，改善的优化技术现在可被实施。 这是一个复用协议。并行的请求能在同一个链接中处理，移除了HTTP/1.x 中顺序和阻塞的约束。 压缩了headers，因为 headers 在一系列请求中常常是相似的，其移除了重复和传输重复数据的成本。 其允许服务器在客户端缓存中填充数据，通过一个叫服务器推送的机制来提前请求。  HTTP/3 - 基于UDP的QUIC协议实现\n在HTTP/3中，将启用TCP协议，改为使用基于UDP协议的QUIC协议实现，此改变是为了解决HTTP/2中存在的队头阻塞问题，由于HTTP/2在单个TCP连接上使用了多路复用，收到TCP拥塞控制的影响，少量的丢包就可能导致整个TCP连接上的所有流被阻塞。\nQUIC（快速UDP网络连接）是一种实验性的网络传输协议，由Google开发，该协议旨在使网页传输更快。\n","date":"2022-06-17T10:09:23+08:00","permalink":"https://x-xkang.com/p/http-%E5%8D%8F%E8%AE%AE%E7%89%88%E6%9C%AC%E8%BF%9B%E5%8C%96/","title":"Http 协议版本进化"},{"content":"了解域名结构\n 以mail.qq.com域名为例，com为顶级域名，qq.com 为二级域名， mail.qq.com为三级域名\n 顶级域名服务器\n 顶级域名为最后一个.右侧部分的内容，如mail.qq.com的com就是顶级域名，顶级域名对应的服务器称之为顶级域名服务器\n 二级域名服务器\n 域名 mail.qq.com中的倒数第二个.右侧部分qq.com成为二级域名\n 根域名服务器\n 在2016年之前全球一共拥有13台根服务器，1台主根服务器在美国，其他12台为辅根服务器，其中美国9台，英国1台，瑞典1台，日本1台，这13台根服务器主要管理互联网的主目录，主要作用IPV4。 2016年，中国下一代互联网工程中心领衔发起雪人计划，旨在为下一代互联网(IPV6)提供更多的根服务器解决方案，该计划于2017年完成，其中包含3台主根服务器，中国1台，美国1台，日本1台，22台辅根服务器，中国3台，美国2台，印度和法国分别有3台，德国2台，俄罗斯、意大利、西班牙、奥地利、智利、南非、澳大利亚、瑞士、荷兰各1台，共22台，从此形成了13台原有根加25台IPV6根服务器的新格局\n 本地域名服务器\n 本地域名服务器的范围非常广，没有一个详细的定位，可能是某个运营商部署在该城市的一台服务器，也可能是某个公司的一台服务器，甚至可能是某个学校的服务器\n 存放地址\n 浏览器缓存 系统缓存 本地域名服务器 根域名服务器 顶级域名服务器 二级域名服务器 \u0026hellip; \u0026hellip;  查询顺序\n   检查浏览器缓存中是否存在改域名与IP地址的映射关系，如果有则解析结束，没有则继续    到系统本地查找映射关系，一般存在 hosts 文件中，如果有则解析结束，否则继续    到本地域名服务器去查询，有则结束，否则继续    本地域名服务器去查询 根域名服务器，该过程不会返回映射关系，只会告诉你去下级服务器（顶级域名服务器）查询    本地域名服务器查询顶级域名服务器（即 com 服务器），同样不会返回映射关系，只会引导你去二级域名服务器查询    本地域名服务器查询二级域名服务器（即 qq.com 服务器），引导去三级域名服务器    本地域名服务器查询三级域名服务器（即 mail.qq.com）， 此时已经是最后一级，如果有则返回映射关系，并且本地服务器加入自身的映射表中，方便下次查询，同时返回给用户的计算机，没有找到则网页报错   ","date":"2022-06-16T17:00:53+08:00","permalink":"https://x-xkang.com/p/dns-%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E6%B5%81%E7%A8%8B/","title":"DNS 域名解析流程"},{"content":"OSI七层模型与TCP/IP四层模型对比\n 应用层：为应用程序或用户请求提供请求服务。OSI参考模型最高层，也是最靠近用户的一层，为计算机用户、各种应用程序以及网络提供端口，也为用户直接提供各种网络服务。\n表示层：数据编码、格式转换、数据加密。提供各种用于应用层的编码和转换功能，确保一个系统的应用层发送的数据能被另一个系统的应用层识别。如果必要，该层可提供一种标准表示形式，用于讲计算机内部的各种数据格式转换成通信中采用的标准表示形式。数据压缩和加密也是表示层可提供的转换功能之一。\n会话层：创建、管理和维护会话。接收来自传输层的数据，负责建立、管理和终止表示层实体之间的通信会话，支持它们之间的数据交换。该层的通信由不同设备中的应用程序之间的服务请求和响应组成。\n传输层：数据通信。建立主机端到端的链接，为会话层和网络层提供端到端可靠的和透明的数据传输服务，确保数据能完整的传输到网络层。\n网络层：IP选址及路由选择。通过路由选择算法，为报文或通信子网选择最适当的路径。控制数据链路层与传输层之间的信息转发，建立、维持和终止网络的连接。数据链路层的数据在这一层被转换为数据包，然后通过路径选择、分段组合、顺序、进/出路由等控制，将信息从一个网络设备传送到另一个网络设备。\n数据链路层：提供介质访问和链路管理。接收来自物理层的位流形式的数据，封装成帧，传送到网络层；将网络层的数据帧，拆装为位流形式的数据转发到物理层；负责建立和管理节点间的链路，通过各种控制协议，将有差错的物理信道变为无差错的、能可靠传输数据帧的数据链路。\n物理层：管理通信设备和网络媒体之间的互联互通。传输介质为数据链路层提供物理连接，实现比特流的透明传输。实现相邻计算机节点之间比特流的透明传送，屏蔽具体传输介质和物理设备的差异。\n","date":"2022-06-15T15:57:49+08:00","permalink":"https://x-xkang.com/p/osi-%E4%B8%83%E5%B1%82%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/","title":"OSI 七层网络模型"},{"content":"一、HTTP状态码分类 HTTP 状态码由三个十进制数字组成，第一个十进制数字定义了状态码的类型，响应分为五类：消息响应（100-199），成功响应（200-299），重定向消息（300-399），客户端错误响应（400-499）和服务器错误响应（500-599）\n   分类 分来描述     1** 信息，服务器收到请求，需要请求者继续执行操作   2** 操作被成功接收并处理   3** 重定向，需要进一步操作以完成请求   4** 客户端错误，请求包含语法错误或无法完成请求   5** 服务器错误，服务器在处理请求的过程发生了错误    二、状态码分类描述    状态码 英文状态码 中文描述     100 Continue 这个临时响应表明，迄今为止的所有内容都是可行的，客户端应该继续请求，如果以完成，请忽略它。   101 Switching Protocols 该代码是响应客户端 Upgrade(en-US). 请求头发送的，指明服务器即将切换的协议   102 Processing 此代码表示服务器以收到并且正在处理该请求，但当前没有响应可用   103 Early Hints 次状态码主要用于与 Link 链接头一起使用，以允许用户代理在服务器响应阶段时开始预加载 preloading 资源   200 OK 请求成功   201 Created 该请求已成功，并因此创建了一个新的资源。这通常是在 POST 请求，或是某些 PUT 请求之后返回的响应   202 Accepted 请求已经接收到，但还未响应，没有结果。意味着不会有一个异步的响应去表明当前请求的结果，预期另外的进程和服务去处理请求，或者批处理。   203 Non-Authoritative Information 服务器已成功处理了请求，但返回的实体头部元信息不是在原始服务器上有效的确定集合，而是来自本地或者第三方的拷贝。当前的信息可能是原始版本的子集或者超集。例如，包含资源的元数据可能导致原始服务器知道元信息的超集。使用此状态码不是必须的，而且只有在响应不使用此状态码便会返回200 OK的情况下才是合适的。   204 No Content 对于该请求没有的内容可发送，但头部字段可能有用。用户代理可能会用此时请求头部信息来更新原来资源的头部缓存字段。   205 Reset Content 告诉用户代理重置发送此请求的文档   206 Partial Content 当从客户端发送Range范围标头以只请求资源的一部分时，将使用此响应代码。   207 Multi Status 对于多个状态代码都可能合适的情况，传输有关多个资源的信息。   208 Already Reported 在 DAV 里面使用 dav:propstat 响应元素以避免重复枚举多个绑定的内部成员到同一个集合。   226 IM Used 服务器已经完成了对资源的GET请求，并且响应是对当前实例应用的一个或多个实例操作结果的表示。   300 Multiple Choice 请求拥有不只一个的可鞥响应。用户带来或者用户应当从中选择一个。 (没有标准化的方法来选择其中一个响应，但是建议使用指向可能性的 HTML 链接，以便用户可以选择。)   301 Moved Permanently 请求资源的 URL 已永久更改。在响应中给出了新的 URL。   302 Found 此响应代码表示所请求资源的 URI 已 暂时 更改。未来可能会对 URI 进行进一步的改变。因此，客户机应该在将来的请求中使用这个相同的 URI。   303 See Other 服务器发送此响应，以指示客户端通过一个 GET 请求在另一个 URI 中获取所请求的资源   304 Not Modified 这是用于缓存的目的。它告诉客户端响应还没有被修改，因此客户端可以继续使用相同的缓存版本的响应。   305 Use Proxy 在 HTTP 规范中定义，以指示请求的响应必须被代理访问。由于对代理的带内配置的安全考虑，它已被弃用   306 unused 此响应代码不再使用；它只是保留。它曾在 HTTP/1.1 规范的早期版本中使用过。   307 Temporary Redirect 服务器发送此响应，以指示客户端使用在前一个请求中使用的相同方法在另一个 URI 上获取所请求的资源。这与 302 Found HTTP 响应代码具有相同的语义，但用户代理 不能 更改所使用的 HTTP 方法：如果在第一个请求中使用了 POST，则在第二个请求中必须使用 POST   308 Permanent Redirect 这意味着资源现在永久位于由Location: HTTP Response 标头指定的另一个 URI。 这与 301 Moved Permanently HTTP 响应代码具有相同的语义，但用户代理不能更改所使用的 HTTP 方法：如果在第一个请求中使用 POST，则必须在第二个请求中使用 POST。   400 Bad Request 由于被认为是客户端错误（例如，错误的请求语法、无效的请求消息帧或欺骗性的请求路由），服务器无法或不会处理请求。   401 Unauthorized 虽然 HTTP 标准指定了\u0026quot;unauthorized\u0026quot;，但从语义上来说，这个响应意味着\u0026quot;unauthenticated\u0026quot;。也就是说，客户端必须对自身进行身份验证才能获得请求的响应。   402 Payment Required 此响应代码保留供将来使用。创建此代码的最初目的是将其用于数字支付系统，但是此状态代码很少使用，并且不存在标准约定。   403 Forbidden 客户端没有访问内容的权限；也就是说，它是未经授权的，因此服务器拒绝提供请求的资源。与 401 Unauthorized 不同，服务器知道客户端的身份。   404 Not Found 服务器找不到请求的资源。在浏览器中，这意味着无法识别 URL。在 API 中，这也可能意味着端点有效，但资源本身不存在。服务器也可以发送此响应，而不是 403 Forbidden，以向未经授权的客户端隐藏资源的存在。这个响应代码可能是最广为人知的，因为它经常出现在网络上。   405 Method Not Allowed 服务器知道请求方法，但目标资源不支持该方法。例如，API 可能不允许调用DELETE来删除资源。   406 Not Acceptale 当 web 服务器在执行 服务端驱动型内容协商机制](/zh-CN/docs/Web/HTTP/Content_negotiation#服务端驱动型内容协商机制)后，没有发现任何符合用户代理给定标准的内容时，就会发送此响应。   407 Proxy Authentication Required 类似于 401 Unauthorized 但是认证需要由代理完成。   408 Request Timeout 此响应由一些服务器在空闲连接上发送，即使客户端之前没有任何请求。这意味着服务器想关闭这个未使用的连接。由于一些浏览器，如 Chrome、Firefox 27+ 或 IE9，使用 HTTP 预连接机制来加速冲浪，所以这种响应被使用得更多。还要注意的是，有些服务器只是关闭了连接而没有发送此消息。   409 Conflict 当请求与服务器的当前状态冲突时，将发送此响应。   410 Gone 当请求的内容已从服务器中永久删除且没有转发地址时，将发送此响应。客户端需要删除缓存和指向资源的链接。HTTP 规范打算将此状态代码用于“有限时间的促销服务”。API 不应被迫指出已使用此状态代码删除的资源。   411 Length Required 服务端拒绝该请求因为 Content-Length 头部字段未定义但是服务端需要它。   412 Precondition Failed 客户端在其头文件中指出了服务器不满足的先决条件。   413 Payload Too Large 请求实体大于服务器定义的限制。服务器可能会关闭连接，或在标头字段后返回重试 Retry-After。   414 URI Too Long 客户端请求的 URI 比服务器愿意接收的长度长。   415 Unsupported Media Type 服务器不支持请求数据的媒体格式，因此服务器拒绝请求。   416 Rabge Not Satisfiale 无法满足请求中 Range 标头字段指定的范围。该范围可能超出了目标 URI 数据的大小。   417 Expectation Failed 此响应代码表示服务器无法满足 Expect 请求标头字段所指示的期望。   418 I'm Teapot 服务端拒绝用茶壶煮咖啡。笑话，典故来源茶壶冲泡咖啡   421 Misdirected Request 请求被定向到无法生成响应的服务器。这可以由未配置为针对请求 URI 中包含的方案和权限组合生成响应的服务器发送。   422 Unproccessable Entity 请求格式正确，但由于语义错误而无法遵循。   423 Locked 正在访问的资源已锁定。   424 Failed Dependency 由于前一个请求失败，请求失败。   425 Too Early 表示服务器不愿意冒险处理可能被重播的请求。   426 Upgrade Required 服务器拒绝使用当前协议执行请求，但在客户端升级到其他协议后可能愿意这样做。 服务端发送带有 Upgrade (en-US) 字段的 426 响应 来表明它所需的协议（们）。   428 Precondition Required 用户在给定的时间内发送了太多请求（\u0026ldquo;限制请求速率\u0026rdquo;）   431 Request Header Fields Too Large 服务器不愿意处理请求，因为其头字段太大。在减小请求头字段的大小后，可以重新提交请求。   451 Unavailable For Legal Reasons 用户代理请求了无法合法提供的资源，例如政府审查的网页。   500 Internal Server Error 服务器遇到了不知道如何处理的情况。   501 Not Implement 服务器不支持请求方法，因此无法处理。服务器需要支持的唯二方法（因此不能返回此代码）是 GET and HEAD.   502 Bad Gateway 此错误响应表明服务器作为网关需要得到一个处理这个请求的响应，但是得到一个错误的响应。   503 Service Unavailable 服务器没有准备好处理请求。常见原因是服务器因维护或重载而停机。请注意，与此响应一起，应发送解释问题的用户友好页面。这个响应应该用于临时条件和如果可能的话，HTTP 标头 Retry-After 字段应该包含恢复服务之前的估计时间。网站管理员还必须注意与此响应一起发送的与缓存相关的标头，因为这些临时条件响应通常不应被缓存。   504 Gateway Timeout 当服务器充当网关且无法及时获得响应时，会给出此错误响应。   505 HTTP Version Not Supported 服务器不支持请求中使用的 HTTP 版本。   506 Variant Also Negotiates 服务器存在内部配置错误：所选的变体资源被配置为参与透明内容协商本身，因此不是协商过程中的适当终点   507 Insufficient Storage 无法在资源上执行该方法，因为服务器无法存储成功完成请求所需的表示。   508 Loop Detected 服务器在处理请求时检测到无限循环。   510 Not Extented 服务器需要对请求进行进一步扩展才能完成请求。   511 Network Authentication Required 指示客户端需要进行身份验证才能获得网络访问权限。   ","date":"2022-06-14T21:33:58+08:00","permalink":"https://x-xkang.com/p/http-%E7%8A%B6%E6%80%81%E7%A0%81/","title":"Http 状态码"},{"content":"常用正则表达式    字符 描述     \\ 将下一个字符标记为一个特殊字符、或一个原义字符、或一个向后引用、或一个八进制转义符。例如，“n”匹配字符“n”。“\\n”匹配一个换行符。串行“\\”匹配“\\”而“(”则匹配“(”。   ^ 匹配输入字符串的开始位置。如果设置了RegExp对象的Multiline属性，^也匹配“\\n”或“\\r”之后的位置。   $ 匹配输入字符串的结束位置。如果设置了RegExp对象的Multiline属性，$也匹配“\\n”或“\\r”之前的位置。   * 匹配前面的子表达式零次或多次。例如，zo*能匹配“z”以及“zoo”。*等价于{0,}。   + 匹配前面的子表达式一次或多次。例如，“zo+”能匹配“zo”以及“zoo”，但不能匹配“z”。+等价于{1,}。   ? 匹配前面的子表达式零次或一次。例如，“do(es)?”可以匹配“does”或“does”中的“do”。?等价于{0,1}。   {n} n是一个非负整数。匹配确定的n次。例如，“o{2}”不能匹配“Bob”中的“o”，但是能匹配“food”中的两个o。   {n,} n是一个非负整数。至少匹配n次。例如，“o{2,}”不能匹配“Bob”中的“o”，但能匹配“foooood”中的所有o。“o{1,}”等价于“o+”。“o{0,}”则等价于“o*”。   {n,m} m和n均为非负整数，其中n\u0026lt;=m。最少匹配n次且最多匹配m次。例如，“o{1,3}”将匹配“fooooood”中的前三个o。“o{0,1}”等价于“o?”。请注意在逗号和两个数之间不能有空格。   ? 当该字符紧跟在任何一个其他限制符（*,+,?，{n}，{n,}，{n,m}）后面时，匹配模式是非贪婪的。非贪婪模式尽可能少的匹配所搜索的字符串，而默认的贪婪模式则尽可能多的匹配所搜索的字符串。例如，对于字符串“oooo”，“o+?”将匹配单个“o”，而“o   . 匹配除“\\n”之外的任何单个字符。要匹配包括“\\n”在内的任何字符，请使用像“(.   (pattern) 匹配pattern并获取这一匹配。所获取的匹配可以从产生的Matches集合得到，在VBScript中使用SubMatches集合，在JScript中则使用$0…$9属性。要匹配圆括号字符，请使用“(”或“)”。   (?:pattern) 匹配pattern但不获取匹配结果，也就是说这是一个非获取匹配，不进行存储供以后使用。这在使用或字符“(   (?=pattern) 正向肯定预查，在任何匹配pattern的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用。例如，“Windows(?=95   (?!pattern) 正向否定预查，在任何不匹配pattern的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用。例如“Windows(?!95   (?\u0026lt;=pattern) 反向肯定预查，与正向肯定预查类拟，只是方向相反。例如，“(?\u0026lt;=95   (?\u0026lt;!pattern) 反向否定预查，与正向否定预查类拟，只是方向相反。例如“(?\u0026lt;!95   x\\|y 匹配x或y。例如，“z   [xyz] 字符集合。匹配所包含的任意一个字符。例如，“[abc]”可以匹配“plain”中的“a”。   [^xyz] 负值字符集合。匹配未包含的任意字符。例如，“[^abc]”可以匹配“plain”中的“p”。   [a-z] 字符范围。匹配指定范围内的任意字符。例如，“[a-z]”可以匹配“a”到“z”范围内的任意小写字母字符。   [^a-z] 负值字符范围。匹配任何不在指定范围内的任意字符。例如，“[^a-z]”可以匹配任何不在“a”到“z”范围内的任意字符。   \\b 匹配一个单词边界，也就是指单词和空格间的位置。例如，“er\\b”可以匹配“never”中的“er”，但不能匹配“verb”中的“er”。   \\B 匹配非单词边界。“er\\B”能匹配“verb”中的“er”，但不能匹配“never”中的“er”。   \\cx 匹配由x指明的控制字符。例如，\\cM匹配一个Control-M或回车符。x的值必须为A-Z或a-z之一。否则，将c视为一个原义的“c”字符。   \\d 匹配一个数字字符。等价于[0-9]。   \\D 匹配一个非数字字符。等价于[^0-9]。   \\f 匹配一个换页符。等价于\\x0c和\\cL。   \\n 匹配一个换行符。等价于\\x0a和\\cJ。   \\r 匹配一个回车符。等价于\\x0d和\\cM。   \\s 匹配任何空白字符，包括空格、制表符、换页符等等。等价于[ \\f\\n\\r\\t\\v]。   \\S 匹配任何非空白字符。等价于[^ \\f\\n\\r\\t\\v]。   \\t 匹配一个制表符。等价于\\x09和\\cI。   \\v 匹配一个垂直制表符。等价于\\x0b和\\cK。   \\w 匹配包括下划线的任何单词字符。等价于“[A-Za-z0-9_]”。   \\W 匹配任何非单词字符。等价于“[^A-Za-z0-9_]”。   \\xn 匹配n，其中n为十六进制转义值。十六进制转义值必须为确定的两个数字长。例如，“\\x41”匹配“A”。“\\x041”则等价于“\\x04\u0026amp;1”。正则表达式中可以使用ASCII编码。.   \\num 匹配num，其中num是一个正整数。对所获取的匹配的引用。例如，“(.)\\1”匹配两个连续的相同字符。   \\n 标识一个八进制转义值或一个向后引用。如果\\n之前至少n个获取的子表达式，则n为向后引用。否则，如果n为八进制数字（0-7），则n为一个八进制转义值。   \\nm 标识一个八进制转义值或一个向后引用。如果\\nm之前至少有nm个获得子表达式，则nm为向后引用。如果\\nm之前至少有n个获取，则n为一个后跟文字m的向后引用。如果前面的条件都不满足，若n和m均为八进制数字（0-7），则\\nm将匹配八进制转义值nm。   \\nml 如果n为八进制数字（0-3），且m和l均为八进制数字（0-7），则匹配八进制转义值nml。   \\un 匹配n，其中n是一个用四个十六进制数字表示的Unicode字符。例如，\\u00A9匹配版权符号（©）。   ","date":"2022-01-11T09:29:51+08:00","permalink":"https://x-xkang.com/p/%E5%B8%B8%E7%94%A8%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/","title":"常用正则表达式"},{"content":"go:build go:linkname 1  //go:linkname localname importpath.name    该指令指示编译器使用 importpath.name 作为源码中声明为 localname 的变量的或函数的目标文件符号名称，但是由于这个伪指令可以破坏类型系统和包模块化，只有引用了 unsafe 包才可以使用。 简单来讲，就是 importpath.name 是 localname 的符号别名，编译器实际上会调用 localname，使用的前提是引入了 unsafe 包才能使用。\n 例如 time/time.go\n1 2  // Provided by package runtime. func now() (sec int64, nsec int32, mono int64)   runtime/timestub.go\n1 2 3 4 5 6 7  import _ \u0026#34;unsafe\u0026#34; // for go:linkname  //go:linkname time_now time.now func time_now() (sec int64, nsec int32, mono int64) { sec, nsec = walltime() return sec, nsec, nanotime() }   now 方法并没有具体实现，注释上也描述具体实现由 runtime 包完成，看一下 runtime 包中的代码，先引入了 unsafe 包，再定义了 //go:lickname time_now time.now。\n第一个参数time_now 代表本地变量或方法，第二个参数time.now标识需要建立链接的变量、方法路径。也就是说，//go:lickname 是可以跨包使用的。\n另外 go build 是无法编译 go:linkname的，必须使用单独的 compile 命令进行编译，因为 go build 会加上 -complete 参数，这个参数会检查到没有方法体的方法，并且不通过。\n go:noscape 1  //go:noscape   该指令指定下一个有声明但没有主体（意味着实现有可能不是 Go）的函数，不允许编译器对其做逃逸分析。\n一般情况下，该指令用于内存分配优化。编译器默认会进行逃逸分析，会通过规则判定一个变量是分配到堆上还是栈上。\n但凡事有意外，一些函数虽然逃逸分析其是存放到堆上。但是对于我们来说，它是特别的。我们就可以使用 go:noescape 指令强制要求编译器将其分配到函数栈上。\n例如 1 2 3 4  // memmove copies n bytes from \u0026#34;from\u0026#34; to \u0026#34;to\u0026#34;. // in memmove_*.s //go:noescape func memmove(to, from unsafe.Pointer, n uintptr)   我们观察一下这个案例，它满足了该指令的常见特性。如下：\nmemmove_*.s: 只有声明，没有主体，其主体是由底层汇编实现的。\nmemmove: 函数功能，在栈上处理性能会更好。\n go:noslip 1  //go:noslip   该指令指定文件中声明的下一个函数不得包含堆栈溢出检查。\n简单来讲，就是这个函数跳过堆栈溢出的检查。\n例如 1 2 3 4  //go:nosplit func key32(p *uintptr) *uint32 { return (*uint32)(unsafe.Pointer(p)) }    go:nowritebarrierrec 1  //go:nowritebarrierrec   该指令表示编译器遇到写屏障时就会产生一个错误，并且允许递归。也就是这个函数调用的其他函数如果有写屏障也会报错。\n简单来讲，就是针对写屏障的处理，防止其死循环。\n例如 1 2 3 4 5  //go:nowritebarrierrec func gcFlushBgCredit(scanWork int64) { ... }    go:yeswritebarrierrec 1  //go:yeswritebarrierrec   该指令与 go:nowritebarrierrec 相对，在标注 go:nowritebarrierrec 指令的函数上，遇到写屏障会产生错误。\n而当编译器遇到 go:yeswritebarrierrec 指令时将会停止。\n例如 1 2 3 4  //go:yeswritebarrierrec func gchelper() { ... }    go:noinline 1  //go:noinline   该指令表示该函数禁止进行内联。\n1 2 3 4  //go:noinline func unexportedPanicForTesting(b []byte, i int) byte { return b[i] }   例如 我们观察一下这个案例，是直接通过索引取值，逻辑比较简单。如果不加上 go:noinline 的话，就会出现编译器对其进行内联优化。\n显然，内联有好有坏。该指令就是提供这一特殊处理。\n go:norace 1  //go:norace   该指令表示禁止进行竞态检测。\n常见的形式就是在启动时执行 go run -race，能够检测应用程序中是否存在双向的数据竞争，非常有用。\n例如 1 2 3 4  //go:norace func forkAndExecInChild(argv0 *byte, argv, envv []*byte, chroot, dir *byte, attr *ProcAttr, sys *SysProcAttr, pipe int) (pid int, err Errno) { ... }    go:notinheap 1  //go:notinheap   该指令常用于类型声明，它表示这个类型不允许从 GC 堆上进行申请内存。\n在运行时中常用其来做较低层次的内部结构，避免调度器和内存分配中的写屏障，能够提高性能。\n例如 1 2 3 4 5 6 7 8 9  // notInHeap is off-heap memory allocated by a lower-level allocator // like sysAlloc or persistentAlloc. // // In general, it\u0026#39;s better to use real types marked as go:notinheap, // but this serves as a generic type for situations where that isn\u0026#39;t // possible (like in the allocators). // //go:notinheap type notInHeap struct{}    每日一算 描述 2个逆序的链表，要求从低位开始相加，得出结果也逆序输出，返回值是逆序结果链表的头结点\n解题思路 需要注意的是进位的问题，极端情况如下：\n Input: (9 -\u0026gt; 9 -\u0026gt; 9 -\u0026gt; 9) + (1 -\u0026gt; ) Output 0 -\u0026gt; 0 -\u0026gt; 0 -\u0026gt; 0 -\u0026gt; 1\n 为了处理方法统一，可以先建立一个虚拟头结点，这个虚拟头结点的Next指向真正的head，这样head不需要单独处理，直接wihle循环即可。另外判断循环终止的条件不用是 p.Next != nil，这样最后一位还需要额外计算，终止条件应该是 p != nil。\n代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63  type ListNode struct { Val int Next *ListNode } func addTwoNumbers(l1 *ListNode, l2 *ListNode) *ListNode { if l1 == nil || l2 == nil{ return nil } // 虚拟头结点 \thead := \u0026amp;ListNode{ Val: 0, Next: nil, } current := head carry := 0\t// 是否需要进位  // 遍历 \tfor l1 != nil || l2 != nil { var x, y int if l1 == nil { x = 0 }else{ x = l1.Val } if l2 == nil { y = 0 }else { y = l2.Val } current.Next = \u0026amp;ListNode{ Val: (x + y + carry) % 10, Next: nil, } current = current.Next carry = (x+y+carry) / 10 if l1 != nil { l1 = l1.Next } if l2 != nil { l2 = l2.Next } fmt.Println(\u0026#34;carry:\u0026#34;, carry) } if carry \u0026gt; 0 {\t// 最后一位相加又进位，要在尾结点再加一个结点 \tcurrent.Next = \u0026amp;ListNode{ Val: carry % 10, Next: nil, } } return head.Next }   ","date":"2021-12-16T09:35:02+08:00","permalink":"https://x-xkang.com/p/golang-%E6%BA%90%E7%A0%81%E9%87%8C%E7%9A%84-/go-%E6%8C%87%E4%BB%A4%E9%83%BD%E4%BB%A3%E8%A1%A8%E4%BB%80%E4%B9%88%E6%84%8F%E6%80%9D/","title":"Golang 源码里的 //go: 指令，都代表什么意思？"},{"content":"ASCII   ascii码使用指定的7位或8位二进制数组合表示128或256种可能的字符，标准ASCII码也叫基础ASCII码，使用7位二进制数（剩下的1位二进制为0）来表示所有的大写和小写字母，数字0到9、标点符号，以及再美式英语中使用的特殊字符，其中：\n   0 ~ 31及127（共33个）是控制字符或通信专用字符（其余为可显示字符），如控制字符：LF（换行）, CR（回车）；通信专用字符：SOH（文头）、ACK（确认）等；ASCII值为8、9、10和13分别转换为退格、制表、换行和回车，它们并没有特定的图形显示，但会依不同的应用程序，而对文本显示不同的影响。     32 ~ 126（共95个）是字符（32是空格），其中48~57位0到9十个阿拉伯数字，65 ~ 90为26个大写英文字母，97 ~ 122为26个小写英文字母，其余为一些标点符号、运算符号等。    同时还要注意，在标准ASCII中，其中最高位（b7）用作奇偶校验。所谓奇偶校验，是指在代码传送过程中用来校验是否出现错误的一种方法，一般分为奇校验和偶校验两种。\n   奇校验规定：正确的代码一个字节中1的个数必须是1，若非技术，则在最高位b7添1.     偶校验规定：正确的代码一个字节中1的个数必须是偶数，若非偶数，则在最高位（b7）添1.    后128个称为扩展ASCII码。许多基于x86的系统都支持使用扩展（或“高”ASCII）。扩展ASCII码允许将每个字符的第8位用于确定附加的128个特殊符号字符、外来语字母和图形符号\n  Unicode   Unicode是国际组织制定的可以容纳世界上所有文字和符号的字符编码方案。Unicode用数字0-0x10FFFF来映射这些字符，最多可以容纳1114112个字符，或者说有1114112个码位。码位就是可以分配给字符的数字。UTF-8、UTF-16、UTF-32都是将数字转换到程序数据的编码方案。\n  Unicode 源于一个很简单的想法：将全世界所有的字符包含在一个集合里，计算机只要支持这一个字符集，就能显示所有的字符，再也不会有乱码了。\n    它从 0 开始，为每个符号指定一个编号，这叫做”码点”（code point）。比如，码点 0 的符号就是 null（表示所有二进制位都是 0）。 这么多符号，Unicode 不是一次性定义的，而是分区定义。每个区可以存放 65536 个（2^16）字符，称为一个平面（plane）。目前，一共有 17 个平面，也就是说，整个 Unicode 字符集的大小现在是 2^21。 最前面的 65536 个字符位，称为基本平面（缩写 BMP），它的码点范围是从 0 一直到 2^16-1，写成 16 进制就是从 U+0000 到 U+FFFF。所有最常见的字符都放在这个平面，这是 Unicode 最先定义和公布的一个平面。 剩下的字符都放在辅助平面（缩写 SMP），码点范围从 U+010000 一直到 U+10FFFF。 Unicode 只规定了每个字符的码点，到底用什么样的字节序表示这个码点，就涉及到编码方法。    Unicode 编码方案    之前提到，Unicode 没有规定字符对应的二进制码如何存储。以汉字“汉”为例，它的 Unicode 码点是 0x6c49，对应的二进制数是 110110001001001，二进制数有 15 位，这也就说明了它至少需要 2 个字节来表示。可以想象，在 Unicode 字典中往后的字符可能就需要 3 个字节或者 4 个字节，甚至更多字节来表示了。 这就导致了一些问题，计算机怎么知道你这个 2 个字节表示的是一个字符，而不是分别表示两个字符呢？这里我们可能会想到，那就取个最大的，假如 Unicode 中最大的字符用 4 字节就可以表示了，那么我们就将所有的字符都用 4 个字节来表示，不够的就往前面补 0。这样确实可以解决编码问题，但是却造成了空间的极大浪费，如果是一个英文文档，那文件大小就大出了 3 倍，这显然是无法接受的。 于是，为了较好的解决 Unicode 的编码问题， UTF-8 和 UTF-16 两种当前比较流行的编码方式诞生了。当然还有一个 UTF-32 的编码方式，也就是上述那种定长编码，字符统一使用 4 个字节，虽然看似方便，但是却不如另外两种编码方式使用广泛。   UTF8 UTF-8 是一个非常惊艳的编码方式，漂亮的实现了对 ASCII 码的向后兼容，以保证 Unicode 可以被大众接受。\n UTF-8 是目前互联网上使用最广泛的一种 Unicode 编码方式，它的最大特点就是可变长。它可以使用 1 - 4 个字节表示一个字符，根据字符的不同变换长度。编码规则如下：    1.对于单个字节的字符，第一位设为 0，后面的 7 位对应这个字符的 Unicode 码点。因此，对于英文中的 0 - 127 号字符，与 ASCII 码完全相同。这意味着 ASCII 码那个年代的文档用 UTF-8 编码打开完全没有问题。 2.对于需要使用 N 个字节来表示的字符（N \u0026gt; 1），第一个字节的前 N 位都设为 1，第 N + 1 位设为 0，剩余的 N - 1 个字节的前两位都设位 10，剩下的二进制位则使用这个字符的 Unicode 码点来填充。   编码规则如下：\n   Unicode 十六进制码点范围 UTF-8 二进制     0000 0000 - 0000 007F 0xxxxxxx   0000 0080 - 0000 07FF 110xxxxx 10xxxxxx   0000 0800 - 0000 FFFF 1110xxxx 10xxxxxx 10xxxxxx   0001 0000 - 0010 FFFF 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx      根据上面编码规则对照表，进行 UTF-8 编码和解码就简单多了。下面以汉字“汉”为利，具体说明如何进行 UTF-8 编码和解码。\n  “汉”的 Unicode 码点是 0x6c49（110 1100 0100 1001），通过上面的对照表可以发现，0x0000 6c49 位于第三行的范围，那么得出其格式为 1110xxxx 10xxxxxx 10xxxxxx。接着，从“汉”的二进制数最后一位开始，从后向前依次填充对应格式中的 x，多出的 x 用 0 补上。这样，就得到了“汉”的 UTF-8 编码为 11100110 10110001 10001001，转换成十六进制就是 0xE6 0xB7 0x89。\n  解码的过程也十分简单：如果一个字节的第一位是 0 ，则说明这个字节对应一个字符；如果一个字节的第一位 1，那么连续有多少个 1，就表示该字符占用多少个字节。\n   每日一算 描述  在数组中找到 2 个数之和等于给定值的数字，结果返回 2 个数字在数组中的下标\n 解题思路  利用map的特性，遍历数组预算计算出和给定值的差值，也就是目标元素，若值已经在map中，说明已找到，若没有，则把当前元素以 值=\u0026gt;下标 的形式存到map中\n 代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  func twoSum(arr []int, target int)[]int{ var m = map[int]int{} for i := 0; i \u0026lt; len(arr); i++ { another := target - arr[i] if _, ok := m[another]; ok == true { return []int{m[another], i} }else{ m[arr[i]] = i } } return []int{-1, -1} }   ","date":"2021-12-02T09:31:17+08:00","permalink":"https://x-xkang.com/p/asciiunicode%E5%92%8Cutf8/","title":"ASCII、Unicode和UTF8"},{"content":"问题描述 生产环境每隔一段时间会出现mysql数据库的死锁日志:\n同一条update语句，where 条件不同，但是会触发死锁，于是查看阿里云的数据库死锁日志，如下：\nupdate 语句如果使用的是主键索引，会将主键索引锁住，如果是普通索引，会先将普通索引锁住，然后根据普通索引的主键id再锁住主键索引，同一条update语句的索引执行应该是一样的，不应该存在互相等待释放的情况，于是有点陷入僵局，google一下有遇到相似问题的帖子，查看了执行计划，之前也看了执行计划，但是忽略了type字段和extra字段，结果如下：\n索引合并查询，会同时使用idx_status_vmstatus 和 uniq_instance 扫描记录并给普通索引加锁，然后通过普通索引中的主键ID去锁定主键索引，问题就出现在这里，由于 idx_status_vmstatus 索引扫描和 uniq_instance索引扫描是同时的，如果两条update语句同时执行，则 事务2 先锁定 锁定 uniq_instance 成功后锁定对应的主键，然后事务1 锁定idx_status_vmstatus 成功后也去锁定主键,此时主键已被事务2锁定，于是阻塞等待primary释放，接着事务2开始扫描 idx_status_vmstatus 发现普通索引被事务1锁住，于是阻塞等待idx_status_vmstatus，于是出现最终的 事务2等待 事务2释放idx_status_vmstatus，事务1等待事务1释放primary，即出现死锁。\n解决方案也比较简单，先查出主键ID，使用主键ID再更新记录，因为使用主键ID直接加锁的话，锁粒度更小，及时同时更新一条记录，也不会出现同时等待对方将锁释放的场景。问题描述的比较简单，但在排查过程中还是走了不少弯路的。\n","date":"2021-11-22T10:48:54+08:00","permalink":"https://x-xkang.com/p/mysql-deadlock/","title":"Mysql Deadlock"},{"content":"一、GMT 1、什么是GMT GMT (Greenwich Mean Time) 格林威治标准时间\n 它规定太阳每天经过位于英国伦敦郊区的皇家格林威治天文台的时间为中午12点。  2、GMT的历史   格林威治皇家天文台为了海上霸权的扩张计划，在十七世纪就开始进行天体观测。为了天文观测，选择了穿过英国伦敦格林威治天文台子午仪中心的一条经线作为零度参考线，这条线，简称格林威治子午线。\n  1884年10月在美国华盛顿召开了一个国际子午线会议，该会议将格林威治子午线设定为本初子午线，并将格林威治平时 (GMT, Greenwich Mean Time) 作为世界时间标准（UT, Universal Time）。由此也确定了全球24小时自然时区的划分，所有时区都以和 GMT 之间的偏移量做为参考。\n  1972年之前，格林威治时间（GMT）一直是世界时间的标准。1972年之后，GMT 不再是一个时间标准了。\n  由于地球在它的椭圆轨道里的运动速度不均匀，这个时间可能和实际的太阳时相差16分钟。\n  二、UTC 1、什么是UTC ？ UTC (Coodinated Universal Time)，协调世界时\n  又称世界统一时间、世界标准时间、国际协调时间。由于英文（CUT）和法文（TUC）的缩写不同，作为妥协，简称UTC\n  UTC是现在全球通用的时间标准，全球各地都同意将各自的时间进行同步协调。UTC时间是经过平均太阳时（以格林威治时间GMT为准）、地轴运动修正后的新时标以及以秒为单位的国际原子时所综合精算而成\n  在军事中协调世界时会用”Z“来表示。又由于Z在无线电联络中使用”Zulu“作代称，协调世界时也会被称为”Zulu time“。\n  2、UTC的组成  原子时间（TAI，International Atomic Time）：结合了劝阻400个所有的原子钟而得到的时间，它决定了我们每个人的中标中，时间流动的速度。 世界时间（UT，Universal Time）： 也称天文时间，或太阳时，它的依据是地球的自转，我们用它来确定多少原子时，对应于一个地球日的时间长度。  3、UTC的历史  1960年，国际无线电咨询委员会规范统一了UTC的概念，并在次年投入实际使用。1967年以前，UTC被数次调整过，原因是要使用润秒（leap second）来将UTC和地球自转时间进行统一。\n 三、GMT与UTC  GMT是前世界标准时，UTC是现世界标准时； UTC比GMT更准确，以原子时计时，适应现代社会的精准计时，但在不需要精确到秒的情况下，二者可视为等同； 每年格林尼治天文台会发调时信息，给予UTC。 ","date":"2021-09-29T13:38:27+08:00","permalink":"https://x-xkang.com/p/gmt-%E4%B8%8E-utc-%E6%97%B6%E9%97%B4%E6%A0%BC%E5%BC%8F/","title":"GMT 与 UTC 时间格式"},{"content":"我们平时使用的查询 sql 基本格式如下：\n1 2 3 4 5 6 7 8 9  SELECT DISTINCT \u0026lt;select_list\u0026gt; FROM \u0026lt;left_table\u0026gt; \u0026lt;join_type\u0026gt; JOIN \u0026lt;right_table\u0026gt; ON \u0026lt;join_condition\u0026gt; WHERE \u0026lt;where_condition\u0026gt; GROUP BY \u0026lt;group_by_condition\u0026gt; HAVING \u0026lt;having_condition\u0026gt; ORDER BY \u0026lt;order_by\u0026gt; LIMIT \u0026lt;limit_number\u0026gt;;   实际的执行顺序并不是如上书写顺序一样的：\n FROM: 对 from 左右的表计算笛卡尔积，产生虚拟表VT1； ON: 对笛卡尔积进行筛选，只有符合条件的行才会被记录到虚拟表VT2中； JOIN: 如果是 OUT JOIN，那么将保留表中（如左表或者右表）未匹配的行作为外部行添加到虚拟表VT2中，从而产生了虚拟表VT3； WHERE: 对 JOIN 之后的虚拟表VT3进行进一步的筛选，满足条件的留下生成虚拟表VT4； GROUP BY: 对虚拟表VT4进行分组，生成VT5； HAVING: 对分组后的VT5进行筛选，生成虚拟表VT6； SELECT: 选择 SELECT 指定的列，插入到虚拟表VT7中； DISTINCT: 对虚拟表VT7中的数据进行去重，产生VT8； ORDER BY: 对虚拟表VT8的中的数据进行排序生成VT9； LIMIT: 取出VT9中指定行的数据，产生虚拟表VT10，并返回数据 ","date":"2021-09-25T17:18:01+08:00","permalink":"https://x-xkang.com/p/mysql-%E5%85%B3%E9%94%AE%E5%AD%97%E7%9A%84%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F/","title":"Mysql 关键字的执行顺序"},{"content":"一、Int 1、string 转 int 1 2 3 4 5 6 7  var str = \u0026#34;1001\u0026#34; n, _ := strconv.Atoi(str) // 官方 \u0026#34;strconv\u0026#34; 包  fmt.Printf(\u0026#34;type:%T, value:%v\\n\u0026#34;, n, n) // type:int, value:1001    2、string 转 int64 1 2 3 4 5 6 7  var str = \u0026#34;1001\u0026#34; n, _ := strconv.ParseInt(str, 10, 64) fmt.Printf(\u0026#34;type:%T, value:%v\\n\u0026#34;, n, n) // type:int64, value:1001    3、string 转浮点数 1 2 3 4 5 6 7  var str = \u0026#34;3.1415926\u0026#34; n, _ := strconv.ParseFloat(str, 10) fmt.Printf(\u0026#34;type:%T, value:%v\\n\u0026#34;, n, n) // type:float64, value:3.1415926    二、String 1、int 转 string 1 2 3 4 5 6 7  var n int = 100 str := strconv.Itoa(n) fmt.Printf(\u0026#34;type:%T, value:%v\\n\u0026#34;, str, str) // type:string, value:100    2、int64 转 string 1 2 3 4 5 6 7  var n int64 = 100 str := strconv.FormatInt(n, 10) fmt.Printf(\u0026#34;type:%T, value:%v\\n\u0026#34;, str, str) // type:string, value:100    3、uint32 转 string 1 2 3 4 5 6 7  var n uint32 = 10 str := strconv.ParseUint(uint64(n), 10) fmt.Printf(\u0026#34;type:%T, value:%v\\n\u0026#34;, str, str) // type:string, value:100    三、Struct 1、json 转 struct 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  type Person struct { Name string `json:\u0026#34;name\u0026#34;` Age int `json:\u0026#34;age\u0026#34;` } var jsonStr = []byte(`{\u0026#34;name\u0026#34;: \u0026#34;neil\u0026#34;, \u0026#34;age\u0026#34;: 21}`) var p = Person{} err := json.Unmarshal(jsonStr, \u0026amp;p) // 官方 “json” 包 if err != nil { fmt.Println(\u0026#34;Unmarshal failed:\u0026#34;, err) return } fmt.Printf(\u0026#34;p:%#v\\n\u0026#34;, p) // p:main.Person{Name:\u0026#34;neil\u0026#34;, Age:21}    2、map 转 struct 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  type Person struct { Name string `json:\u0026#34;name\u0026#34;` Age int `json:\u0026#34;age\u0026#34;` } var m = map[string]interface{}{ \u0026#34;name\u0026#34;: \u0026#34;king\u0026#34;, \u0026#34;age\u0026#34;: 19, } jsonStr, err := json.Marshal(m) if err != nil { fmt.Println(\u0026#34;Marshal failed:\u0026#34;, err) return } err = json.Unmarshal(jsonStr, \u0026amp;p) if err != nil { fmt.Println(\u0026#34;Unmarshal failed too:\u0026#34;, err) return } fmt.Printf(\u0026#34;p:%#v\\n\u0026#34;, p) // p:main.Person{Name:\u0026#34;king\u0026#34;, Age:19}    Map 1、json 转 map 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  var jsonStr = []byte(`{\u0026#34;name\u0026#34;: \u0026#34;neil\u0026#34;, \u0026#34;age\u0026#34;: 21}`) var m = map[string]interface{}{} err := json.Unmarshal(jsonStr, \u0026amp;m) if err != nil { fmt.Println(\u0026#34;Unmarshal failed:\u0026#34;, err) return } fmt.Printf(\u0026#34;m:%#v\\n\u0026#34;, m) // m:map[string]interface {}{\u0026#34;age\u0026#34;:21, \u0026#34;name\u0026#34;:\u0026#34;neil\u0026#34;}    2、 struct 转 map 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  type Person struct { Name string `json:\u0026#34;name\u0026#34;` Age int `json:\u0026#34;age\u0026#34;` } var p = Person{ Name: \u0026#34;king\u0026#34;, Age: 17, } var jsonStr, _ = json.Marshal(p) var m = map[string]interface{}{} _ = json.Unmarshal(jsonStr, \u0026amp;m) fmt.Printf(\u0026#34;m:%#v\\n\u0026#34;, m) // m:map[string]interface {}{\u0026#34;age\u0026#34;:17, \u0026#34;name\u0026#34;:\u0026#34;king\u0026#34;}    ","date":"2021-06-24T13:58:33+08:00","permalink":"https://x-xkang.com/p/golang-%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2/","title":"Golang 常用数据类型转换"},{"content":"一、Mysql 事务 MySQL 事务主要用于处理操作量大、复杂度高的数据\n MySQL 数据库中只有 Innodb 存储引擎支持事务操作 事务处理可以用来维护数据库的完整性，保证成批的 SQL 要么全部执行，要么全部不执行 事务用来管理insert，update，delete语句   二、事务特性 一般来说，事务必须满足 4 个条件（ACID），即原子性、一致性、持久性、隔离性，具体如下：\n1、原子性 (Atomicity)  一个事务（Transaction）中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节，事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像事务没有执行过一样。\n 2、一致性（Consistency）  在事务开始之前以及事务结束之后，数据库的完整性没有被破坏。这标识写入的数据必须完全符合所有的预设规则，这包含数据的精确度，串联性以及后续数据库可以自发的完成预定的工作。\n 3、隔离性（Isolation）  数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致，事务隔离分为以下不同级别：\n   读未提交（Read uncommited）: 允许脏读，也就是可能读到其他会话中未提交事务修改的数据。\n  读已提交（Read commited）: 只能读取到已提交的数据。\n  可重复读（Repeatable read）: 在同一个事务内的查询都是从开始时刻一致的，InnoDB 存储引擎默认的事务隔离级别就是可重复读，在 SQL 标准中，该隔离级别消除了不可重复读，但还是存在幻读。\n  串行化（Serializable）: 完全串行化的读，每次读都需要获得表级的共享锁，读写相互都会阻塞。\n  4、持久性（Durability）  事务处理结束后，对数据的修改就是永久的，几遍系统故障也不会丢失。\n  三、事务的并发处理 准备工作：创建数据表，插入一条数据\n1 2 3 4 5 6 7 8  create table user( id int(10) not null auto_increment comment \u0026#39;主键ID\u0026#39;, name varchar(30) not null default \u0026#39;\u0026#39; comment \u0026#39;用户名\u0026#39;, primary key(id) ) engine=innodb charset=utf8mb4; # 插入数据 insert into `user`(`name`) values(\u0026#39;老王01\u0026#39;);   事务并发可能出现的情况：\n 脏读  一个事务读到了另一个未提交事务修改过的数据\n 1、会话 B 开启一个事务，把id=1的name改为老王01；\n2、会话 A 也开启一个事务，读取id=1的name，次时的查询结果为老王02；\n3、会话 B 的事务回滚了修改的操作，这样会话 A 读到的数据就是不存在的；\n这个现象就是脏读。（脏读只会在读未提交的隔离级别中才会出现）。\n  不可重复读  一个事务只能读到另一个已经提交的事务修改过的数据，并且其他事务每对该数据进行一次修改并提交后，该事务都能查询得到最新值。（不可重复读在读未提交和读已提交隔离级别都可能会出现）\n 1、会话 A 开启事务，查询id=1的 name 是老王01；\n2、会话 B 将id=1的 name 更新为老王02（隐式事务，autocommit=1，执行完 sql 会自动 commit）；\n3、会话 A 再查询时id=1的 name 为老王02；\n4、会话 B 又将id=1的 name 更新为老王03；\n5、会话 A 再查询id=1的 name 时，结果为老王03。\n这种现象就是不可重复读。\n  幻读  一个事务先根据某些条件查出一些记录，之后另一个事务又想表中插入了符合这些条件的记录，原先的事务再次按照该条件查询时，能够把另一个事务插入的数据也查出来。 （幻读在读未提交、读已提交、可重复读隔离级别中都可能会出现）\n 1、会话 A 开始事务，查询id\u0026gt;0的数据，结果只有 name=老王 01 的一条数据\n2、会话 B 像数据表中插入了一条name=老王02的数据（隐式事务，执行 sql 后自动 commit）\n3、会话 A 的事务再次查询 id\u0026gt;0的数据\n  不同隔离级别下出现事务并发问题的可能     隔离级别 脏读 不可重复读 幻读     读未提交 可能 可能 可能   读已提交 不可能 可能 可能   可重复读 不可能 不可能 可能   串行化 不可能 不可能 不可能     四、事务的实现原理 首先了解一下 redo log 和 undo log\n1、redo log MYSQL 为了提升性能不会把每次的修改都实时同步到磁盘，而是会优先存储到 Buffer Pool（缓冲池）里面，把这个当做缓存来用，然后使用后台线程去做缓冲池和磁盘之间的同步\n如果还没来得及同步数据就出现宕机或者断电，就会导致丢失部分已提交事务的修改信息，\n所以引入了redo log来记录已成功提交事务的修改信息，并且把 redo log 持久化到磁盘，系统重启之后读取 redo log 恢复最新数据\n redo log 是用来恢复数据的，用于保障已提交事务的持久化特性。\n 2、undo log undo log 叫做回滚日志，用于记录数据被修改前的信息，与 redo log 记录的数据相反，redo log 是记录修改后的数据，undo log 记录的是数据的逻辑变化，为了发生错误时回滚之前的操作，需要将之前的操作都记录下来，然后在发生错误时才可以回滚\n每次写入数据或者修改数据之前都会把修改前的信息记录到 undo log\n3、事务特性的具体实现原理  事务的原子性通过 undo log 来实现的 事务的持久性是通过 redo log 实现的 事务的隔离性是通过 读写锁 + MVCC 实现的 事务的一致性是通过 **原子性、持久性、隔离性**来实现的  3.1、原子性的实现    每条数据变更（insert/update/delete）操作都会记录一条undo log，并且undo log必须先于数据持久化到磁盘上。\n  所谓的回滚就是根据undo log做逆向操作，比如delete的逆向操作是insert，insert的逆向操作是delete，update的逆向操作是update等。\n  为了做到同时成功或者同时失败，当系统发生错误或者执行rollback时需根据undo log进行回滚\n   3.2、持久性的实现  Mysql 的数据存储机制是将数据最终持久化到磁盘上，并且频繁的进行磁盘 IO 是非常消耗性能的，为了提升性能，InnoDB 提供了缓冲池（Buffer Pool），缓冲池中包含了磁盘数据也的映射，可以当做缓存来使用\n读数据：会首先从缓冲池中读取，若没有，则从磁盘读取并放入缓冲池中\n写数据：会首先写入缓冲池中，缓冲池中的数据会定期同步到磁盘中\n 那么问题来了，如果在缓冲池的数据还没有同步到磁盘上时，出现了机器宕机或者断电，可能会出现数据丢失的问题，因此我们需要记录已提交事务的数据，于是，redo log登场了， redo log 在执行数据变更（insert/update/delete）操作的时候，会变更后的结果记录在缓冲区，待commit事务之后同步到磁盘\n至于redo log也要进行磁盘 IO，为什么还要用\n (1)、redo log是顺序存储，而缓存同步是随机操作\n(2)、缓存同步是以数据页为单位，每次传输的数据大小小于redo log\n 3.3、隔离性的实现   读未提交： 读写并行，读的操作不能排斥写的操作，因此会出现脏读,不可重复读,幻读的问题\n  读已提交： 使用排他锁X，更新数据需要获取排他锁，已经获取排他锁的数据，不可以再获取共享锁S以及排他锁X，读取数据使用了MVCC（Mutil-Version Concurrency Control）多版本并发控制机制（后续单独展开）以及Read view的概念，每次读取都会产生新的Read view，因此可以解决脏读问题，但解决不了不可重复读和幻读的问题\n  可重复读： 同上也是利用MVCC机制实现，但是只在第一次查询的时候创建Read view，后续的查询还是沿用之前的Read view，因此可以解决不可重复读的问题，具体不在这展开，但还是有可能出现幻读\n  串行化 ：读操作的时候加共享锁，其他事务可以并发读，但是不能并发写，执行写操作的时候加排他锁，其他事务既不能并发写，也不能并发读，串行化可以解决事务并发中出现的脏读、不可重复读、幻读问题，但是并发性能却因为加锁的开销变得很差\n  3.4、一致性的实现 一致性的实现其实是通过原子性、持久性，隔离性共同完成的\n五、结束语 了解 MySQL 的事务机制，以及实现原理，对于使用或者优化都有很大的帮助，要保持知其然和知其所以然的心态和持续学习的劲头，了解更多关于 Mysql 相关的知识！\n","date":"2021-03-24T11:18:58+08:00","permalink":"https://x-xkang.com/p/mysql-%E4%BA%8B%E5%8A%A1%E6%B5%85%E6%9E%90/","title":"Mysql 事务浅析"},{"content":"一、问题描述  1、生产环境主站每隔一段时间就会出现卡顿，接口响应慢，甚至超时的情况、\n2、测试环境重现不了（一抹诡异的光）\n 二、问题排查  针对响应慢的接口进行优化，之前的代码风格也存在问题，还是有些滥用sync/await，一些没有依赖关系的操作，全部分开每行await同步执行，分析后把部分DB操作合并一个Promise执行 阿里云查了一下mysql的slow_log，有挺多的慢查询，优化了一部分SQL，业务逻辑太复杂，但是！没有解决问题，主站还是隔一段时间就卡 找到部分接口日志，超时的接口返回的是Knex.js数据库管理工具抛出的异常，KnexTimeoutError: Knex: Timeout acquiring a connection. The pool is probably full.，可能是连接池已满，获取连接失败导致的 看了一下数据库连接池的配置，最大连接数是30，获取连接的超时时间是60s，可能是并发量大加上部分操作未释放连接导致后续的操作无法正常获取数据库连接池的连接，大概又定位了一下可能的问题点\n 1，主站的信息列表会隔几秒钟轮询，获取最新的数据，如果1000个用户在线，轮询周期内就会有1000个查询，中间也没有做缓存处理，导致并发到DB的请求会比较多 2，为了维护DB的状态统一，用户的部分操作用了事务，一些事务内包含了太多操作（感觉是长期占用连接未释放的罪魁祸首） 3，测试环境重现不了是因为没有经过压测，只测试了功能，没有测试性能，日常测试也没有大并发   三、问题验证    将数据库连接池的数量改成了1，使用事务的接口中做了延时的transaction.commit()操作，然后另外一个请求再去正常查询，\n  结果显示，如果一个用户调用了事务操作的接口，然后再调用查询接口，查询会一直阻塞在获取连接的步骤，直至事务commit之后释放连接，如果在配置的timeout时间之前没有获取到，Knex就会抛出Timeout acquiring a connection. The pool is probably full的异常，\n  也侧面印证了为什么测试环境复现不了这种情况，毕竟在没有压测的前提下。两个测试通过手动操作，并发量是达不到配置的数量的，也就不会出现卡顿的情况\n   四、解决方案    1，优化长事务的操作，减少不必要的事务，提高处理效率（难度较大，业务逻辑比较复杂）；\n  2，合理范围内增加数据库连接池的最大连接数配置，线上的mysql可连接300个，后端3个服务，现在配置是10、30、30，先把主站改50看看，连接数太大也会导致磁盘I/O效率大幅降低又会导致其他问题；\n  3，轮询获取列表的操作，可以改成服务端主动去推（使用socket.io），然后加一个中间缓存（redis），毕竟列表数据变化的频率不是很高；\n  4，数据库扩展成读写分离，update和insert的操作直接操作主库，大部分select操作转移到从库，即使有部分的事务操作慢，也不会导致主站的基本查询卡住\n   五、写在最后  系统的业务逻辑比较复杂，从业务代码层面下手成本还是比较高，接口的耦合都比较高，重构都比改的成本低，开始的设计，可能也没有考虑扩展的问题，并发的问题等， 包括每个服务之间的通信问题，后期再慢慢优化吧，不怕有问题，就怕一直没遇到过问题！加油~。\n","date":"2021-03-12T17:42:20+08:00","permalink":"https://x-xkang.com/p/mysql-%E8%BF%9E%E6%8E%A5%E6%B1%A0%E9%97%AE%E9%A2%98/","title":"Mysql 连接池问题"},{"content":" 资源转自[B站]，如侵删\n","date":"2021-02-04T13:35:56+08:00","permalink":"https://x-xkang.com/p/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%89/","title":"计算机网络学习笔记（三）"},{"content":" 资源转自[B站]，如侵删\n","date":"2021-02-04T09:55:34+08:00","permalink":"https://x-xkang.com/p/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%BA%8C/","title":"计算机网络学习笔记（二）"},{"content":" 资源转自[B站]，如侵删\n","date":"2021-01-22T16:13:30+08:00","permalink":"https://x-xkang.com/p/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%80/","title":"计算机网络学习笔记（一）"},{"content":"一、物理层基本概念  物理层解决如何在连接各种计算机的传输媒体上传输数据比特流，而不是指具体的传输媒体，物理层的主要任务是确定与传输媒体接口有关的一些特性\n 1、机械特性  定义物理连接的特性，规定物理连接时所采用的规格，接口形状，引线数目，引脚数量和排列情况\n 2、电气特性  规定传输二进制位时，线路上信号的电压范围，阻抗匹配，传输速率和距离限制\n 3、功能特性  知名某条线上出现的某一电平标识何种意义，接口不见的信号线的用途\n 4、规程特性  (过程特性)定义各条物理线路的工作规程和时序关系\n  二、数据通信基础知识 1、典型的数据通信模型 2、数据通信相关术语  通信的目的是传送消息\n数据：传送消息的实体，通常是有意义的符号序列\n信号：数据的电气/电磁表现，是数据在传输过程中的存在形式\n  数字信号：代表消息的参数取值是离散的 模拟信号：代表消息的参数是连续的   信源：产生和发送数据的源头\n信宿：接受数据的终点\n信道：信号的传输媒介，一般用来表示向某一个方向传送信息的介质，因此一条通信线路往往包含一条发送信道和一条接受信道\n 3、三种通信方式  从通信双方信息的交互方式来看，可以有三种基本方式：\n单工通信：只有一个方向的通信而没有反方向的交互，仅需一条信道\n半双工信道：通信的双方都可以发送或接受信息，但任何一方都不能同时发送和接受，需要两条信道\n全双工信道：通信双方可以同时发送和接受信息，也需要两条信道\n 4、两种数据传输方式  串行传输：速度慢、费用低、适合远距离\n并行传输：速度快、费用高、适合近距离\n  三、码元、波特、速率、带宽 1、码元  码元： 是指用给一个固定时长的信号波形（信号波形），代表不同离散数值的基本波形，是数字通信中数字信号的计量单位，这个时长内的信号称为k进制码元，而该时长称为码元宽度，当码元的离散状态有M个时（M大于2）此时码元为M进制码元\n1码元可以携带多个比特的信息量， 例如： 在使用二进制编码时，只有两种不同的码元，一种代表0状态，另一种代表1状态\n 2、速率、波特、带宽  速率： 也叫 数据率 是指数据的传输速率，表示单位时间内传输的数据量，可以用码元传输速率和信息传输速率表示\n   1）码元传输速率： 别名码元速率、波形速率、调制速率、符号速率等，它标识单位时间内数字通讯系统所传输的码元个数（也可称为脉冲个数或信号变化的次数），单位是 波特(Baud)。1波特表示数字通讯系统每秒传输一个码元，这里的码元可以是多进制的，但码元速率与进制无关。\n  2）信息传输速率： 别名信息速率、比特率等，表示单位时间内数字通讯系统传输的二进制码元个数（即比特数），单位是 比特/秒（b/s）\n   关系： 若一个码元携带 n bit的信息量，则M Baud的码元传输速率所对应的信息传输速率为 M x n bit/s\n带宽： 表示在单位时间内从网络中的某一点到另一点所能通过的 “最高数据率”，常用来表示网络的通信线路所能传输数据的能力，单位是b/s。\n  四、奈氏准则和香农定理 1、失真 2、码间串扰 3、奈氏准则 4、香农定理  五、编码与调制 1、基带信号和宽带信号 2、编码与调制  1）非归零编码\n  2）归零编码\n  3）反向不归零编码\n  4）曼彻斯特编码\n  5）差分曼彻斯特编码\n  6）4B/5B编码\n  7）数字数据调制为模拟信号\n  8）模拟数据编码为数字信号\n ","date":"2020-01-25T17:21:56+08:00","permalink":"https://x-xkang.com/p/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-%E7%89%A9%E7%90%86%E5%B1%82/","title":"计算机网络--物理层"},{"content":"一、模式切换    i 切换到输入模式，以输入字符。\n  x 删除当前光标所在处的字符。\n  : 切换到底线命令模式，以在最底一行输入命令。\n    二、输入模式  在命令模式下按下i就进入了输入模式。 在输入模式中，可以使用以下按键：\n  字符按键以及Shift组合，输入字符\n  ENTER，回车键，换行\n  BACK SPACE，退格键，删除光标前一个字符\n  DEL，删除键，删除光标后一个字符\n  ↑/↓/←/→ 方向键，在文本中移动光标\n  HOME/END，移动光标到行首/行尾\n  Page Up/Page Down，上/下翻页\n  Insert，切换光标为输入/替换模式，光标将变成竖线/下划线\n  ESC，退出输入模式，切换到命令模式\n    三、命令模式 1、移动光标    命令 作用     h 或 向左箭头键(←) 光标向左移动一个字符   j 或 向下箭头键(↓) 光标向下移动一个字符   k 或 向上箭头键(↑) 光标向上移动一个字符   l 或 向右箭头键(→) 光标向右移动一个字符   [Ctrl] + [f] 屏幕『向下』移动一页，相当于 [Page Down]按键 (常用)   [Ctrl] + [b] 屏幕『向上』移动一页，相当于 [Page Up] 按键 (常用)   [Ctrl] + [d] 屏幕『向下』移动半页   [Ctrl] + [u] 屏幕『向上』移动半页   + 光标移动到非空格符的下一行   - 光标移动到非空格符的上一行   n\u0026lt;space\u0026gt; 那个 n 表示『数字』，例如 20 。按下数字后再按空格键，光标会向右移动这一行的 n 个字符。例如 20\u0026lt;space\u0026gt; 则光标会向后面移动 20 个字符距离。   0 或功能键[Home] 这是数字『 0 』：移动到这一行的最前面字符处 (常用)   $ 或功能键[End] 移动到这一行的最后面字符处(常用)   H 光标移动到这个屏幕的最上方那一行的第一个字符   M 光标移动到这个屏幕的中央那一行的第一个字符   L 光标移动到这个屏幕的最下方那一行的第一个字符   G 移动到这个档案的最后一行(常用)   nG n 为数字。移动到这个档案的第 n 行。例如 20G 则会移动到这个档案的第 20 行(可配合 :set nu)   gg 移动到这个档案的第一行，相当于 1G 啊！ (常用)   n\u0026lt;Enter\u0026gt; n 为数字。光标向下移动 n 行(常用)     2、搜索替换    命令 作用     /word 向光标之下寻找一个名称为 word 的字符串。例如要在档案内搜寻 vbird 这个字符串，就输入 /vbird 即可！ (常用)   ?word 向光标之上寻找一个字符串名称为 word 的字符串。   n 这个 n 是英文按键。代表重复前一个搜寻的动作。举例来说， 如果刚刚我们执行 /vbird 去向下搜寻 vbird 这个字符串，则按下 n 后，会向下继续搜寻下一个名称为 vbird 的字符串。如果是执行 ?vbird 的话，那么按下 n 则会向上继续搜寻名称为 vbird 的字符串！   N 这个 N 是英文按键。与 n 刚好相反，为『反向』进行前一个搜寻动作。 例如 /vbird 后，按下 N 则表示『向上』搜寻 vbird 。   :n1,n2s/word1/word2/g n1 与 n2 为数字。在第 n1 与 n2 行之间寻找 word1 这个字符串，并将该字符串取代为 word2 ！举例来说，在 100 到 200 行之间搜寻 vbird 并取代为 VBIRD 则：『:100,200s/vbird/VBIRD/g』。(常用)   :1,$s/word1/word2/g或 :%s/word1/word2/g 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！(常用)   :1,$s/word1/word2/gc 或 :%s/word1/word2/gc 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！且在取代前显示提示字符给用户确认 (confirm) 是否需要取代！(常用)     【注】：使用 /word 配合 n 及 N 是非常有帮助的！可以让你重复的找到一些你搜寻的关键词！\n  3、删除、复制与粘贴    命令 作用     x, X 在一行字当中，x 为向后删除一个字符 (相当于 [del] 按键)， X 为向前删除一个字符(相当于 [backspace] 亦即是退格键) (常用)   nx n 为数字，连续向后删除 n 个字符。举例来说，我要连续删除 10 个字符， 『10x』。   dd 删除游标所在的那一整行(常用)   ndd n 为数字。删除光标所在的向下 n 行，例如 20dd 则是删除 20 行 (常用)   d1G 删除光标所在到第一行的所有数据   dG 删除光标所在到最后一行的所有数据   d$ 删除游标所在处，到该行的最后一个字符   d0 那个是数字的 0 ，删除游标所在处，到该行的最前面一个字符   yy 复制游标所在的那一行(常用)   nyy n 为数字。复制光标所在的向下 n 行，例如 20yy 则是复制 20 行(常用)   y1G 复制游标所在行到第一行的所有数据   yG 复制游标所在行到最后一行的所有数据   y0 复制光标所在的那个字符到该行行首的所有数据   y$ 复制光标所在的那个字符到该行行尾的所有数据   p, P p 为将已复制的数据在光标下一行贴上，P 则为贴在游标上一行！ 举例来说，我目前光标在第 20 行，且已经复制了 10 行数据。则按下 p 后， 那 10 行数据会贴在原本的 20 行之后，亦即由 21 行开始贴。但如果是按下 P 呢？ 那么原本的第 20 行会被推到变成 30 行。 (常用)   J 将光标所在行与下一行的数据结合成同一行   c 重复删除多个数据，例如向下删除 10 行，[ 10cj ]   u 复原前一个动作。(常用)   [Ctrl]+r 重做上一个动作。(常用)   . 不要怀疑！这就是小数点！意思是重复前一个动作的意思。 如果你想要重复删除、重复贴上等等动作，按下小数点『.』就好了！ (常用)     【注】：这个 u 与 [Ctrl]+r 是很常用的指令！一个是复原，另一个则是重做一次～ 利用这两个功能按键，你的编辑，嘿嘿！很快乐的啦！\n  4、进入输入或取代的编辑模式    命令 作用     i, I 进入输入模式(Insert mode)： i 为『从目前光标所在处输入』， I 为『在目前所在行的第一个非空格符处开始输入』。 (常用)   a, A 进入输入模式(Insert mode)： a 为『从目前光标所在的下一个字符处开始输入』， A 为『从光标所在行的最后一个字符处开始输入』。(常用)   o, O 进入输入模式(Insert mode)： 这是英文字母 o 的大小写。o 为『在目前光标所在的下一行处输入新的一行』； O 为在目前光标所在处的上一行输入新的一行！(常用)   r, R 进入取代模式(Replace mode)： r 只会取代光标所在的那一个字符一次； R 会一直取代光标所在的文字，直到按下 ESC 为止；(常用)   [Esc] 退出编辑模式，回到一般模式中(常用)     【注】：上面这些按键中，在 vi 画面的左下角处会出现『\u0026ndash;INSERT\u0026ndash;』或『\u0026ndash;REPLACE\u0026ndash;』的字样。 由名称就知道该动作了吧！！特别注意的是，我们上面也提过了，你想要在档案里面输入字符时， 一定要在左下角处看到 INSERT 或 REPLACE 才能输入喔！\n  5、指令行的储存、离开等指令    命令 作用     :w 将编辑的数据写入硬盘档案中(常用)   :w! 若文件属性为『只读』时，强制写入该档案。不过，到底能不能写入， 还是跟你对该档案的档案权限有关啊！   :q 离开 vi (常用)   :q! 若曾修改过档案，又不想储存，使用 ! 为强制离开不储存档案。注意一下啊，那个惊叹号 (!) 在 vi 当中，常常具有『强制』的意思～   :wq 储存后离开，若为 :wq! 则为强制储存后离开 (常用)   ZZ 这是大写的 Z 喔！若档案没有更动，则不储存离开，若档案已经被更动过，则储存后离开！   :w [filename] 将编辑的数据储存成另一个档案（类似另存新档）   :r [filename] 在编辑的数据中，读入另一个档案的数据。亦即将 『filename』 这个档案内容加到游标所在行后面   :! command 暂时离开 vi 到指令行模式下执行 command 的显示结果！例如『:! ls /home』即可在 vi 当中察看 /home 底下以 ls 输出的档案信息！     6、vim 环境的变更    命令 作用     :set nu 显示行号，设定之后，会在每一行的前缀显示该行的行号   :set nonu 与 set nu 相反，为取消行号！   ","date":"2020-01-06T09:42:51+08:00","permalink":"https://x-xkang.com/p/vim%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","title":"Vim常用命令"},{"content":" 1、构建镜像  在项目根目录下新建 Dockerfile 文件并编辑保存\n 1 2 3 4 5 6 7 8 9  FROMgolang:latest # 依赖的镜像:镜像版本ADD . /var/www/go-aimaster # 将当前工作目录copy到镜像的/var/www/go-aimaster 目录下WORKDIR/var/www/go-aimaster # 设置镜像内的工作目录RUN GOPROXY=\u0026#34;https://goproxy.cn,direct\u0026#34; go build -o main /var/www/go-aimaster/main.go # 运行命令(当前为golang 项目demo)CMD [\u0026#34;/var/www/go-aimaster/main\u0026#34;] # 可执行文件目录，上一步build生成的main可执行文件EXPOSE8080 # 暴露端口，最终暴露的端口不一定是当前的8080 端口ENTRYPOINT [\u0026#34;./main\u0026#34;] # 入口文件   执行命令：docker image build -t 镜像名称[:版本号] . (注意最后有个点 .)\n 1  docker image build -t go-aimamster:v0.01 .   上面代码中，-t 参数用来指定 image 文件的名字，后面还可以用冒号指定标签。如果不指定，默认的标签就是latest。最后的那个点表示 Dockerfile 文件所在的路径，上例是当前路径，所以是一个点。 执行结果如下：\n  出现上图的Successfully成功标识表示已构建成功，执行 docker images 查看，列表中出现刚刚构建的go-aimaster镜像\n  2、下载远端镜像  命令：docker pull 仓库名称\n 1  docker pull nginx   3、推送本地镜像至远端仓库  命令：  1 2 3  docker image tag go-docker:v1.0 devxiaokang/go-docker:v1.0 docker push devxiaokang/go-docker:v1.0   4、查看镜像列表  命令：docker image ls | docker images\n 1  docker images   5、删除本地镜像  命令：docker rmi 镜像标识|镜像名称:版本号\n  【注意】若有容器正在依赖该镜像，则无法删除\n 1  docker rmi go-aimamster:v0.01   6、生成容器  命令： docker [container] run 镜像标识 /bin/bash（简单操作）| docker [container] run -d -p 宿主机端口:容器端口 -it --name 容器名称 镜像标识 /bin/bash （常用操作）\n 1  docker run nginx  或者：\n1  docker run -d -p 8080:80 -it --name nginx nginx:latest /bin/bash   以上代码中-d 代表后台运行， -p 代表宿主机端口与容器端口的映射关系，-it 代表容器的 shell 映射到当前的 shell，然后再本机窗口输入命令，就会传入容器中，--name nginx 代表定义容器名称nginx 为自定义名称，。\n 执行结果如下：  7、查看容器列表  命令：docker ps -a[q]， -a 表示显示所有容器（包括已停止的），-q 列表值显示容器的唯一标识\n 1  docker ps -a  1  docker ps -aq   8、进入容器  命令：docker exec -it 容器ID|容器名称 /bin/bash\n 1  docker exec -it nginx /bin/bash   9、启动容器  命令：docker start 容器ID|容器名称\n 1  docker start nginx   10、重启容器  命令：docker restart 容器ID|容器名称\n 1  docker restart nginx   11、停止容器  命令：docker stop 容器ID|容器名称\n 1  docker stop nginx   停止全部容器\n 1  docker stop $(docker ps -qa)   12、删除容器  命令：docker rm 容器ID|容器名称\n  删除指定容器\n 1  docker rm nginx   删除全部容器\n 1  docker rm $(docker ps -qa)   13、数据卷  数据卷：将宿主机的一个目录映射到容器的一个目录中，可以在宿主机中操作目录中的内容，那么容器内部映射的文件，也会跟着一起改变,创建数据卷之后，默认会存在一个目录下 /var/lib/docker/volumes/数据卷名称/_data\n  创建数据卷\n 1  docker volume create volume_name   查看数据卷\n 1  docker volume inspect volume_name   查看全部数据卷\n 1  docker volume ls   删除数据卷：docker volume rm 数据卷名称\n 1  docker volume rm volume_name   14、管理多容器  .yml文件以key: value 方式来指定配置信息，多个配置信息以换行+锁紧的方式来区分，在docker-compose文件中，不要使用制表符\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  # yml version: ‘3.1’ service: mysql: restart: always # 代表只要docker启动，这个容器就会跟着启动 image: daoclound.io/lib/mysql:5.7.4 # 镜像路径 container_name: mysql # 指定容器名称 ports: - 3306:3306 environment: MYSQL_ROOT_PASSWORD: 123456 TZ: Asia/Shanghai # 时区 volumes: # 数据卷 - /opt/docker_mysql/data:/var/lib/mysql tomcat: restart: always Image: daocloud.io/library/tomcat:8.5.15-jre8 # 镜像 container_name: tomcat ports: - 8080:8080 environment: TZ: Asia/Shanghai volumes: - /opt/docker_mysql_tomcat/tomcat_webapps:/usr/local/tomcat/webapps - /opt/docker_mysql_tomcat/tomcat_logs:/usr/local/tomcat/logs     Docker-Compose 配置Dockerfile使用 使用docker-compose.yml文件以及Dockerfile文件在生成自定义镜像的同时启动当前镜像，并且由docker-compose去管理容器\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  # yml version: ‘3.1’ services: mysql: restart: always build: context: ../ # 指定dockerfile文件所在路径 dockerfile: Dockerfile #指定dockerfile文件名称 container_name: mysql ports: - 3306:3306 environment: TZ: Asia/Shanghai    可以直接启动基于 docker-compose.yml以及Dockerfile文件构建的自定义镜像 docker-compose up -d 如果自定义镜像不存在，会帮助我们构建出自定义镜像，如果自定义镜像已存在，会直接运行这个自定义镜像，重新构建的话需执行 docker-compose build , 运行前重新构建 docker-compose up -d —build\n Docker-compose 命令：\n 后台启动： docker-compose up -d 关闭并删除容器： docker-compose down 开启|关闭|重启已经存在的有docker-compose维护的容器： docker-compose start | stop | restart 查看docker-compose管理的容器： docker-compose ps 查看日志： docker-compose logs -f  ","date":"2019-09-18T02:01:58+05:30","permalink":"https://x-xkang.com/p/docker-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","title":"Docker 常用命令"}]